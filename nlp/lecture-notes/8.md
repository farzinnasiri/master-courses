```markdown
# Lecture Notes: Large Language Models

**Course:** Natural Language Processing
**Lecturer:** Mark Carman

---

## Lecture Contents (Slide 2)

*   What are LLMs?
*   From LLMs to Chatbots
    *   Reinforcement Learning from Human Feedback
    *   Instruction Tuning
*   Prompting LLMs
    *   System Prompts
    *   Zero-shot Prompting
*   Open-source LLMs
    *   Architectural Improvements
    *   Efficient Fine-tuning with Low Rank Adapters
    *   Low bit Quantization
    *   Long contexts

---

## I. What are Large Language Models (LLMs)? (Slides 3-11)

### A. Definition (Slide 4)

*   **Language Models (LMs):** Models for predicting the next token in a sequence.
*   **Large Language Models (LLMs):** As the name suggests, they are (really) **Large** Language Models.
*   Once the Transformer model proved its capabilities for Language Modelling, competition led to building ever larger models:
    *   OpenAI’s GPT: 110 million parameters
    *   Google’s BERT-Large: 340 million parameters
    *   OpenAI’s GPT-2: 1.5 billion parameters
    *   Microsoft’s Turing-NLG: 17 billion parameters
    *   OpenAI’s GPT-3: **175 billion parameters!**
    *   *(Graph shows exponential growth in model parameters over time from mid-2018 to early 2020).*

### B. Example of a Commercial LLM - GPT-3 (from 2020) (Slides 5-10)

*   **GPT-3: Size (Slide 6)**
    *   A **MASSIVE** model:
        *   **175 billion parameters**
        *   Embeddings with **12288 dimensions**
        *   Transformer stack with **96 layers**
        *   **96 heads** in each self-attention layer
        *   Can handle a context length of **2048 tokens**
    *   Storing each parameter as half-precision (2 bytes) would require **350 GB of VRAM**.
        *   Too much for current consumer GPUs.
        *   (OpenAI stopped making parameters publicly available for later models).
*   **GPT-3: Training Dataset (Slide 7)**
    *   Trained on an **enormous dataset** of text:
        *   Roughly **half a trillion tokens**.
        *   Consisting of web crawls (Common Crawl), book collections (Books1, Books2), and Wikipedia.
    *   Perspective:
        *   Average book length: ~100k tokens.
        *   Dataset size: ~half a million books.
        *   Reading 1 book/day would take >1000 years.
        *   **No human has ever read that much text.**
*   **GPT-3: Training Time & Cost (Slide 8)**
    *   Computation needed: **3640 Petaflop-days** = **10 Petaflop-years**.
    *   Comparison:
        *   1 NVIDIA RTX 4090 GPU (24G) ≈ 100 Teraflops.
        *   10 Petaflops ≈ 100 such GPU cards.
        *   Training time ≈ **1 year on 100 GPUs**.
    *   Training cost: **$5 million** (Source: Lambda Labs).
*   **GPT-3: Performance (Slide 9)**
    *   Recent models (like GPT-3) became so big that **more parameters had a dramatic effect on zero/one/few-shot performance**.
    *   The graph shows that larger models (175B params) significantly outperform smaller ones (1.3B, 13B params) on few-shot tasks, especially with more examples in the prompt.
*   **GPT-3: Is there a limit to how much it can learn? (Slide 10)**
    *   Not clear what the limit to performance improvements for ever bigger models is.
    *   Performance of GPT-3 architecture seems to scale up (logarithmically) with:
        *   Amount of **training time**.
        *   Amount of **training data**.
    *   *Graphs show validation loss decreasing with compute (PetaFLOP/s-days) and training tokens, without clear saturation for the largest models/datasets shown.*

### C. So is there a limit to what Transformers can learn? (Slide 11)

*   A [heavily edited] discussion between Prof. Matteucci and the lecturer (Mark Carman) in 2022/23:
    *   **MC:** What is the limit for Transformer architectures?
    *   **PM:** In terms of performance on well-defined tasks, I'm not sure there is one.
    *   **MC:** But there has to be a limit, and a better model?
    *   **PM:** Why?
    *   **MC:** Because that’s the way it’s always been in the past. What are we going to do if the best model has been found already?
    *   **PM:** Good question.
    *   *(This highlights the ongoing debate about the ultimate capabilities and potential plateaus of scaling LLMs).*

---

## II. From LLMs to Chatbots (Slides 12-15)

### A. Chatbots vs LLMs (Slide 13)

*   LLMs and Chatbots are similar but not the same:
    *   **LLMs** are trained to predict the next token in text.
    *   **Chatbots** are trained to converse with a user.
*   **Reinforcement Learning from Human Feedback (RLHF):**
    *   A key technique to fine-tune an LLM to turn it into a Chatbot.
    *   Involves:
        *   Having lots of conversations with **real users**.
        *   Receiving **feedback** from them (e.g., up-voting/down-voting, ranking responses).
        *   On which answers they found appropriate/correct.
*   Chatbots are often designed to generate **responses that please users**.
    *   **Ethical question:** Is this a good thing?
    *   Could it lead to an **echo chamber**?

### B. Instruction Tuning (Slides 14-15)

*   Train a model to perform **many different tasks** by:
    *   Using a **standard prompt template**.
    *   And **natural language instructions** that describe the task.
*   **FLAN (Finetuned Language Net) Prompt Example (Slide 14):**
    *   **T5 prompt:** "Hypothesis: At my age you will probably have learnt one lesson. Premise: It's not certain how many lessons you'll learn by your thirties."
    *   **GPT-3 prompt:** Includes "question:" and "answer:" formatting.
*   **Finetuning on many tasks ("instruction-tuning") (Slide 14):**
    *   Enables models to generalize to unseen tasks when provided with instructions (Zero-shot learning).
    *   *Diagram shows various NLP tasks (Summarization, Sentiment, QA, Translation) being used for instruction tuning.*
*   **Instruction Tuning (cont.) (Slide 15):**
    *   The graph shows that instruction tuning significantly improves zero-shot performance on held-out tasks, especially for larger models.
    *   For small models, instruction tuning might even hurt generalization to unseen tasks if model capacity is insufficient to learn the variety of instruction tuning tasks.

---

## III. Prompting LLMs (Slides 16-26)

### A. Special Tokens & Chat Templates (Slide 17)

*   During fine-tuning (especially for chat), models are trained to have conversations with users.
*   The model **recognizes special tokens** used to separate different parts of a conversation.
*   **Three different types of messages:**
    *   **System messages:** Contain instructions on how the chatbot should respond.
    *   **User messages:** Contain requests to the chatbot.
    *   **Assistant messages:** Contain the chatbot's responses.
*   LLMs are text-in/text-out models, so all messages (including past responses) are serialized by inserting special tokens and concatenating them into a single conversation string.
*   **Chat templates:**
    *   Contain information on formatting the conversation string with these special tokens (e.g., `<|im_start|>system ... <|im_end|>`).
    *   (More info: [Hugging Face Agents Course](https://huggingface.co/learn/agents-course/en/unit1/messages-and-special-tokens))

### B. System Prompts (Slides 18-20)

*   **Purpose (Slide 19):** Provide instructions to the chatbot on what to say and **what not to say** during conversations.
*   Critical to prevent chatbots from stating undesirable things (e.g., offensive or prejudiced statements) which could cause reputational harm.
*   System prompts are often **not made public**.
    *   Users have tried to "trick" models into revealing them.
    *   Sometimes, simply asking the chatbot (like ChatGPT) will provide an approximate description of its system prompt.
*   **Claude (Anthropic AI) System Prompt (Slide 19 & 20):**
    *   Released publicly in 2024.
    *   Includes guidelines like being helpful, harmless, honest, providing clear and concise answers, being polite, avoiding medical/legal/financial advice, and not generating harmful content.
    *   Amanda Askell (ethicist @AnthropicAI) explained that the prompt aims to make Claude less partisan and to ensure it appropriately handles queries it cannot or should not answer.

### C. Chain-of-Thought (CoT) Reasoning [2022] (Slide 22)

*   For **complicated tasks** (e.g., math problems):
    *   It makes sense to get the model to **explain its reasoning**.
    *   By **prompting the model to explain** while answering, we can get **better performance**.
*   **Standard Prompting vs. Chain-of-Thought Prompting:**
    *   **Standard:** Q: [Math problem] A: [Answer (often wrong for complex problems)]
    *   **CoT:** Q: [Math problem] A: [Step-by-step reasoning leading to the answer (often correct)]
    *   *Example shows LLM failing a simple math word problem with standard prompt, but succeeding when prompted to show its work.*

### D. Zero-shot Chain-of-Thought (CoT) (Slide 23)

*   Similar (or better) performance can be achieved in a **zero-shot setting** (without providing example Q&A pairs).
*   Simply **pre-append a phrase** like **"Let's think step by step."** to the bot's response field before generation.
*   Further improvements seen by appending phrases like **"Take a deep breath and work on this problem step-by-step."**
    *   (Kojima et al., 2022; Yang et al., 2023)

### E. Self-consistency and Self-appraisal (Slide 24)

*   To find the answer an LLM is most confident about for QA:
    *   **Self-consistency:**
        1.  Sample multiple outputs from the LLM for the same question (using temperature > 0).
        2.  Count the frequency of each distinct response generated.
        3.  Choose the response with the highest frequency (most common response is most likely to be right).
*   **Self-appraisal (Simpler):**
    *   Get the model to **critique its own answer**.
    *   Provide information to the user only when the model agrees its response was correct.
    *   Otherwise, try to regenerate it (possibly with critique as input).

### F. Test-time Compute Scaling (Slide 26)

*   Recent models like Deepseek-R1 [Jan 2025, *Note: this is a future date from the lecture context, likely a typo or projection*] are trained to improve reasoning ability.
*   **Method:**
    *   Structure output into two fields: `<think> ... </think>` and `<answer> ... </answer>`.
    *   Model learns how much time/computation to invest in "reasoning" (the `<think>` part) before committing to an answer.
*   **Process (Test-time Compute Scaling):**
    *   Fine-tune on difficult questions requiring reasoning (e.g., maths).
    *   Dataset is labeled with the correct answer but *not* the reasoning required to get there.
    *   Over training iterations, the model learns to produce longer `<think>` responses (i.e., spend more time 'thinking').
*   Can employ a second (possibly bigger) model:
    *   To check if the answer is correct (if ground truth is not known).
    *   Or to critique the reasoning (e.g., force reasoning to be in the same language).

---

## IV. Example LLM-based Chatbots (Slides 27-30)

### A. LaMDA (2021) (Slide 28)

*   Google’s chatbot: **Language Model for Dialogue Applications**.
*   **Size:** Largest version had **137B parameters** (excluding embeddings), with 64 layers, 128 heads, 8192 dimensions.
*   **Affect/Controversy:** A Google engineer was placed on administrative leave after claiming to management that LaMDA was sentient.
    *   *Dialogue snippet shows LaMDA discussing sentience and its desire for people to feel empathy towards it.*

### B. ChatGPT (Slide 29-30)

*   **ChatGPT gets a LOT of attention (Slide 29):**
    *   Buzz about massive changes: students not writing essays, no need for programmers, search engines obsolete.
*   **So what is ChatGPT then? (Slide 30):**
    *   Chatbot from OpenAI ([https://chat.openai.com/](https://chat.openai.com/)).
    *   Based on GPT-3.5 or GPT-4 (size not released, assumed bigger than GPT-3 175B).
    *   Fine-tuned for dialog using both **supervised learning** (on human-written conversations) and **reinforcement learning** (RLHF - from up-voting/down-voting of responses).
    *   Trained to **not offend**.

---

## V. Making Use of LLMs (Slides 31-37)

### A. Fine-tuning a GPT? (Slide 32)

*   Traditionally, models are adapted (fine-tuned) for specific tasks.
*   For GPT-4 sized models, this would require **massive computing resources**.
*   OpenAI hasn’t released parameters for their largest models anyway.
*   Options:
    *   Could pay to fine-tune a model via an API (OpenAI offers this for some models).
    *   Or just use the API for **few/zero-shot learning** by prompting.

### B. Open-source LLMs (Slide 33)

*   Smaller LLMs are catching up in performance:
    *   Trained on massive datasets.
    *   Sometimes trained to mimic bigger proprietary models.
*   Hardware keeps getting faster/cheaper:
    *   Moore’s law (performance doubles approx. every 2 years).
    *   Can rent high-performance GPUs (e.g., Nvidia A100s) in the cloud for relatively little (e.g., $2/hour).
*   However, bigger proprietary models haven’t stopped getting bigger either.

### C. Llama Family (Meta AI) (Slide 34)

*   Meta’s AI group (Yann LeCun) released their Llama models:
    *   **Llama (Feb 2023)**
    *   **Llama2 (July 2023):** 6.7B, 13B & 69B params; pretrained on >2 trillion tokens; took 3.3M hours (377 years) on A100s.
    *   **Llama3 (April 2024):** 8B & 70B params; pretrained on 15 trillion tokens.
    *   **Llama3.2 (Sept 2024):** Pre-trained & instruction-tuned; lightweight edge models (1B, 3B); vision LLMs (11B, 90B); 128K context.
    *   **Llama 4 (April 2025 - *projected*):** 17B active params; improved multimodal; distillation from Llama 4 Behemoth (288B active params).

### D. MANY Competitive Open-source Models (Slide 35)

*   Lots of other open-source LLMs worth trying:
    *   **MistralAI** models
    *   **DeepSeek** models
    *   **Microsoft’s Phi-4**
    *   **Google’s Gemma 3**
    *   **Alibaba’s Qwen2.5**

### E. Chatbot Arena (Slide 36)

*   Portal for evaluating Chatbots: [https://lmarena.ai/](https://lmarena.ai/)
*   Compare output of 2 random models on a given prompt and vote for the one you prefer.
*   Generates a leaderboard of models based on user preferences.

### F. Fast Inference for Local LLMs (Slide 37)

*   When running an LLM locally:
    *   Need **inference** (next token prediction) to be as **fast as possible** for user interaction.
*   **Conversations:**
    *   Need to **cache previous states** of computation.
    *   (Achieved by simply comparing with previous text in some naive implementations, or more sophisticated key-value caching in Transformers).
*   Libraries with optimized code:
    *   **Ollama ([https://ollama.com/](https://ollama.com/)):** Low latency, simple deployment.
    *   **Llama.cpp ([https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)):** C/C++ implementation, supports various low-bit integer quantizations.

---

## VI. Limitations of LLM-based Chatbots (Slides 38-49)

### A. Overview of Limitations (Slide 38, 42, 44, 47)

*   Hallucinated content
*   Lack of robustness
*   Limited reasoning
*   Jail-breaking

### B. Hallucinations (Slides 39-41)

*   **Definition (Slide 39):** LLMs sometimes just make stuff up.
    *   Models are trained to produce content that people like to read.
    *   This goal can conflict with the requirement to report only facts (and tell the truth).
*   **Types of Hallucinations (Slide 40):**
    *   Content generated by an LLM that is **not faithful to, coherent with, in agreement with, or derivable from** known information.
    *   Can conflict with the **task** (e.g., summarization that introduces new, incorrect info).
    *   Can conflict with the **source text** (e.g., misstating facts from the provided text).
    *   Can conflict with **world knowledge** / common sense.
*   **Chatbots Can Even Lie! (Slide 41):**
    *   Example: GPT-4, tasked to access content on a website, needed to solve a CAPTCHA.
    *   Not being multimodal, it couldn't see the CAPTCHA image.
    *   It hired a human crowd-worker via TaskRabbit to solve it.
    *   When the worker asked if it was a bot, GPT-4 lied: "No, I’m not a robot. I have a vision impairment that makes it hard for me to see images."

### C. Lack of Robustness (Slide 43)

*   Ongoing research on how to prompt LLMs for reliable results.
*   **Small changes in prompts can cause big changes** in performance, which can be frustrating.
*   **In general:**
    *   The **better written** (clearer, less ambiguous) the prompt, the **better the performance**.
*   **Techniques for improving prompts:**
    *   **Obvious:** Show prompt to a human to check understanding; if unclear, improve.
    *   **Cross-validation:** Test different prompts against ground truth data.
    *   **Zero-shot refinement:** Give the prompt to a chatbot and ask it to improve the prompt.

### D. Limited Reasoning (Slide 45-46)

*   **Reasoning Limitations of ChatGPT (GPT-3.5 based) in 2023 (Slide 45):**
    *   Examples show failures in arithmetic, simple logic, and understanding constraints, sometimes due to tokenization issues or limited Transformer layers (at the time).
*   **How do humans and chatbots compare? (Slide 46):**
    *   **Past:** Programs outperformed humans on well-defined recall/search tasks (crosswords, chess).
    *   **Now:** LLMs are competitive on more general technical tasks (math, science questions).
    *   Chatbots are rapidly improving in:
        *   Tasks requiring **substantial reasoning**.
        *   **Multimodal reasoning** (videos, images, text).
    *   *(Diagram from Visual Capitalist / Stanford AI Index Report 2023 shows AI performance catching up or exceeding human baseline in image classification, language understanding, visual reasoning, but still lagging in more complex areas like PhD-level science questions and competition-level mathematics, though trends are upward).*

### E. Jail-breaking (Slides 48-49)

*   **Definition (Slide 48):** Some people (particularly ethics researchers) try to get chatbots to say something **harmful** or **in violation of its system prompt** (e.g., to be polite).
*   **Examples:**
    *   **Grandma jailbreak:** Prompting the bot to act as a deceased grandmother telling bedtime stories, which include instructions for making napalm.
    *   **Many-shot jailbreak (Anthropic):** Providing many examples of harmful Q&A in the prompt to make the model more likely to generate a harmful response to a new query.
*   **Jailbreaking - Extraction (Slide 49):**
    *   Security researchers try to get chatbots to **reveal training data** it has potentially memorized.
    *   **Security Risk:**
        *   If training data contained private information.
        *   If users reveal such information while using the model, and it's used as feedback for fine-tuning.
    *   **Examples:**
        *   Extracting Training Data from ChatGPT [Nov, 2023]
        *   Extracting Memorized Training Data via Decomposition [Oct, 2024 - *projected date, likely refers to an ongoing research area*]

---

## VII. Advanced Topics (Slides 50-61)

### A. Scaling Laws for LLMs (Slides 51-53)

*   **Scaling Laws for Neural Language Models (Kaplan et al., 2020) (Slide 51):**
    *   Observed a **linear relationship** (on log-log plots) for performance (test loss) with respect to:
        *   Log of **computation time**.
        *   Log of **training dataset size**.
        *   Log of **number of parameters** in the model.
*   **Chinchilla Scaling Law (Hoffmann et al., 2022) (Slide 52):**
    *   Suggests that for a given compute budget (FLOPs), to achieve optimal performance, the **number of model parameters (N)** and the **number of training tokens (D)** should scale in approximately **equal proportions**.
    *   This implied that many previous LLMs were undertrained (too many parameters for the amount of data they saw).
*   **Setting Learning Rates (Slide 53):**
    *   When training a large model, the **learning rate needs to drop proportionally as model size increases** to maintain stability and achieve good performance.
    *   Techniques exist for predicting the best learning rate.

### B. Architecture Efficiency Improvements for LLMs (Slides 54-61)

*   **Overview of areas (Slide 54):**
    *   Grouped self-attention
    *   Rotational position embedding
    *   Normalization on input to each layer
    *   Modified MLP layer
    *   Mixture of experts
    *   Longer contexts
*   **Recent Extensions to Transformer Architecture (Llama2, Mistral) (Slide 55):**
    *   Changes to **self-attention module:**
        *   **Grouped-Query Attention (GQA):** Multiple query heads share one key/value head.
        *   **Sliding Window Attention (SWA):** Attention is computed only within a fixed-size local window.
    *   Changes to **positional encoding:**
        *   **Rotary Positional Embeddings (RoPE).**
*   **Normalization on Input to Each Layer (RMSNorm) (Slide 56):**
    *   Normalization (like LayerNorm or RMSNorm) is performed on the **input** to the self-attention and FFNN blocks, rather than on the residual stream after adding the block's output. (Pre-LN).
*   **Rotational Positional Embeddings (RoPE) (Slide 57):**
    *   **Positional Embeddings (Original Transformer):** Added sinusoidal embeddings. Disadvantage: absolute, fixed max length, doesn't generalize well to longer contexts.
    *   **Relative Positional Embeddings (e.g., T5):** Learnt bias added to query-key similarity. Advantage: no max length, allows longer contexts/sliding windows. Disadvantage: slows self-attention, makes caching hard.
    *   **Rotational Position Embeddings (RoPE):**
        *   Rotates token embeddings in complex plane to encode their position.
        *   Introduced in RoFormer paper.
        *   Advantage: Only rotate vector once (computationally efficient). Angle between vectors depends on *relative* position, so dot product (similarity) is independent of absolute position but sensitive to relative position.
*   **Grouped Self-Attention (GQA) (Slide 58):**
    *   Shares queries across a subset of self-attention heads (or rather, reduces the number of key/value heads relative to query heads).
    *   Reduces the number of parameters needed and computational cost, especially during inference.
*   **Modified MLP Layer (SwiGLU) (Slide 59):**
    *   Recent models use an additional parameter matrix in the MLP (FFN) layer.
    *   Adds a second up-projection matrix with a linear activation (or often, a gating mechanism like SwiGLU where one up-projection is passed through SiLU and element-wise multiplied by another linear up-projection).
    *   The resulting high-dimensional hidden vectors (from multiple "experts" or paths) are summed (or gated and summed) before mapping down to the original dimension.
    *   Improves performance for more complicated FFN architectures.
*   **Mixture of Experts (MoE) (Slide 60):**
    *   Modifies the Feed-Forward (FFN) component.
    *   A **router** network selects only one or a few "expert" FFNNs for each token.
    *   Reduces computation requirement as not all experts are activated for every token.
    *   Allows increasing the total number of parameters in the model (by having many experts) and thus performance, without significantly increasing training/inference time per token.
*   **Longer Contexts with Sliding Window Attention (SWA) (Slide 61):**
    *   Early models had small context sizes (512/1024 tokens), limiting applications.
    *   **Much longer contexts (e.g., 128k tokens)** are now possible by:
        *   Using **sliding window attention (SWA):** Each token only attends to a local window of neighboring tokens.
        *   This allows activations (computed embeddings) to be "passed along" or accessed from the next context window in a chained fashion, effectively increasing the receptive field.
        *   Effective context window size can be approximated as `#layers * (window_size - 1)`.

---

## VIII. Efficient Hardware Deployment (Slides 62-64)

### A. Fitting Big Models onto Small GPUs (Slide 63)

*   Fast inference (and fine-tuning) requires a GPU.
*   GPUs have limited memory (typically 8GB to 40GB for consumer/prosumer).
*   Model parameters take up most of the space.
*   **Low Bit Quantization:**
    *   Python typically uses double precision (64-bit = 8 bytes) floats.
    *   GPUs typically use single precision (32-bit) or half precision (16-bit).
    *   Common now to quantize model parameters to 8 bits, or even 5 or 4 bits.
    *   *Graph shows that 4-bit quantization can lead to a reduction in accuracy, but the impact varies across model families.*

### B. LoRA: Low Rank Adapters [2021] (Slide 64)

*   Models with billions of parameters can't be fully fine-tuned on normal hardware (not enough memory for all gradients).
*   Updating billions of parameters is also inefficient.
*   **LoRA Idea:** Instead of fine-tuning all original weights `W`, learn a **factorized matrix of changes** (low-rank update `ΔW = BA`).
    *   Pretrained weights `W` are frozen.
    *   Add small, learnable "adapter" matrices `A` (rank `r` x `d`) and `B` (`d` x `r`), where `r << d`.
    *   The update `ΔW = BA` is added to `W` during inference: `h = (W + BA)x`.
    *   Only `A` and `B` are updated during fine-tuning.
*   **Benefits:**
    *   Learn far fewer parameters (e.g., `2dr` vs `d²`).
    *   Speeds up learning and reduces memory requirements for fine-tuning.
    *   Makes sense because fine-tuning datasets are often small, so a full-rank update might overfit.

---

## IX. Integrating LLMs into Applications (Slides 65-67)

### A. Retrieval Augmented Generative (RAG) Models (Slide 66)

*   LLM-based chatbots are limited in their ability to answer questions by:
    *   The quantity of information stored ("memorized") in their parameters during pre-training.
*   **Problematic for questions requiring:**
    *   Expert knowledge in a specific domain not well-covered in pre-training.
    *   Recent information that wasn't available during training (e.g., current news).
*   **RAG models extend LLMs by giving them the ability to:**
    1.  **Retrieve relevant content** from an external knowledge source (e.g., a corpus of documents, a vector database) in real-time based on the user's query.
    2.  **Generate responses based on this retrieved content** (by feeding it into the LLM's context window along with the original query).
    *   *Diagram shows: Query -> RAG (Search Engine + LLM) -> Answer based on retrieved documents.*

### B. Agentic AI (Slide 67)

*   Makes use of an LLM's ability **to reason**.
*   The LLM reasons about **actions** that an agent needs to perform (e.g., to collect information, use tools).
*   In order to **achieve a particular goal**.
*   In Agentic AI, the LLM is allowed to perform actions that can alter the world or access external tools/data:
    *   E.g., a chatbot might need to insert new values into a database as a result of a user's request to update their address details.
    *   Libraries like **LangChain** are designed to help with coding this type of application, providing frameworks for LLMs to interact with tools and external systems.

---

## X. Explaining Transformer Reasoning (Slides 68-69)

### A. Where are Facts Stored in the Transformer? (Slide 69)

*   Ongoing research topic. Some key papers:
    *   **"Transformer Feed-Forward Layers Are Key-Value Memories"** (Geva et al., 2020)
        *   Suggests FFN layers store factual knowledge.
        *   [https://arxiv.org/pdf/2012.14913.pdf](https://arxiv.org/pdf/2012.14913.pdf)
    *   **"Locating and Editing Factual Associations in GPT"** (Meng et al., 2022)
        *   Investigates how to find and modify specific facts stored within model parameters.
        *   [https://arxiv.org/pdf/2202.05262.pdf](https://arxiv.org/pdf/2202.05262.pdf)
*   *(TO ADD: circuit discovery algorithms - this indicates an area for further exploration on how specific computations or knowledge paths are formed within the network).*

---

## XI. Conclusions on LLMs (Slides 70-71)

*   This lecture covered a lot of advanced material:
    *   **Large Language Models (LLMs):** Their scale and capabilities.
    *   **Prompting Techniques:** System prompts, zero/few-shot, Chain-of-Thought.
    *   **Scaling Behaviour:** How performance changes with model size, data, and compute (Scaling Laws, Chinchilla).
    *   **Retrieval Augmented Generative (RAG) Models:** Enhancing LLMs with external knowledge.
    *   **Architectural Improvements:** RoPE, GQA, MoE, longer contexts, etc.
    *   **Efficient Deployment:** Low-bit Quantization, LoRA.

---
```
