```markdown
# Lecture Notes: Speech Detection & Generation

**Course:** Natural Language Processing
**Lecturer:** Mark Carman
**Textbook Reference:** "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition" by Daniel Jurafsky and James H. Martin.

---

## Lecture Contents (Slide 2)

*   Human Voice
*   Mel Spectrogram
*   Speech-to-Text
*   Text-to-Speech

---

## I. Human Speech (Slides 3-5)

### A. Basics of Human Speech (Slide 4)

*   Human speech (see [Wikipedia: Speech](https://en.wikipedia.org/wiki/Speech)) consists of:
    *   **Vowels:** Sounds pronounced *without restricting* the vocal tract.
    *   **Consonants:** Sounds made by *partial or complete closing* of the vocal tract.
*   Different sounds that make up words are referred to as **phones/phonemes** (see [Wikipedia: Phoneme](https://en.wikipedia.org/wiki/Phoneme)).
*   *Video shows real-time MRI of speech production.*

### B. Source-Filter Model (Slide 5)

*   Model of human phonation:
    *   **Source:** Larynx/glottis produces pulses of air.
    *   **Filter:** Vocal tract shapes these pulses.
*   The source (glottal pulses) is not very important for speech recognition since the filter (vocal tract shape) carries most of the information that distinguishes different sounds.
*   *Diagram shows glottal pulses (source spectrum), vocal tract (filter function), and resulting speech signal (output energy spectrum).*

---

## II. Speech as a Time Series & Sound Properties (Slides 6-9)

### A. Speech as a Time Series (Slide 7)

*   Speech is a sound wave.
*   This is a time series of pressure values over time.
*   *Example waveform shown corresponds to somebody saying: "It’s time for lunch!"*

### B. Series of Sounds (Slide 8)

*   Specific types of sounds in speech:
    *   **Vowel:** Periodic signal (e.g., the /æ/ sound in *has*).
    *   **Fricatives:** Consonants produced by forcing air through a narrow channel (e.g., the /tʃ/ sound in *watch*).
    *   **Glides:** Smooth transition between sounds (e.g., the /w/ transitioning to /ɑː/ in *watch*).
    *   **Bursts (Plosives/Stops):** Rapid transition, release of air (e.g., the /d/ sound in *dime*).
*   *Diagram shows waveforms of "has a watch then a dime", highlighting segments corresponding to different sound types.*

### C. Sounds in Frequency Domain (Slide 9)

*   To distinguish different types of sounds:
    *   View them in the **frequency domain** using the Fourier Transform.
    *   This identifies the frequencies (and their amplitudes) that make up the signal.
*   *Diagram shows time domain waveforms and corresponding frequency domain spectra for vowel, fricative, glide, and stop sounds.*

---

## III. Converting Audio Signal into 2D Representation (Mel Spectrogram) (Slides 10-17)

### A. Spectrogram (is a 2D signal) (Slide 11)

*   An audio signal consists of a sequence of sounds.
*   To analyze this, we compute the frequency representation of **consecutive, short segments** of sounds in the sequence.
*   This 2D representation (frequency vs. time, with intensity/amplitude as the third dimension) is called a **spectrogram**.
*   *3D plot shows frequency, time, and amplitude.*

### B. Short-Time Fourier Transform (STFT) (Slide 12)

*   STFT finds frequency components (amplitude of sinusoids) in short signal sections (frames/chunks).
*   **Process:**
    1.  Divide the signal into `M` chunks (frames) of `L` samples each, `xₘ[]`. These chunks often overlap.
    2.  Multiply each chunk by a **window function** `w[]` (e.g., Hamming window) to reduce edge effects.
    3.  Compute the Fourier Transform (usually FFT) of each windowed chunk.
*   The STFT for the `m`-th chunk is: `STFTₘ[k]{xₘ} = Σ (n=0 to L-1) xₘ[n]w[n]e^(-j2πkn/L)`.

### C. Chunks & Windows (Slide 13)

*   **Overlap windows** to reduce border effects from windowing.
    *   Each chunk (frame) is typically around **25 ms**.
    *   Separated by a **hop length** (frame step) of typically **10 ms**.
*   Commonly used windowing function is the **Hamming window**:
    *   `w[n] = 0.54 - 0.46 * cos(2πn / L)` for `0 ≤ n ≤ L-1`.
    *   Further reduces border effects compared to a rectangular window.

### D. Spectrogram (2D View) (Slide 14)

*   Usually, a spectrogram is viewed as a 2D heatmap:
    *   X-axis: Time
    *   Y-axis: Frequency
    *   Color/Intensity: Magnitude (energy/amplitude) of frequency components (often in dB).
*   *Diagram shows a waveform and its corresponding spectrogram heatmap, highlighting one "chunk" or frame.*

### E. Pre-emphasis Filter (Slide 15)

*   Applied **before** running STFT.
*   The filter **amplifies high frequencies**.
*   **Useful for:**
    *   **Balancing spectrum:** High frequencies usually have smaller amplitudes.
    *   **Avoiding numerical problems:** When calculating Fourier transform.
    *   Possibly improving **signal-to-noise ratio (SNR)**.
*   Applied using a first-order filter: `output[t] = input[t] - α * input[t-1]`.
    *   Typical values for `α` are 0.95 or 0.97.

### F. Human Hearing (Slide 16)

*   Humans hear frequencies in the range: **20Hz to 20 kHz**.
    *   (Dogs and cats hear higher frequencies).
*   We distinguish noises based on their **relative pitch**.
    *   Piano keys increase pitch by a fixed multiple (2^(1/12) ≈ 1.0595 per semitone).
    *   Notes increase **multiplicatively**, not additively.
*   Humans hear many orders of **loudness**.
    *   So, amplitude is often represented on a **logarithmic scale** (e.g., decibels).

### G. Mel Spectrogram (Slide 17)

*   A spectrogram adapted to better reflect human auditory perception.
1.  **Limit frequency range:** Often to a maximum of ~8kHz (as most speech info is below this).
2.  **Represent frequencies on a logarithmic (Mel) scale:**
    *   The y-axis (frequency) increases with multiples of frequency based on the Mel scale, which models human perception of equidistant pitches.
    *   Mel scale formula: `mel = 2595 * log₁₀(1 + f/700)`.
3.  **Represent amplitude on a logarithmic scale:**
    *   Measure signal in **decibels (dB)**: `dB = 10 * log₁₀(power)` or `20 * log₁₀(amplitude)`.
*   *Diagrams show a basic spectrogram vs. a Mel spectrogram, with the Mel scale emphasizing lower frequencies more, similar to human hearing.*
*   (For more info: [Wikipedia: Mel scale](https://en.wikipedia.org/wiki/Mel_scale), [Ketan Doshi's blog post](https://ketanhdoshi.github.io/Audio-Mel/)).

---

## IV. Speech-to-Text (aka Automatic Speech Recognition - ASR) (Slides 18-27)

### A. Speech Recognition Problem (Slide 19)

*   The problem of converting an audio signal (waveform) into text.
    *   Example: Waveform -> "I want my mommy!"
*   **Traditional Approach (until recently):**
    1.  Extract features from the Mel Spectrogram, such as **Mel-Frequency Cepstrum Coefficients (MFCCs)**.
    2.  Use **Hidden Markov Models (HMMs)** with **Gaussian Mixture Models (GMMs)** to model phoneme sequences and their acoustic realizations.
*   Then **Deep Learning** came along and significantly changed ASR.

### B. Problems with (Simple) Classifiers for ASR (Slide 20)

*   If we train a Convolutional Neural Network (CNN) model to detect phonemes (either directly on the waveform or on the Mel Spectrogram):
    *   The classifier produces predictions over a **fixed window** of input.
*   **Problem:** Need to work out how many input windows correspond to the same phoneme/letter, as phonemes have variable durations.
    *   Alignment issue: Was the word "diner" (one /n/) or "dinner" (two /n/s, or a longer /n/)?
    *   *Diagram shows input `X` (frames) mapped to an alignment `A` of phonemes, where multiple frames can map to the same phoneme label.*

### C. Use Sequence-to-Sequence (Seq2Seq) Model (Slide 21)

*   **Solution:** Use an Encoder-Decoder model.
*   Deals with the problem of producing an output sequence (text) that isn't the same length as the input sequence (audio frames).
*   *Diagram shows: Audio waveform -> Feature Computation (e.g., Mel Spectrogram) -> Subsampling -> Encoder (RNN/Transformer) -> Encoded Representation (H) -> Decoder (RNN/Transformer) -> Output Text ("it's time").*

### D. Wav2vec (2020) (Slide 22)

*   Powerful recent **Transformer-based** architecture for ASR.
*   Works with **raw audio** as input (time series representation).
*   **Architecture:**
    1.  Uses a **Convolutional Neural Network (CNN)** to produce initial embeddings from short segments of the raw audio.
    2.  These embeddings are then processed by a **Transformer** encoder.
    3.  Often trained with self-supervised learning (e.g., contrastive loss on masked portions of speech).

### E. Whisper (2022) (Slide 23-24)

*   Even more recent Transformer-based system with state-of-the-art performance (from OpenAI).
*   Makes use of **Mel spectrogram** as input representation.
*   Similar architecture to a Vision Transformer (treating spectrogram as an image).
*   **Key Features (Slide 24):**
    *   **Weakly supervised:** Trained on a large, diverse dataset (680k hours) of audio and (sometimes noisy) transcripts, allowing for bigger training sets.
    *   **Multi-lingual:** Handles multiple languages.
    *   **Multi-task training:** Can perform transcription, translation, voice activity detection, etc.
    *   Uses special tokens to indicate task, language, and timestamps.

### F. Evaluating Speech-to-Text (Slide 25)

*   How to evaluate an ASR system?
    *   **Word Error Rate (WER):**
        *   Measures how many detected words differ from the correct words in a reference transcript.
        *   Based on **edit distance** (Levenshtein distance): `WER = 100 * (Insertions + Substitutions + Deletions) / (Total words in correct transcript)`.
    *   **Sentence Error Rate (SER):**
        *   Measures how many sentences had at least one error.
        *   `SER = 100 * (# of sentences with at least one error) / (Total number of sentences)`.

### G. Advanced ASR: Speaker Dependence (Slide 26)

*   Methods for personalizing speech-to-text for specific speakers:
    *   **Vocal Tract Length Normalization (VTLN):**
        *   Warps the frequency axis of the speech power spectrum.
        *   Accounts for the fact that vocal-tract resonances vary with the physical size of a speaker's vocal tract.
        *   Pitch normalization based on speaker's age may improve accuracy for children.
    *   **Modify Acoustic Model (Speaker Adaptation):**
        *   Start with a speaker-independent trained model.
        *   Fine-tune/adapt it using a small dataset from the new speaker.
        *   Learn a transform to maximize performance for that specific speaker.

### H. Advanced ASR: Non-words (Slide 27)

*   Dealing with non-word sounds:
    *   **Non-words include:**
        *   Short non-verbal sounds (coughs, loud breathing, throat clearing) -> often correspond to filled pauses (e.g., `um, uh`).
        *   Environmental sounds (beeps, telephone rings, door slams).
    *   **Handling:**
        1.  For each non-verbal sound: create a special phone representation.
        2.  Add a special word to the lexicon for that phone.
        3.  Use normal training to train these phones:
            *   Training data transcripts need to include labels for these new special words.
            *   Special words must be added to the language model component of the ASR system.

---

## V. Text-to-Speech (TTS) (aka Speech Synthesis) (Slides 28-34)

### A. Introduction (Slide 29)

*   As with Speech-to-Text, Text-to-Speech technology has improved significantly in recent years.
*   **Aim:** Convert a text string into an audio waveform.
*   **Often implemented as a 3-stage system:**
    1.  **Text to Phoneme:** Convert input text to a sequence of phonemes.
    2.  **Phoneme to Mel Spectrogram:** Predict a Mel spectrogram from the phoneme sequence.
    3.  **Mel Spectrogram to Audio Signal (Vocoder):** Synthesize the audio waveform from the Mel spectrogram.
*   *Example: "I want my mommy!" -> /aɪ wɒnt maɪ ˈmɒmi!/ -> Mel Spectrogram -> Audio Waveform.*

### B. Text Normalization (Slides 30-31)

*   Abbreviated text often needs to be **expanded** during the TTS process.
*   **Example (Slide 30):** "They live at 224 Mission St." -> "They live at *two twenty four* Mission Street."
*   Normalization process **depends on context**:
    *   Number 1750:
        *   "The economy in the year 1750" -> "...seventeen fifty"
        *   "The password is 1750" -> "...one seven five zero"
        *   "It costs 1750 dollars" -> "...one thousand seven hundred and fifty dollars"
    *   Can train a seq2seq model to expand text appropriately.
*   **Homograph Disambiguation (Slide 31):**
    *   English (and other languages) contain words written the same way but **pronounced in different ways** (homographs).
    *   Example: "bass"
        *   Musical instrument: /b eɪ s/
        *   Type of fish: /b æ s/
    *   Correct pronunciation needs to be determined from context.

### C. Tacotron2 (2018) (Slide 32)

*   An example text-to-speech architecture (from 2018).
*   Used an LSTM-based encoder-decoder to generate a Mel Spectrogram from input text (or phonemes).
*   **Encoder:** Processes input text characters/phonemes.
*   **Decoder (with attention):** Generates Mel Spectrogram frames.
*   **Post-Net:** Refines the Mel Spectrogram.
*   The Mel Spectrogram is then fed to a Vocoder.

### D. WaveNet (Vocoder) (Slide 33)

*   WaveNet is a **Vocoder** often used with models like Tacotron2.
*   Converts a Mel Spectrogram into an audio signal.
*   **Architecture:**
    *   Autoregressive **dilated convolution-based** generation of the signal.
    *   Dilated convolutions expand the kernel's receptive field by inserting spaces between kernel elements, allowing it to capture longer-range dependencies efficiently.

### E. Evaluation of Text-to-Speech (Slide 34)

*   Requires human testers, checking for:
    *   **Intelligibility:** Ability of the tester to correctly interpret the meaning of the utterance.
        *   Includes phone discrimination.
    *   **Quality:** Measure of naturalness, fluency, clarity of the speech.
        *   **Mean Opinion Score (MOS):** Testers score each system on a scale (e.g., 1-to-5).
        *   **AB test:** The same utterance is produced by two systems; testers choose the best system. Repeated for many utterances.

---

## VI. Some Older Techniques for Speech Synthesis (Slides 35-43)

*(These are more traditional, pre-deep learning approaches, often forming parts of the pipeline before modern end-to-end systems became dominant).*

### A. Overview (Slide 36)

*   **Goal:** Transform text string into waveform.
*   **Steps:**
    1.  **Text analysis:** Text string -> Phonetic representation (including prosody).
    2.  **Waveform synthesis:** Phonetic representation -> Waveform.
*   **Approaches for waveform synthesis:**
    *   **Formant synthesis:** Use an acoustic model of formants (resonant frequencies of the vocal tract) & additive synthesis -> results in "robotic" voice.
    *   **Articulatory synthesis:** Simulate movements of articulators and acoustics of vocal tract -> complex.
    *   **Concatenative synthesis:** Concatenate small pre-recorded wave units -> most used traditional approach.

### B. Phonetic Analysis (Slide 37)

*   Convert words into a list of phonemes.
*   **Dictionary-based conversion:** Use a pronunciation dictionary.
*   **Grapheme-to-Phoneme (g2p) conversion:** Train a classifier (or rule-based system) for handling names and other unknown words.
*   For **transparent languages** (e.g., Italian), use pronunciation rules, with a dictionary for irregular forms and foreign words.

### C. Prosodic Analysis (Slides 38-40)

*   **Prosody:** Intonation, stress, and rhythm of speech.
    *   Involves changes in pitch (F0), phoneme duration, and energy.
*   **Utterances have prosodic structure (Slide 38):**
    *   **Intonation phrases** and **intermediate phrases**.
    *   Boundaries can be found using classifiers.
    *   Example: "I wanted to go to London, | but could only get tickets for France." ( | indicates phrase boundary).
*   **Prosodic Prominence (Stress) (Slide 39):**
    *   Some words are stressed and made prominent -> "pitch accent".
    *   Realized by pitch, rhythm, and/or energy.
    *   Different accent levels: empathic, normal, reduced.
    *   **Tune:** Rise or fall of base frequency (F0) contour.
    *   *Example: Emphasis on "mean" in "you know what I mean" can indicate a question.*
*   **Phoneme Duration (Slide 40):**
    *   Rules change typical duration based on context (Klatt, 1979):
        1.  Pre-pausal lengthening: Vowel before a pause is lengthened.
        2.  Non-phrase-final shortening: Segments not at the end of a phrase are shortened.
        3.  Klatt’s formula combines factors for duration `d = d_min + Π fᵢ × (d̄ - d_min)`.
    *   Alternatively, use Machine Learning to regress phoneme duration.

### D. Waveform Synthesis (Concatenative) (Slide 41)

*   Two main models for concatenative waveform synthesis:
    *   **Diphone synthesis.**
    *   **Unit selection synthesis.**
*   *Diagram shows pipeline: Text -> Text Normalization -> Phonetic Analysis (+Dictionary/Rules) -> Prosodic Analysis -> Waveform Synthesis (+Diphone/Unit DB) -> Audio.*

### E. Diphone Synthesis (Slide 42)

*   **Diphone:** A phone-like unit from the middle of one phone to the middle of the next.
    *   Captures the transition between two phones.
    *   Addresses **coarticulation** (each phone differs slightly depending on preceding/following phones).
*   **Diphone database:** A collection of recorded diphones.
    *   Record a speaker saying one example of each diphone.
    *   Mark boundaries and cut each one out.
*   **Synthesizing an utterance:**
    1.  Grab the relevant sequence of diphones from the database.
    2.  Concatenate the diphones; clean boundaries (smooth transitions).
    3.  Use signal processing to change/impose prosody (pitch, duration).
*   *Example: "ciao" -> /tʃ aʊ/ -> (<s>, tʃ), (tʃ, a), (a, ʊ), (ʊ, </s>)*

### F. Unit Selection Synthesis (Slide 43)

*   Database contains **units** of various sizes: any piece of speech that can be concatenated (diphones, syllables, words, phrases).
*   Given a list of target phonemes with prosodic annotation, find the best list of units from the database.
*   This involves a **cost function:**
    *   **"Target cost":** Closest match to the target description in terms of:
        *   Phonetic context.
        *   F0 (pitch), stress, phrase position.
    *   **"Join cost":** Best join with neighboring units in terms of:
        *   Matching formants + other spectral characteristics.
        *   Matching energy.
        *   Matching F0.
*   The optimal sequence of units is often found using Viterbi search or beam search.

---

## VII. Conclusions (Slides 44-45)

*   In this lecture, we discussed the basics of:
    *   **Voice signals** and their representation as **Mel Spectrograms**.
    *   **Text-to-Speech (TTS) Systems**, including traditional concatenative methods and modern neural approaches (like Tacotron2 + WaveNet).
    *   **Speech-to-Text (ASR) Systems**, from older HMM-based systems to modern Transformer-based architectures like Wav2vec and Whisper.

---
```
