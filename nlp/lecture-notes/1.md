Okay, here are the complete but concise lecture notes from the provided slides, in Markdown format:

---

# Lecture Notes: Introduction to NLP (Natural Language Processing)

**Course:** Natural Language Processing
**Institution:** Politecnico di Milano
**Lecturer:** Mark Carman
**Instructor:** Nicolò Brunello
**Date:** (Assumed: Start of Course)

---

## I. Welcome & Introduction (Slides 1-5)

*   **Welcome to Natural Language Processing!** (Slide 2)
*   **Lecturer: Mark Carman** (Slide 3)
    *   Associate Professor in DEIB (Dipartimento di Elettronica, Informazione e Bioingegneria)
    *   Email: `mark.carman@polimi.it`
    *   **Research Background:** Information Retrieval & statistical Natural Language Processing, Machine Learning & Data Science.
    *   **Applications:** Personalisation & Recommendation, Web Search, Social Media Analysis, Digital Forensics, Bioinformatics.
        *   Google Scholar: [https://scholar.google.com/citations?user=fcPONTQAAAAJ&hl=en](https://scholar.google.com/citations?user=fcPONTQAAAAJ&hl=en)
    *   **Teaching:** Data Science, Artificial Intelligence & NLP.
    *   **Note:** Encourages interactive classes and questions.
    *   *Favourite NLP Quote (Frederick Jelinek):* "Every time I fire a linguist, the performance of the speech recognizer goes up."
*   **Instructor: Nicolò Brunello** (Slide 4)
    *   PhD student in DEIB
    *   Email: `nicolo.brunello@polimi.it`
    *   **Research Background:** Large Language Modeling (LLM), Retrieval Augmented Generative (RAG) models, eXplainable AI (XAI), Bioinformatics.
        *   Google Scholar: [https://scholar.google.com/citations?user=wgnP67kAAAAJ&hl=en](https://scholar.google.com/citations?user=wgnP67kAAAAJ&hl=en)
*   **Student Background Quiz:** (Slide 5)
    *   To understand student background in Engineering, Computer Science, Statistics.
    *   To gauge existing knowledge of Machine learning, Deep learning, NLP.
    *   Quiz Link: [https://forms.office.com/e/rWqJVhLPZ2](https://forms.office.com/e/rWqJVhLPZ2)
*   **Textbook Basis:**
    *   "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition" by Daniel Jurafsky and James H. Martin.
    *   Other content adapted from slides by Roberto Tedesco.

## II. What is Natural Language Processing? (Slides 6-7)

*   **Etymology (Origin of words):**
    *   **Natural:** (Latin: *natura* - birth, nature, quality) -> having a certain status by birth.
    *   **Language:** (Latin: *lingua* - tongue) -> Middle English: language.
    *   **Processing:** (Latin: *procedere* - progression, course) -> Middle English: proceed.
    *   Derived from: **birth, tongue, progress**.
*   **Relevance to NLP:**
    *   Natural languages are spoken (tongue).
    *   Meaning of words evolves over time (progress).
    *   Course Logo explanation.
*   **Course Focus:** Learning how to **process** natural (human) **language**.

## III. What is Natural Language? (Slide 8)

*   Many natural **animal languages** exist (monkeys, dolphins, birds).
*   Some are sophisticated:
    *   Monkeys: Distinct alarm calls for different threats.
    *   Dolphins: Sounds for hunting/social activities (statistical analysis suggests ~125 different whistles).
        *   Ref: [Inside Science on animal language size](https://www.insidescience.org/news/information-theory-counts-size-animal-languages)
    *   Birds: Sing to communicate; parrots can mimic human sounds.
*   Forms of animal communication are **symbolic** (sound represents object/action).
*   **Key Difference from Human Language:** No evidence for **compositionality** in animal languages.
*   Further Reading: "Q&A: What is human language, when did it evolve and why should we care?": [BMC Biology Article](https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-017-0405-3#)

## IV. Development of Human Language (Slide 9)

*   **Spoken Language:**
    *   Debated development: 2.5 million to 60 thousand years ago.
    *   Depends on definition of "human language."
    *   Ref: [Wikipedia - Origin of language](https://en.wikipedia.org/wiki/Origin_of_language)
*   **Written Language:**
    *   Origin clearer: First systems in Mesopotamia (Iraq) around 3500 BCE.
    *   Progression: **Pictograms** (representing objects) -> **Abstract symbols** (representing sounds).
    *   Example: Early pictographic writing (3500 BCE) evolving to Cuneiform.
    *   Ref: [Wikipedia - Cuneiform](https://en.wikipedia.org/wiki/Cuneiform)

## V. What is Special About Human Language? (Slide 10)

*   **Compositional:**
    *   Express thoughts in sentences: subjects, verbs, objects (e.g., `<I> <walk> <the dog>`).
    *   Endless capacity for new sentences (e.g., 100 words for each role can result in a million sentences).
*   **Referential:**
    *   Can express information about objects and their locations/actions.
*   **Temporal:**
    *   Conveys past, present, and future tenses.
*   **Varied:**
    *   Thousands of different languages spoken worldwide.
*   Source: Mark Pagel's article (link provided on previous slide).

## VI. Text Data (Slides 11-12)

*   **NLP mainly deals with Text data:**
    *   Textual documents (in natural language).
*   **Other types of data NLP techniques can handle:**
    *   Semi-structured data (e.g., HTML)
    *   Programming code
    *   Tabular (relational) data
    *   Biological sequences (e.g., genomic data)
    *   Binary data (e.g., malware executables)
    *   Audio data (e.g., speech signals)
    *   Other time series
    *   Images & video
*   **Key Insight:** NLP techniques are useful for handling these other types of data too.

## VII. What Can We Do With Text? (Slides 13-14)

*   **Natural Language Processing (NLP) / Text Mining:**
    *   The process of working with & extracting useful knowledge from textual data.
*   **Tasks with Text Data:**
    *   Visualize, fact-check, categorize, correct, attribute authorship
    *   Cluster, discover relationships, extract entities, identify sentiment
    *   Translate, summarize, disambiguate, generate, transcribe, search
*   **Impact of Deep Learning:** Revolutionized performance on all these tasks in recent years.
*   **Example: Text Generation** (Slide 14)
    *   Modern text generation is very good.
    *   Example: Two abstracts provided – one human-written, one AI-generated (conditioned on title).
        *   The *first* abstract shown was automatically generated.
    *   Try GPT-2 Text Generator: [https://transformer.huggingface.co/](https://transformer.huggingface.co/)

## VIII. Course Content (Slides 15-16)

*   **NLP Concerns:** Computational analysis, interpretation, and production of natural language (written or spoken).
*   **NLP Techniques Covered:**
    *   Regular expressions
    *   Vector space representations & text classification
    *   Text retrieval and text clustering
    *   Word embedding based representations
    *   Language models for text generation
    *   Sequence-to-sequence models and Transformers
    *   Dialog systems (task-oriented and retrieval-augmented chatbots)
    *   Large Language Models (LLMs)
    *   Audio aspects: speech-to-text and text-to-speech
*   **Practical Sessions (Building NLP Applications):**
    *   Sentiment analysis, retrieval, summarisation, translation, named entity extraction, question answering, chatbots, personal assistants.
*   **Note:** Final course program will depend on student background & time constraints.

## IX. Course Material (Slides 17-18)

*   **Slides:** Available on Webeep: [https://webeep.polimi.it/my/](https://webeep.polimi.it/my/)
*   **Main Textbook:**
    *   *Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition* by Daniel Jurafsky and James H. Martin.
    *   Draft of 3rd Edition: [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/)
*   **Additional Textbook (Basic Machine Learning):**
    *   *The Elements of Statistical Learning: Data Mining, Inference, and Prediction* by Trevor Hastie, Robert Tibshirani & Jerome Friedman.
    *   Second Edition online: [https://web.stanford.edu/~hastie/ElemStatLearn/](https://web.stanford.edu/~hastie/ElemStatLearn/)
*   **Additional Textbook (Deep Learning):**
    *   *Deep Learning* by Ian Goodfellow, Yoshua Bengio & Aaron Courville.
    *   Available online: [http://www.deeplearningbook.org](http://www.deeplearningbook.org)
    *   **Caveat:** Written in 2016, pre-dating recent Transformer techniques.

## X. Assessment (Slides 19-20)

*   **Assignment:**
    *   Worth 40% of grade.
    *   Work in groups on a fun NLP project (details later).
*   **Written (or Oral) Exam:**
    *   Worth 60% of grade.
    *   Aimed to be engaging.
    *   **Tip:** Pay attention during lectures.

## XI. Text Processing Lecture Overview (Slides 21-22)

*   Why process text?
*   What can be done with text?
*   Text processing is hard.
*   Brief history of NLP.
*   Pre-processing Text.
*   Regular Expressions.

## XII. Why Process Text? (Slide 23)

*   **Text is Pervasive:**
    *   Found in personal communications, news, finance, law, literature, scientific publications.
    *   Techniques for automatic processing are critical.
*   **Text is Important:**
    *   Can influence public opinion (e.g., social media, news).
    *   Can lead to scientific discoveries (e.g., mining research literature).

## XIII. What Tasks Can Be Done With Text? (Slides 24-28)

*   **General Tasks (Slide 24):**
    *   **Classifying it:** (e.g., fact-checking "Donald Trump won the 2020 US Presidential Election? -> FALSE").
    *   **Searching it:** (e.g., "Where is the world's biggest banana? -> island of New Guinea").
    *   **Clustering it:** (e.g., grouping news articles on the same topic).
    *   **Translating it:** (e.g., "Signs you like your dog more than your family" -> Chinese translation).
    *   **Extracting information from it.**
*   **Example NLP Tasks in Medical Domain (Slides 25-27):**
    *   **Medical Text Classification:**
        *   Label documents (procedure, diagnosis, motivation, billing code).
        *   Predict patient outcome (e.g., re-admission risk).
    *   **Medical Data Extraction:**
        *   Extracting entities (diagnostic tests).
        *   Linking entities (reconcile drug names).
        *   Relation extraction (determine drug dosage).
        *   Event detection (drug administered on X date).
    *   **Disambiguation:**
        *   Expanding abbreviations (e.g., "MR" -> magnetic resonance OR mitral regurgitation).
    *   **Patient Similarity Search:**
        *   Find most similar patient for diagnosis or cohort selection.
    *   **Text Generation (Medical):**
        *   **Translation:** Medical jargon to plain language for patient consumption.
        *   **Summarisation:** Of patient medical history or related literature.
        *   **Anonymisation & Synthetic Data Generation:** Remove sensitive info or create synthetic datasets.
        *   **Question Answering:** Answer medical questions based on text in EHR.
        *   **Explanations:** Explain model predictions/diagnoses.
*   **Example: Data Extraction from Discharge Letters (Slide 28)**
    *   **Goal:** Extract prescription information (drug dosage, diagnosis, appointment).
    *   **Challenge:** Information often only in unstructured discharge letters, requiring manual search.
    *   **Example Extraction:**
        *   Drug: Prinivil
        *   Dosage: 5mg per day
        *   Regimen: daily
        *   Duration: unknown

## XIV. How Hard is Processing Natural Language? (NLP is Difficult) (Slides 29-33)

*   **Extremely Expressive (Slide 30):**
    *   Most human knowledge is recorded in books.
    *   One can say *anything* in natural language.
    *   Even nonsensical (but grammatically correct) statements: "Colorless green ideas sleep furiously." (Noam Chomsky)
    *   Logically inconsistent but meaningful: "I didn't just say what I just said."
*   **Highly Ambiguous (Slide 31):**
    *   Resolving ambiguity is a fundamental problem of computational linguistics.
    *   Example: "I made her duck."
        *   *Lexical Category:* "duck" (noun/verb?), "her" (possessive/dative pronoun?)
        *   *Lexical Semantics:* "make" (create/cook?)
        *   *Grammar:* "make" (transitive, ditransitive, action-transitive?)
        *   Possible meanings: lower her head, cook the bird, build a statue, magically transform her.
*   **Often Very Redundant (Slide 32):**
    *   Example: "I'm a massive fan of Britney Spears!" (Many ways to say/misspell).
    *   Google's job search example for "Britney": [http://archive.google.com/jobs/britney.html](http://archive.google.com/jobs/britney.html)
*   **Prosody Matters (Slide 33):**
    *   The way text is pronounced/emphasized affects meaning.
    *   Example: "I never said she stole my money."
        *   **I** never said... (Someone else did)
        *   I **never** said... (I didn't say it)
        *   I never **said**... (I only implied it)
        *   I never said **she**... (Someone else did, not her)
        *   I never said she **stole**... (I considered it borrowed)
        *   I never said she stole **my**... (She stole something of mine)
        *   I never said she stole my **money**. (Only that she stole money)

## XV. Very Brief History of NLP (Slides 34-35)

*   **Origins:** Linguistics, Computer Science, Speech Recognition & Psychology.
*   **1940-1950 (WWII):**
    *   Finite State Automata, Formal Language Theory, Probabilistic algorithms for speech, Information Theory (Shannon).
*   **1957-1970 (Two Paradigms):**
    *   **Symbolic:** Formal Language Theory, AI Logic Theories.
    *   **Stochastic:** Bayesian methods, dictionaries, corpora (first OCR, Brown Corpus).
*   **1970-1993 (Empiricism and Finite-State Models):**
    *   Understanding semantics, discourse modeling.
    *   Probabilistic speech recognition (@IBM), data-driven POS tagging, parsing, ambiguity resolution, NLG.
*   **1994-1999 (Decline of Symbolic Approach):**
    *   Heavy use of data-driven methods, new web applications.
*   **2000-2010 (Empiricism and Machine Learning):**
    *   Large annotated online material, ML+HPC community, rise of unsupervised systems.
*   **2010-2018 (Machine Learning Everywhere):**
    *   Neural Networks for NLP, Conversational Agents, Subjectivity & Sentiment Analysis.
*   **2018-... (Transformer Architectures):**
    *   Transfer learning, pretrained language models, massive online language models.

## XVI. Current Technology & Chatbots (Slides 36-38)

*   **Current Technology is "amaaaaaazzzziiing!!!!"** (Slide 36)
*   **Chatbots for Search (Slide 37):**
    *   Latest Language Models are incredibly good at conversation.
    *   Microsoft and Google are using chatbots to power/extend search interfaces (e.g., Bing vs. Google AI search).
*   **Aside: Anthropomorphism (Slide 38):**
    *   **Definition:** Ascribing human emotions to non-human entities.
    *   May become a problem as chatbots improve.
    *   People are already worried about the "emotional state" of Bing Search, reporting "unhinged" or "argumentative" behavior.

## XVII. Preprocessing Text (Slides 39-59)

### A. Overview (Slide 40)
*   Common to clean text before further processing.
*   **Prior to tokenization:**
    *   Remove mark-up (e.g., `<HTML>` tags).
    *   Lowercase text (can lose information, e.g., 'WHO' vs 'who').
    *   Remove punctuation (e.g., `;, &%$@!><`).
*   **After tokenization:**
    *   Remove stopwords (extremely high-frequency words).
    *   Remove low-frequency words.
    *   Perform stemming or lemmatization (reduce vocabulary size).
    *   Perform spelling correction.

### B. Extracting Plain Text (Slides 41-42)
*   From **textual documents** (.txt, HTML, e-mail):
    *   Usually discard mark-up and format-specific commands.
    *   Web crawl parsers need robustness to badly formed HTML.
*   From **binary documents** (Word, PDF, etc.):
    *   More complex to handle.
    *   PDFs: structure reconstruction needed (e.g., identify columns for correct text flow). Rules or ML may be needed.
*   From **images of scanned documents:**
    *   Requires specialized Optical Character Recognition (OCR) software (often deep learning-based).
    *   OCR is not perfect; may introduce recognition errors.

### C. Text Encodings (Slide 43)
*   **ASCII:** Traditional keyboard, 128 characters total.
    *   'A' -> 65, 'B' -> 66, ..., 'a' -> 97, 'b' -> 98.
*   **UTF-8 (Unicode):**
    *   Handles 149k+ Unicode characters.
    *   Works for 160+ languages.
    *   Link: [https://en.wikipedia.org/wiki/Unicode](https://en.wikipedia.org/wiki/Unicode)
*   **Why Unicode?**
    *   Non-latin character sets (Arabic, Cyrillic, Greek, Devanagari).
    *   Special characters (diacritics, e.g., Italian "Questa è così", English "Naïve").

### D. Tokenizing Text (Slides 44-49)
*   **Tokenization:** Segmenting text into sequences of characters called **tokens**.
    *   Usually involves splitting sentences into **words**.
    *   Sometimes character-level tokenization.
    *   Often requires language-specific resources (e.g., for Chinese).
*   **Space-based Tokenization (Slide 45):**
    *   Many languages use spaces between words (Arabic, Cyrillic, Greek, Latin).
    *   Segment based on white-space characters.
    *   Note: `notethatitispossibletoreadasentenceinenglishwithoutspacesbetweenwords`
*   **Issues with Space-based Tokenization (Slide 46-47):**
    *   **Hyphenated words:** "Italian-style furniture” -> "Italian", "style", "furniture"?
    *   **Agglutinative languages:** Long words (e.g., German `Unabhängigkeitserklärung`, Italian `incontrovertibilissimamente`).
    *   **Non-hyphenated multi-word units:** "New York" -> `New_York`.
    *   **Punctuation:** Can't blindly remove (e.g., m.p.h., $45.55, URLs, #nlproc, emails).
    *   **Clitics:** Words that don't stand on their own (e.g., "we're", French "j'ai", "l'honneur").
*   **Tokenization in Languages Without Spaces (e.g., Chinese) (Slide 48):**
    *   Chinese words average 2.4 characters; word boundaries difficult.
    *   Example: "姚明进入总决赛" (Yao Ming reaches the finals) has multiple potential tokenizations.
    *   Common to treat each character as a token.
    *   Thai & Japanese require complex segmentation.
*   **Other Options for Text Tokenization (Slide 49):**
    *   Use the data to learn how to tokenize.
    *   **Sub-word tokenization:** Useful for splitting longer words, allows ML model to learn morphology.
    *   **Byte-pair encoding (BPE):** Will be covered later in Deep Learning.

### E. Sentence Segmentation (Slide 50)
*   Certain tasks require sentences to be segmented.
*   Punctuation marks "!" and "?" often indicate end of statement/question.
    *   Exceptions: math "5! = 120", unknowns "Fill in values: 1, 2, ?, ...".
*   Period "." is commonly used but ambiguous.
    *   Appears in abbreviations (Inc., Dr.) and numbers (.02%, 4.3).
*   **Common Algorithm:** Tokenize, then use rules or ML to classify a period as (a) part of the word or (b) a sentence-boundary.

### F. Normalizing Text (Word Normalization) (Slides 51-57)
*   **Word Normalization (Slide 52):**
    *   Putting words/tokens into a standard format.
    *   Examples: U.S.A. or USA?; uhhuh or uh-huh?; Fed or fed?; am, is, be, are?
    *   Critical for lexical search engines (query "USA" should match "U.S.A.").
*   **Case Folding (Slide 53):**
    *   Reducing all letters to lower case (e.g., "ABBA" -> "abba").
    *   **Web Search:** Reduces vocabulary size, increases recall, users often use lowercase.
    *   **Classification:** Reduces vocabulary/parameters, helps generalization.
    *   **Problem:** Can lose important information (e.g., "WHO" (World Health Org) vs "who" (pronoun); "US" (United States) vs "us" (pronoun)).
    *   **Retaining Case:** Helpful for sentiment analysis, Machine Translation, Information Extraction.
*   **Morphology & Morphemes (Slides 54-55):**
    *   **Morphology:** Analysis of the structure of words in a language.
    *   **Morpheme:** Smallest linguistic unit with semantic meaning.
        *   Example: `unbelievably` -> `un-believe-able-ly`.
        *   Divided into:
            *   **Root:** The base form (e.g., `believe`).
            *   **Affixes:** `prefix` (un-), `infix` (-able-), `suffix` (-ly).
    *   Some languages have complex morphology (e.g., Turkish `Uygarlastiramadiklarimizdanmissinizcasina`).
*   **Lemmatization (Slide 56):**
    *   Represent all words as their **lemma** (shared root, dictionary headword form).
    *   Examples: `am, are, is` -> `be`; `car, cars, car's` -> `car`; Italian `voglio` ('I want') -> `volere` ('to want').
    *   Sentence example: "He is reading detective stories" -> "He be read detective story".
    *   Requires a **lexicon** (rules for morphology, irregular word mappings).
*   **Stemming (Slide 57):**
    *   Simpler algorithm; reduces terms to stems by removing affixes.
    *   **No lexicon needed.**
    *   Often used in text retrieval to reduce computational requirements.
    *   Example: Porter Stemming Algorithm (from 1980).
        *   Rewriting rules: `+ATIONAL` -> `+ATE` (e.g., `relational` -> `relate`).
        *   `+ING` -> `ε` (e.g., `motoring` -> `motor`).
    *   Simple, but can have **collisions** (different words -> same stem): `police` -> `polic`; `policize` -> `polic`.

### G. Stopword Removal (Slides 58-59)
*   **Stopwords:** Most frequent terms in a language (e.g., `the, be, to, of, and, a, in, that, have, I, it`).
    *   Have extremely high document frequency scores (low discriminative power).
    *   Convey very little information about the topic of the text.
*   **Benefits of Removal:**
    *   Can sometimes boost retrieval/classification performance.
    *   More likely to reduce computational/memory burden.
    *   Speeds up index by removing massively long posting lists.
*   **Problem:** Sometimes stopwords are useful.
    *   Example: Rockband "the The".
    *   Example: "a white house" vs "the white house".

## XVIII. Regular Expressions (Regex) (Slides 60-67)

*   **What are they? (Slide 61)**
    *   Patterns to **search** within text documents for specific **sequences of characters**.
    *   Used to:
        1.  Find out **whether a pattern exists** in a document.
        2.  **Extract information** from a document where the pattern occurs.
    *   Example: `[+-]?(\d+(\.\d+)?)` (matches numbers, possibly signed, with optional decimal part).
*   **Simple Examples (Slide 62):**
    *   **Exact match:** `'abc'`
        *   Matches: `'aaabcdddd'`
        *   Doesn't match: `'aaabdddd'`
    *   **Choice:** `'(abc|bdd)'`
        *   Matches: `'aaabcdddd'` and `'aaabdddd'`
*   **Wildcards & Square Brackets (Slide 63):**
    *   **`.` (Wildcard):** Matches any character (except newline).
        *   Example: `'a..d'` matches `'aaabcdddd'`.
    *   **`[]` (Square Brackets):** Choice for a single character.
        *   `[abc]` = `(a|b|c)`: any one of a, b, or c.
        *   `[a-z]`: any character in range a to z.
        *   `[^abc]`: any character *except* a, b, or c.
*   **Special Characters (prefixed with `\`) (Slide 64):**
    *   `\n`: newline character.
    *   `\t`: tab character.
    *   `\s`: any whitespace character.
    *   `\S`: any non-whitespace character.
    *   `\d`: any digit (`[0-9]`).
    *   `\w`: any 'word' character (`[a-zA-Z0-9]`).
*   **Repetition (Slide 65):**
    *   `*`: zero or more times.
    *   `+`: one or more times.
    *   `?`: zero or one time.
    *   `{n}`: exactly n times.
    *   `{n,m}`: at least n, up to m times.
    *   Example: `'ad*'`
        *   Matches `'aaaaadddddddcc'` (greedily matches longest sub-sequence).
        *   Matches `'aaaaacc'` ('d' appears zero times).
*   **More Complicated Example (Email Regex) (Slide 66):**
    *   `[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}`
    *   Matches: `'my email is Steve.Rogers@iamyourcaptain.com'`
    *   Matches: `'send jamesbond007@hermajestyssecretservice.co.uk a mail & wait for a reply'` (extracts the email part)
*   **Pros and Cons of Regular Expressions (Slide 67):**
    *   **Advantages:**
        *   Simplicity of approach.
        *   Rules can be made precise to reduce false positives.
    *   **Limitations:**
        *   Rules usually written by hand (difficult/laborious).
        *   Some false positives (due to insufficient syntactic structure identification).
            *   e.g., `productID 849302949` might be mistaken for a phone number.
        *   Often many false negatives (rule not general enough).
        *   Hard to integrate knowledge of context around the extracted entity.

## XIX. Tutorials in Python & Notebooks (Slides 68-69)

*   **Learning by Doing:** Course is practical by design, with less theory and more practical sessions.
*   **Jupyter Notebooks:**
    *   Will be used starting next session.
    *   Options:
        *   Install Anaconda: [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)
        *   Use Google Colab (free online): [https://colab.research.google.com/notebooks/intro.ipynb](https://colab.research.google.com/notebooks/intro.ipynb)
*   **Python:**
    *   All coding for the course will be in Python.
    *   Most important language for NLP/Deep Learning.
    *   If new to Python, recommended free online course: "Introduction to Python for Data Science" on Datacamp: [https://www.datacamp.com/courses/intro-to-python-for-data-science](https://www.datacamp.com/courses/intro-to-python-for-data-science)

## XX. Conclusions (Slides 70-71)

*   Natural language is **pervasive**; techniques for automatic processing are critical.
*   Natural language processing is **hard** due to unbounded expressivity and ambiguity.
*   Hand-written **regular expressions** provide a simple mechanism for data extraction from text documents.

---
