```markdown
# Lecture Notes: Applications Of Transformers

**Course:** Natural Language Processing
**Lecturer:** Mark Carman
**Textbook References:**
*   "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition" by Daniel Jurafsky and James H. Martin
*   "Machine Learning and Security: Protecting Systems with Data and Algorithms" by Clarence Chio & David Freeman

---

## Lecture Contents (Slide 2)

*   Fine-tuning BERT and GPT-2
*   Multi-task Learning
*   Document Embeddings
*   Vector Databases
*   Multi-modal Learning
*   Zero-shot Learning

---

## I. Reminder: 3 Possible Transformer Architectures (Slide 3)

*   **Encoder-Decoder Models:**
    *   Original Transformer (2017), designed for translation.
    *   Also T5 (Text-to-Text Transfer Transformer) model (2019).
    *   Useful if input and output text have **different vocabularies**.
*   **Encoder-Only Models:**
    *   E.g., BERT (Bidirectional Encoder Representations from Transformers) and RoBERTa.
    *   Pretrained as a noisy **auto-encoder** to recover **masked input**.
    *   Great for **representing text** (e.g., for building classifiers).
*   **Decoder-Only Models:**
    *   E.g., GPT-2 (Generative Pretrained Transformer).
    *   Pretrained as an **autoregressive** model to **predict next token**.
    *   Great for **generating text**.

---

## II. Fine-tuning Transformers to Perform Specific Tasks (Slides 4-6)

### A. Fine-tuning BERT for ... (Slide 5)

*   Bidirectional language models like BERT are very flexible.
*   **Text Classification:**
    *   Usually trained by fine-tuning to replace the output of the `[CLS]` token with the class label.
*   **Sequence Labelling (e.g., NER, POS tagging):**
    *   Fine-tuned by replacing the output text (from pre-training) with sequences of begin/inside/outside (BIO) labels or other per-token labels.
*   **Text Pair Classification (e.g., Natural Language Inference, Paraphrase Detection):**
    *   Fine-tuned by adding a special `[SEP]` token to separate two pieces of text.
    *   The `[CLS]` token output is then used to predict the relationship (e.g., agreement, similarity).
*   **Question-Answering:**
    *   BERT can be fine-tuned (e.g., predict start/end tokens of answer span).
    *   However, GPT (decoder-only) models are often more appropriate for generative QA.

### B. Fine-tuning GPT-2 for ... (Slide 6)

*   GPT-2's strength is **text generation**, making it suitable for:
    *   Translation
    *   Summarization
    *   Dialog systems, etc.
*   **During fine-tuning:**
    *   Introduce **special tokens** (or use text prompts) to separate input from output.
    *   And to indicate the type of output desired.
    *   **Examples:**
        *   **Translation:** `Input: I am a student <to-fr> Output: je suis étudiant`
        *   **Summarization:** `Input: Article #1 tokens <summarize> Output: Article #1 Summary`

---

## III. Multi-task Learning (Slides 7-8)

### A. The End of Task-Specific Models (Slide 8)

*   Realizing the task-independent power of Transformer language models (especially large ones like T5).
*   Researchers investigated fine-tuning them to perform **multiple tasks at once**.
*   **Finding:** A multi-task model often **outperformed** task-specific models trained only on a particular task.
*   This is often achieved by framing all tasks as text-to-text problems, using specific prompts to indicate the desired task.
    *   *Diagram shows T0 model handling summarization, sentiment analysis, NLI, QA etc., by using different prompts.*

---

## IV. Further Uses of BERT: Estimating Similarity between Documents (Slides 9-11)

### A. Learning to Rerank Documents (Slide 10)

*   BERT-based models can be used to **rerank documents** in web search.
*   **Process:**
    *   Fine-tune BERT to predict a **relevance label** (e.g., "highly relevant", "relevant", "not relevant", "spam").
    *   Input: `<query, document>` pairs.
    *   Use the `[SEP]` token to separate the query and document within the input to BERT.
    *   The `[CLS]` token's output is used for classification.

### B. Document Similarity (Slide 11)

*   To calculate similarity between documents (e.g., for clustering):
*   **Fine-tune BERT to estimate semantic similarity:**
    1.  Start with pairs of **similar documents** and randomly chosen pairs of (probably) **dissimilar documents**.
    2.  Fine-tune a **pairwise classifier** (BERT with `[CLS]` output and `[SEP]` between docs) to identify similar documents.
    3.  Use logits (or probability of the "similar" class label) as a **similarity score**.
*   **If ground truth similarity/distance information is available:**
    *   Could also fine-tune the model on a **regression task** to predict the similarity value directly.

---

## V. Further Uses of BERT: Document Embeddings (Slides 12-16)

### A. Problem: Computational Overhead (Slide 13)

*   Using a pairwise BERT classifier (like for reranking or document similarity) is:
    *   **Very powerful:** Leverages word order and semantics via embeddings.
    *   **Very costly:**
        *   Many matrix multiplications during inference.
        *   Needs GPU for speed, still takes considerable time per pair.
*   **Issue for large collections:** Computing a score for *every document* in the collection against a query is too slow for real-time.
    *   E.g., 1ms/doc, 1M docs -> >15 minutes per query.
*   **Goal:** Perform as much **precomputation** as possible, but similarity can't be fully precomputed without seeing the query.

### B. Solution 1: Lexical Search + Reranking (Slide 14)

*   **Two-stage approach:**
    1.  Use a fast lexical search engine (TF-IDF, BM25) to quickly find a small **candidate set** of documents.
    2.  Use a fine-tuned pairwise BERT classifier **only to rerank** this smaller set of candidate documents.

### C. Solution 2: Pre-compute Document Embeddings (Slide 15)

*   BERT can be trained to compute fixed-size embeddings for documents.
*   **Method:**
    1.  Use the output (contextual) embedding of the `[CLS]` token from BERT as the representation of each document.
    2.  Use the **dot-product** between embeddings of different documents as their similarity.
    3.  Train the model on pairs of similar and dissimilar documents using **"contrastive loss"**.
        *   Goal: Produce high similarity scores for similar document pairs and low scores for dissimilar pairs.
        *   Contrastive loss is often a classification loss (e.g., predict if pair is similar/dissimilar) applied after the dot product and a softmax/sigmoid.
*   **Context length restriction:** BERT's fixed input length (e.g., 512 tokens) might require comparing sections of longer documents and aggregating the results.

### D. Sentence BERT (SBERT) (Slide 16)

*   SBERT uses a BERT (or RoBERTa) model to learn vector representations for entire documents/sentences.
*   Employs **contrastive learning** to create an embedding space where similar documents have similar embeddings.
*   **Practice:**
    *   Often uses a **siamese or triplet network structure**: Train two BERT models (or share weights) that process two input sentences (or a query and a document).
    *   Use the dot-product (or other similarity measure like cosine) between the output `[CLS]` token embeddings to represent document similarity.
    *   Train on pairs of similar and dissimilar documents to produce high similarity values for similar pairs and low values for dissimilar ones.

---

## VI. Vector Databases (Slides 17-19)

### A. Vector Search & Databases (Slide 18)

*   **Vector Databases:** Index objects (text, images) based on their pre-computed embeddings.
*   Provide **fast nearest neighbor search** in the embedding space.
    *   Example: FAISS (Facebook AI Similarity Search).
*   **Nearest Neighbour Search Challenge:**
    *   Finding exact nearest neighbors in high-dimensional spaces is difficult (curse of dimensionality; vectors tend to be equidistant and nearly orthogonal).
*   **Solution:** Clever algorithms effectively **partition the space** into clusters to speed up search.
    *   Example: HNSW (Hierarchical Navigable Small Worlds).

### B. Approximate Nearest Neighbour (ANN) Search (Slide 19)

*   Exact NNS in high-D is hard (e.g., k-d trees break down, O(n) complexity).
*   **ANN search algorithms trade some accuracy for speed.**
*   **HNSW (Hierarchical Navigable Small Worlds):**
    *   **Navigable Small World graphs:** Nodes (data points) are connected to their nearest neighbors, allowing for quick graph traversal to find approximate neighbors.
    *   **Hierarchy of layers:** Graph is built in layers, with fewer nodes on higher layers. Search starts at a top layer and navigates down.
*   Implemented in libraries like FAISS.
*   (Further info: [https://medium.com/@myscale/understanding-vector-indexing-a-comprehensive-guide-d1abe36ccd3c](https://medium.com/@myscale/understanding-vector-indexing-a-comprehensive-guide-d1abe36ccd3c))

---

## VII. Multi-modal Models (Slides 20-22)

### A. CLIP (Contrastive Language–Image Pre-training) (Slide 21)

*   **Goal:** Align embedding spaces for text and images.
*   **Process:**
    1.  Generate embeddings for **text** (e.g., using SentenceTransformer/BERT).
    2.  Generate embeddings for **images** (e.g., using ResNet or VisionTransformer).
    3.  Use **contrastive learning** to force the two embedding spaces to agree:
        *   Take a set of `<image, text>` pairs (e.g., images and their captions).
        *   For a batch of such pairs, train a classifier to identify:
            *   Which piece of text describes which image.
            *   Which image describes which piece of text.
            *   (Essentially, make dot products of correct pairs high, and incorrect pairs low).
*   **Benefit of aligned embedding space:** Allows for semantic **image search using text queries**.
    *   *Example: Query "soccer fan elephant riding a bicycle" retrieves images matching that description.*

### B. Multimodal Learning with Transformers (Slide 22)

*   Transformer architecture is very flexible.
*   Relatively easy to extend text-to-text models to **multimodal (text+image, etc.)** settings.
*   Allows for learning of tasks across all media.
*   *Example (OFA model):* A unified model handling tasks like Visual Question Answering (VQA), Image Captioning, Text Infilling by representing all inputs and outputs as sequences.*

---

## VIII. Further Uses of GPT: Zero, One and Few-shot Learning (Slides 23-29)

### A. GPT Without Fine-tuning (Slide 24)

*   Large Language Models (LLMs) are **universal learners** and can be used even **without fine-tuning** for specific tasks.
*   **Zero-shot learning:**
    *   Describe the task that the model needs to perform directly in the prompt.
    *   No gradient updates are performed.
    *   *Example prompt: "Translate English to French: cheese =>"*
*   **Few-shot learning (or One-shot):**
    *   Provide a few examples (or one example) of the task within the prompt.
    *   No gradient updates are performed.
    *   *Example prompt (One-shot): "Translate English to French: sea otter => loutre de mer cheese =>"*

### B. How is Few-shot Learning Possible? (Slide 25)

*   The model has seen **lots of examples of few-shot learning implicitly during pre-training**.
*   The vast pre-training data contains many instances of tasks being demonstrated or explained, which the LLM learns to recognize and perform ("in-context learning").
    *   *Diagram shows examples of arithmetic, translation, and common sense tasks formatted as few-shot prompts, implying the model learned this pattern during unsupervised pre-training.*

### C. Many Tasks Handled by Zero/Few-shot Learning (Slide 26)

*   Language models are the "Swiss army knives" of NLP.
*   Text generation enables diverse functionality:
    *   **Translation:** Provide examples like `source_language_text = target_language_text`, then prompt with `sentence_to_translate =`.
    *   **Question Answering:** Prompt with the question, or formulate as a statement to complete (e.g., "The height of the Eiffel Tower in metres is").
    *   **Reading Comprehension:** Provide text and examples of (question, answer) pairs, then prompt with an unanswered question.
    *   **Summarization:** Provide content to be summarized and prefix the desired response with a prompt like `"tl;dr:"`.

### D. GPT-2 Examples (Slides 27-29)

*   **Question Answering (Slide 27):**
    *   GPT-2 stored many facts in its "parametric knowledge" and could answer questions.
    *   Confident predictions were often correct, though not always as reliable as dedicated QA systems at the time.
    *   **Note:** The system was **not explicitly trained** to do QA; this was an emergent capability from language modeling pre-training.
*   **Reading Comprehension (Slide 28):**
    *   Provide source context (document containing answers).
    *   For few-shot learning, provide examples of (question, answer) pairs based on the context.
    *   Provide a new question and prompt the model for an answer.
    *   **Note:** This general scheme can be used for tasks like **fact checking**, where potential evidence is first retrieved as context.
*   **Translation (Slide 29):**
    *   GPT-2 worked out-of-the-box as a machine translation system, despite not being trained for it.
    *   It was trained primarily on an ENGLISH corpus.
    *   Ability to "speak" French (or other languages) came from **fragments of those languages hidden in the vast training data**.

---

## IX. Conclusions (Slides 30-31)

*   MANY applications of Transformer Architecture.
*   **Key applications/concepts covered:**
    *   Fine-tuning BERT (encoder) and GPT-2 (decoder) for various tasks.
    *   Multi-task Learning (training one model for many tasks).
    *   Document Embeddings (representing documents as vectors using Transformers).
    *   Vector Databases (for efficient similarity search of embeddings).
    *   Multi-modal Learning (extending Transformers to handle text + other media like images).
    *   Zero-shot and Few-shot Learning (using LLMs without fine-tuning by prompting).

---
```
