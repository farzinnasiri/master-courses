```markdown
# Lecture Notes: Language Models and Word Embeddings

**Course:** Natural Language Processing
**Lecturer:** Mark Carman
**Date:** 10 March 2025
**Textbook Reference:** "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition" by Daniel Jurafsky and James H. Martin.

---

## Lecture Content (Slide 2)

1.  What is language modelling?
2.  Markov models
3.  Evaluating language models
4.  Problems with Markov models
5.  Revision: Neural Networks
6.  Word embeddings
7.  Properties of word embeddings
8.  Applications of word embeddings
9.  Extensions

---

## I. What is Language Modeling? (Slides 3-6)

### A. Introduction (Slide 4)

*   **Language Modeling:** Assigning a probability to a sequence of words. Reflects how likely a sequence is.
    *   Example: P("why does that kid have a bike?") > P("why does that kid have a moustache?") > P("a moustache why that kid does have?")
*   A **statistical language model (LM)** is a **probability distribution over sequences of words**.
*   Given a distribution `P(w₁, w₂, ..., wₙ)`:
    *   We can **condition** the next word on previous words: `P(wₙ|w₁, ..., wₙ₋₁) = P(w₁, ..., wₙ) / P(w₁, ..., wₙ₋₁)`.
    *   And **sample new sequences** from it: `w* ~ P(w|w₁, ..., wₙ₋₁)`.
*   Language models are general-purpose **text generators** (like mobile phone predictive text).

### B. Buzz around Language Models (GPT-2 Example) (Slide 5)

*   Significant buzz around GPT-2 (released 2019).
*   OpenAI initially withheld model parameters due to security concerns, highlighting the potential power and risks of advanced LMs.

### C. Impact of Language Models (Slide 6)

*   Concerns about using LMs for **fake news campaigns**.
*   Potential for LMs to be a lucrative business model.
*   The "automated pen" could be a powerful tool in geopolitics.

---

## II. How do Language Models Work? (Slides 7-11)

### A. Basic Principle (Slide 8)

*   Language Models **discover statistical regularities** in text.
*   They use these regularities to **predict the next word**.
*   Example regularity: The next token is often a **repeat** of a previous word from the same text.
*   Predicting the next word iteratively allows predicting entire sentences.

### B. Markov Models (Slides 9-10)

*   **Concept (Slide 9):** Predict the next word based on a **fixed number of previous words** (the Markov assumption). Addresses modeling arbitrary length sequences with finite parameters.
*   **N-gram Models (Slide 10):**
    *   Simplest LMs count **n-grams** (sequences of *n* words) in a large corpus.
    *   Probability of next word given previous *n-1* words:
        `P(wᵢ | wᵢ₋ₙ₊₁, ..., wᵢ₋₁) ≈ count(wᵢ₋ₙ₊₁...wᵢ₋₁wᵢ) / count(wᵢ₋ₙ₊₁...wᵢ₋₁)`
    *   Example (bigram): `P(croquet | play) = N(play croquet) / N(play)`
    *   **Longer n-grams (larger *n*) generally give better predictions** due to more context.

### C. Smoothing, Back-off & Interpolation (Slide 11)

*   **Smoothing:** Adds a small constant `α` (pseudocount, often 1 for Add-one/Laplace) to all counts before estimating probabilities. Prevents zero probability for unseen n-grams.
    *   Formula: `P(wₙ | wₙ₋₁,wₙ₋₂) = [count(wₙ₋₂wₙ₋₁wₙ) + α] / [count(wₙ₋₂wₙ₋₁) + V*α]` (V is vocab size).
*   **Back-off:** If an n-gram is unseen, "back off" to the (n-1)-gram model instead of inventing a value.
    *   E.g., if `count(xyz)=0` for `P(z|x,y)`, use `P(z|y)`. If `count(yz)=0`, use `P(z)`.
*   **Interpolation:** Linearly combine probabilities from higher and lower-order models.
    *   E.g., `P̂(wₙ|wₙ₋₁,wₙ₋₂) = λ₁P(wₙ|wₙ₋₁,wₙ₋₂) + λ₂P(wₙ|wₙ₋₁) + λ₃P(wₙ)`.
    *   Lambdas (λ) sum to 1 and are chosen to maximize probability on a held-out development set.

---

## III. Generating Text from a Markov Model (Slides 12-16)

### A. Choices for Generating Text (Slide 13)

*   Use the estimated next word probability in various ways:
    *   **Greedy:** Choose the most probable next term: `w* = argmax_t P(wₙ=t | wₙ₋ₖ, ..., wₙ₋₁)`.
    *   **Random sampling:** Sample a term according to the probabilities.
    *   **Top-k sampling:** Limit sampling to the *k* most likely terms.
    *   **Temperature sampling:** Raise probabilities to power `1/T`. Higher `T` = more uniform/random sampling.
    *   **Beam search:** Search forward one step at a time for the most likely sequence, keeping a maximum of *k* candidate sequences.
*   Greedy always produces the same text; sampling produces different text each time.

### B. Examples of Generated Text (Slides 14-16)

*   **Wall Street Journal Corpus (Slide 14):**
    *   Unigrams `P(word)`: Random text.
    *   Bigrams `P(word|word₁)`: More coherent.
    *   Trigrams `P(word|word₁,word₂)`: Almost believable.
*   **Shakespeare Corpus (Slide 15):**
    *   Shows increasing coherence from Unigram to Quadrigram models.
*   **Low-order N-gram LM Output (Slide 16):**
    *   Can resemble "Finnegan's Wake" - nonsensical but potentially grammatical.

---

## IV. Evaluating Language Models (Slides 17-21)

### A. Types of LM Evaluation (Slide 18)

*   **Extrinsic evaluation:** Use the LM in a downstream task (e.g., spelling corrector) and evaluate performance on that task.
*   **Intrinsic evaluation:** Train parameters on a training set and test model performance on a held-out dataset using likelihood.

### B. Perplexity (Slide 19)

*   Quantifies the level of **surprise/confusion** of the LM at seeing new text.
*   Measures how **unlikely** the observed data is under the model.
*   **Calculating Perplexity (PP):**
    1.  **Compute probability** of the observed sequence `P(w₁, w₂, ..., wₙ)` under the model.
    2.  **Normalize probability** for length: `[P(w₁, ..., wₙ)]^(1/n)` (geometric mean of per-word probabilities).
    3.  **Invert probability** to get uncertainty: `PP(w) = [P(w₁, ..., wₙ)]^(-1/n) = 1 / [P(w₁, ..., wₙ)]^(1/n)`.
*   Minimizing perplexity is the same as maximizing probability.
*   **Lower perplexity = better model.**

### C. Perplexity, Negative Log Likelihood (nLL), and Cross-Entropy (CE) (Slide 20)

*   These are all related concepts.
*   **Perplexity:** Inverse of the geometric mean of probability of correctly predicting the next word.
*   **Negative Log Likelihood (nLL):**
    *   Negative logarithm of the probability of a sequence.
    *   Per-word nLL = (nLL of sequence) / (sequence length).
    *   Perplexity is just `2^(per-word nLL)`.
*   **Cross-Entropy (CE):**
    *   Expected (empirical average) log surprise under the model.
    *   Number of bits needed to quantify surprise.
    *   `H(p,q) ≈ -(1/N) Σ log₂ q(W)` for a sequence W under model q.

### D. Aside: Perplexity of GPT-4 (Slide 21)

*   GPT-4 Technical Report shows `log₂[PP(w)]` (bits per word) for code next word prediction.
*   A lower `log₂[PP(w)]` means lower perplexity and a higher chance of correctly predicting the next word.
    *   `log₂[PP(w)] = 1` => `PP(w) = 2` => Chance of correct next word = 1/2.
    *   `log₂[PP(w)] = 2` => `PP(w) = 4` => Chance of correct next word = 1/4.
    *   `log₂[PP(w)] = 3` => `PP(w) = 8` => Chance of correct next word = 1/8.
*   The graph shows GPT-4's performance (final loss) improving (lower bits per word) with more training compute.

---

## V. Problem with N-gram Language Models (Slides 22-24)

### A. Data Sparsity (Slide 23)

*   As *n* (n-gram length) gets larger, the chance of finding that exact sequence in the corpus drops exponentially.
*   This means there is **never enough training data** for high-order n-grams.
*   Continuously backing off to shorter n-grams greatly **limits the power of the model**.

### B. Scaling Issues (Slide 24)

*   To generate reasonable language, LMs need to model **very long-distance dependencies**.
*   Memory and data requirements for n-gram models **scale exponentially** with the length of observable dependency (*n*).
*   So, Markov models **just don't scale** for this (though they were state-of-the-art not long ago).
*   Need methods that can:
    *   **Generalize** from limited data.
    *   Handle **longer dependencies**.

---

## VI. Alternative: Word Embeddings (Slides 25-28)

### A. What are Word Embeddings? (Slide 26)

*   Appeared around 2013 and improved performance on almost every NLP task.
*   **Definition:** **Dense vectors** representing words in a high-dimensional space.
    *   Typically 100-1000 dimensions.
    *   Very low dimensional compared to one-hot encoding of terms (vocabularies 100k-1M).
*   **Note:** Like one-hot encodings, word embeddings can be aggregated (e.g., averaged) to represent sentences and documents.

### B. Motivating Word Embeddings (Slide 27)

*   Task: Fill in the blank: "Sure Sally, let’s have a skype call at 3pm _______ the 3rd of June."
*   Possible words: `on, by, before, Monday, Tuesday, GMT, CET, ...`
*   Very **few words fit the context**.
*   Those that do often come in **groups that are semantically related**.
*   Embeddings are produced by **supervised machine learning models** trained to **predict a missing word** based on its **surrounding context**.
    *   Context can be previous words (causal models) or future words too (non-causal models).

### C. Supervised Learning Problem for Embeddings (Slide 28)

*   **Predicting missing word:**
    *   **Features:** Words in the current context (e.g., "Sure", "Sally", ..., "3pm").
    *   **Target:** The missing word from the sequence.
    *   This is a multi-class problem: estimate probability for every word in the vocabulary.
*   **Issue with Linear Classifiers:** Require a very large number of parameters.
    *   A multi-class linear classifier (e.g., Logistic Regression) using a bag-of-words feature vector for context would need a parameter vector (size of vocab) for *each* vocab term as output.
    *   Overall parameters would be **quadratic** in vocabulary size (e.g., 100k vocab -> 10 billion parameters). This was prohibitive before deep learning.
    *   The word embeddings themselves become the learned parameters of an intermediate layer in a neural network.

---

## VII. Revision: Neural Networks (Slides 29-35)

### A. Brains and Neurons (Slide 30)

*   Brains: Large number of neurons (tens of billions in humans), massively connected graph (10,000+ connections per node).
*   Neuron: Simple processing unit; receives electrochemical stimulus, triggers output if sum of inputs exceeds a threshold.
*   Brain learns by connecting neurons in patterns.

### B. Artificial Neural Networks (ANNs) (Slide 31)

*   Network of artificial (simple model of) neurons.
*   **Long history:**
    *   1940s: Understanding biological neurons.
    *   1970s: von-Neumann architectures outpaced NNs.
    *   1980s: Renewed interest, computation prohibitive.
    *   2010s: Massive resurgence.
*   Model neurons as simple step functions:
    *   Weighted inputs and a threshold produce an output.
    *   Use a non-linear **activation function** (e.g., sigmoid, tanh, ReLU - Rectified Linear Unit).
    *   `y = activation_function(w · x + w₀)` (simplified).

### C. Neural Networks: Configuration (Slide 32)

*   Create different models by varying:
    *   **Topology** (shape) of the network.
    *   **Activation function** used.
    *   **Loss function** optimized (e.g., binary cross-entropy).
*   NNs are made of neurons arranged in **layers**.
    *   Each layer receives inputs from the previous layer.
    *   Activated neurons emit outputs to the next layer.
    *   **Hidden layers** (internal, neither input nor output) allow the model to find non-linear decision boundaries.
*   **Parameters:**
    *   **Weight:** Associated with the connection between two neurons.
    *   **Bias:** Associated with each neuron (threshold).

### D. Example Non-linear Classification Problem (Tic-Tac-Toe) (Slide 33)

*   Classify tic-tac-toe boards as x-wins, o-wins, or nobody-wins.
*   9-dimensional feature vector (1 for X, -1 for O, 0 for _).
*   A simple two-layer neural network can:
    *   First layer neurons learn to identify simple winning patterns (e.g., 3 vertical X's).
    *   Second layer identifies the overall winner (e.g., performs disjunction over winning patterns).

### E. Neural Networks: Training (Slide 34)

*   Parameters are learned by **backpropagation**.
*   Initialize all weights/biases randomly.
*   For each sample in the training set:
    1.  **Forward pass:** Feed input through the network to calculate prediction.
    2.  **Backward pass:** Update parameters:
        *   **Reward** connections producing correct prediction.
        *   **Penalize** those contributing to wrong result.
*   This is a **gradient descent** routine.
    *   Optimizes a nested function.
    *   Uses the **chain rule** from calculus to compute the derivative of the loss function with respect to each layer's parameters.
    *   `∇Loss = ∇MSE(a,b) = (∂MSE(a,b)/∂a, ∂MSE(a,b)/∂b)`

### F. Neural Networks: Properties (Slide 35)

*   Extremely powerful learners:
    *   A network with a single hidden layer can learn **any function**, provided the layer is wide enough.
    *   Lots of variation in network structures.
*   Many **hyperparameters** to tune (network architecture, activation function type).
*   Lots of hardware & software optimizations available for training.
*   More **computationally expensive** than other learners (e.g., decision trees, linear classifiers).

---

## VIII. Word Embeddings: Word2Vec, GloVe & Related Models (Slides 36-43)

### A. Word2Vec (Slide 37)

*   Developed in 2013 by Mikolov et al. (following earlier work by Bengio et al. 2003).
*   Word2Vec solved the parameter space issue of earlier NNLMs by using:
    1.  **Bag-of-words representation** for context (summing/averaging context word embeddings).
    2.  Neural network with a **single (linear) hidden layer** (this layer's weights become the word embeddings).
    3.  Training model in a "discriminative" fashion by inventing **negative examples** (to avoid large softmax computation).

### B. Word2Vec (cont.): CBOW and Skip-gram (Slide 38)

*   Two versions:
    *   **Continuous Bag of Words (CBOW):**
        *   Trained to predict the observed target word based on **all surrounding context words**.
        *   Context consists of all terms in a symmetric window around the target term. (Many-to-1 prediction).
    *   **Skip-gram:**
        *   Trained to predict surrounding context words given a single target word. (Actually, predicts target word given a single context word in implementation). (1-to-1 prediction).

### C. It's All Just Matrix Decomposition... (Slide 39)

*   Word embeddings can be seen as a form of **matrix decomposition**.
*   They factorize a (square) **count matrix** (vocabulary × vocabulary) that contains word **co-occurrences** in text within a fixed-size context window.
*   Factorizing **generalizes** the information in these windows and produces word embedding vectors.
    *   `Words × Contexts ≈ Words × Embeddings × Embeddingsᵀ × Contexts` (simplified concept). The embedding matrix is one of the factors.

### D. Technical Details of Word2Vec and GloVe (Slides 40-42)

*   **Word2Vec – Details (Skip-gram) (Slide 41):**
    *   Predict target word `c` given observed context word `w` using softmax of dot product of their embedding representations: `p(c|w; θ) = exp(v_c · v_w) / Σ_c' exp(v_c' · v_w)`.
    *   Directly optimizing this (sum over all possible `c'`) is computationally intensive.
    *   Mikolov et al. sampled **negative examples** (words `c'` that were NOT observed with `w`) and turned the problem into a **binary classification task** (word pair observed yes/no?) to avoid the large summation. (This is called Negative Sampling).
    *   On GPUs, optimizing softmax directly is less of a problem now.
    *   **Continuous Bag of Words (CBOW):** Estimated similarly, but the context vector `v_w` is a sum/average of word vectors in the context window.
*   **Alternative Embedding: GloVe (Global Vectors) (Slide 42):**
    *   **AIM:** Give a probabilistic interpretation to translation in embedding space.
    *   Original paper: [https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)
    *   E.g., translation from "steam" to "ice" should increase the chance of seeing "solid". The projection of ("ice" - "steam") onto "solid" should be a function of conditional probabilities, specifically ratios of co-occurrence probabilities.
        *   `(wᵢ - wⱼ)ᵀ w̃ₖ ≈ f( P(k|i) / P(k|j) )`
    *   **Objective:** Fit `wᵢᵀ w̃ₖ + bᵢ + b̃ₖ = log(Xᵢₖ)`, where `Xᵢₖ` is the co-occurrence count of word `i` and context word `k`. `w` are word vectors, `w̃` are context vectors, `b` are biases.
    *   Approximated by minimizing a weighted least squares objective `J = Σ f(Xᵢⱼ) (wᵢᵀ w̃ⱼ + bᵢ + b̃ⱼ - log Xᵢⱼ)²`.

### E. Word2Vec or GloVe? Which is Better? (Slide 43)

*   **GloVe paper:** Shows GloVe performing comparably or better than CBOW and Skip-gram on word analogy tasks.
*   **Most sources:** Skip-gram often beats CBOW (except for fastText, see next).
*   **Levy et al. (2015):** word2vec (specifically Skip-gram with Negative Sampling - SGNS) is:
    *   Faster to train.
    *   Has lower memory requirement.
    *   Produces more accurate models (when hyperparameters are tuned and compared fairly against methods like GloVe that explicitly use global co-occurrence counts). The performance differences are often small and task-dependent.

---

## IX. Properties of Word Embeddings (Slides 44-47)

### A. Semantic Clustering (Slide 45)

*   Neighbors in the embedding space are **semantically related**.
    *   Example: Closest vectors to GloVe embedding of "frog" are `frogs, toad, litoria, leptodactylidae, rana, lizard, eleutherodactylus` – all related concepts.

### B. Interpreting Word Embeddings (Slide 46)

*   Provide **dense distributed representation**. Individual dimensions are not usually interpretable.
*   **Translation (vector arithmetic) in the space is meaningful:**
    *   Certain directions in the space have meaning (e.g., male -> female direction).
    *   Semantics is often **additive**.
    *   **Analogies** are encoded: `vector(king) - vector(man) + vector(woman) ≈ vector(queen)`.

### C. Knowledge in Word Embeddings (Slide 47)

*   Embeddings discover various relationships:
    *   **Part-of-speech:** `help` -> `helpful` (adjective form).
    *   **Type-of relationships:** `red, green, blue` cluster as `colours`.
    *   **Synonyms:** `brave` ≈ `courageous`.
    *   **Geographic:** `Chicago + state ≈ Illinois` (conceptually).
    *   **Verb tense:** `walking` -> `walked` (just from co-occurrences).
    *   **Capital city -> country:** `Madrid + "is the capital city of" ≈ Spain`.

---

## X. Uses of Word Embeddings (Slides 48-51)

### A. General Uses (Slide 49)

*   **Causal models (predicting next word):**
    *   Context words only come before the missing word.
    *   Used in language modeling for predicting the next word in a sequence.
    *   Can handle longer dependencies than n-gram models.
*   **Non-causal models (e.g., CBOW, Skip-gram for representation):**
    *   Can be used as an **additional feature vector** for representing words.
    *   Improve performance on most NLP tasks (e.g., sentiment classification, translation).
    *   Effective because they incorporate domain knowledge (semantics of words) learned from large unlabeled corpora.
    *   "Embeddings are the Sriracha sauce of NLP. They make everything work better." - C. Manning (paraphrased).

### B. Word Embeddings Help Language Models (Slide 50)

*   Low-dimensional representation causes similar terms to share similar descriptions (embedding vectors).
*   Allows the LM to **generalize from semantically related examples**.
    *   E.g., if LM sees "the Queen to play croquet", it can generalize to "the Duke to play chess" if "Queen" is similar to "Duke" and "croquet" to "chess" in embedding space, or if type-of relationships (person, game) are captured.
*   Part-of-speech and hypernym (type-of) relationships are implicitly encoded.

### C. Useful for Mining Text (Slide 51)

*   Embeddings place similar concepts close together.
*   Useful for discovering **implied (but unknown) properties** of items.
    *   Example: Visualizing materials science terms; clustering reveals groups like "Piezoelectrics", "Superconductors", "Thermoelectrics", implying shared underlying properties based on textual context.

---

## XI. Extensions of Word Embeddings (Slides 52-53)

### A. Sub-word Embeddings (Slide 53)

*   Word embeddings work well if vocabulary is fixed, but cannot deal with **new (out-of-vocabulary - OOV) words** in a test set.
    *   E.g., a made-up word like "hippopotamification" has no pre-trained embedding and would be ignored, even if its meaning can be guessed from its morphemes/letters.
*   **FastText (Bojanowski et al., 2016):**
    *   Splits words into fixed-length **character n-grams** (e.g., "apple" -> `ap, app, ppl, ple, le`).
    *   Learns embeddings for these character n-grams.
    *   Combines character n-gram embeddings (e.g., by summing) to form the word's embedding.
*   **Advantage:** Deals nicely with morphologically related terms and OOV words.
    *   "believe" and "believing" will have similar representations.
    *   "rain" and "rainfall" will have similar representations.
    *   Example: `Embeddings are cool.` might be broken into sub-words like `_Eme, Embe, mbed, ... _are, ... _coo, cool, ool.`

---

## XII. Conclusions (Slides 54-55)

*   **Language models (LM):**
    *   Predict the next word in a sequence and can be used to generate text.
    *   Markov (n-gram) models don't scale to capture long-range dependencies due to data sparsity.
    *   Perplexity is a common metric to measure LM performance (lower is better).
*   **Word embeddings:**
    *   Provide distributed representations of words as dense vectors in a high-dimensional space.
    *   Capture syntax and semantics of words from their usage patterns in large corpora.
    *   Help solve the limited generalization problem of n-gram language models by allowing models to leverage semantic similarity between words.

---
```
