Okay, here are the lecture notes for "Sequence2Sequence Models & Transformers":

---

# Lecture Notes: Sequence2Sequence Models & Transformers

**Course:** Natural Language Processing
**Lecturer:** Mark Carman

---

## Lecture Contents (Slide 2)

*   Sequence-to-Sequence Models
*   Attention Mechanisms
*   Aside: Deep Learning
*   Self-attention
*   Transformer architecture
*   Text Classification with BERT
*   Text Generation with GPT

---

## I. Sequence-to-Sequence (Seq2Seq) Models (Slides 3-6)

### A. Introduction (Slide 4)

*   RNNs/LSTMs became powerful enough to be applied to **seq2seq (text in and text out)** tasks.
*   Examples: **translation, summarization, dialog systems**.
*   **Learning Translation with LSTMs:** Train two different RNN models: an **encoder** and a **decoder**.
    *   **Encoder:**
        1.  Reads in the input text.
        2.  Generates a representation (often a single vector, the final hidden state) for the entire input sequence.
    *   **Decoder:**
        1.  Takes the output (encoded representation) of the encoder.
        2.  Serializes it by generating the output sequence one word at a time, conditioned on the encoder's output and previously generated words.
    *   *Diagram shows input text -> Encoder -> encoded vector -> Decoder -> output text.*
    *   *Detailed diagram shows word embeddings fed into Encoder RNN, final encoder state fed to Decoder RNN which generates output words sequentially via softmax.*

### B. Seq2Seq Models were Transformative (Slide 5)

*   Upended state-of-the-art in a wide variety of NLP tasks.
*   **Example: Question Answering**
    *   Instead of selecting the best answer from a fixed set of candidates, models learned to *generate* text containing any possible answer.
*   **MANY (if not all) tasks can be posed as seq2seq problems:**
    *   Named Entity Extraction (Input: sentence, Output: list of entities)
    *   Question Answering
    *   Spelling Correction
    *   Query Reformulation
    *   Multi-label Classification (Input: document, Output: list of applicable class labels)
    *   Translation
    *   Summarization (Input: document, Output: summary)

### C. Problem of Translating Long Text (Slide 6)

*   **Analogy:** Human interpreters must wait for a speaker to finish before translating, requiring them to remember a lot.
*   **Encoder-Decoder Architecture Problem:**
    *   Too much information from a long input sequence is compressed into a single fixed-size vector passed to the decoder (information bottleneck).
    *   Translation is easier if the decoder has access to the **encoder's notes** (hidden states from all timesteps of the encoder), not just the final summary.
    *   *Diagram illustrates a long input sentence where the "entire message must be encoded and passed to decoder" via one vector.*

---

## II. Attention Mechanisms (Slides 7-17)

### A. Introduction to Attention (Slide 8)

*   Attention is a critical building block for modern image and text processing.
*   **What is it? Why implemented? How does it work?**
*   **Computer Vision:** Enables models to concentrate processing on specific image regions.
    *   E.g., focus on pixels of a road sign; read text by moving from one character to the next.
*   **NLP:** Enables models to concentrate processing on specific regions of text.
    *   E.g., focus on sentiment-bearing words for sentiment analysis; extract named-entity answers.

### B. Motivating Attention for Translation (Slides 9-10)

*   **Attention models (Slide 9):**
    *   Make encoded input (all encoder hidden states) available to the decoder at each decoding step.
    *   Provides a direct route for information to flow from relevant parts of the input to the output.
*   **Why not directly map input words to output words?**
    *   **Different number of tokens:** Languages require different numbers of tokens for the same concept (e.g., "apple tree" vs. "Apfelbaum").
    *   **Different word order:** (e.g., "United Nations General Assembly" vs. "Assemblea Generale delle Nazioni Unite").
*   **Generating the right output word (Slide 10):**
    *   Often requires knowing **more than just the current word** in the input.
    *   Can even require knowledge of a **future word** in the input sentence (e.g., for determining gender of a determinant in languages like French).
    *   *Diagram shows attention focusing on "European Economic Area" for translating to "zone économique européenne", highlighting word reordering.*

### C. Need for Attention & How it Works (Slides 11-12)

*   **Need (Slide 11):** A mechanism to pass information from relevant input word embeddings to the corresponding output word generation.
*   **Attention:** Provides a **direct route** for information to flow from input to output.
*   Information flow to the decoder is controlled by the **previous state of the decoder**.
*   **How it works (Slide 12):**
    *   Similarity `wᵢⱼ` is computed between the **state of the decoder (hᵢ₋₁)** and the **output embedding/hidden state (ēⱼ) of each encoder term `j`**.
    *   `wᵢⱼ = P(j|i) = softmax(similarity(hᵢ₋₁, ēⱼ))` (these are attention weights).
    *   **Soft-attention** produces a context vector `zᵢ` which is a **weighted average** over all input (encoder) embeddings/hidden states:
        `zᵢ = Σⱼ wᵢⱼ ēⱼ`. This context vector `zᵢ` is then used by the decoder to predict the next output word.

### D. Visualizing Attention (Slide 13)

*   Soft-attention computes a weighted average.
*   Can **visualize weights** used to generate each output token.
*   **Example (English to French translation):**
    *   Translating "the": Attention placed on "Area" (to determine gender `la` vs `le`).
    *   Translating "European Economic Area": Reverse diagonal attention shows word order reversal.
    *   Translating "was signed" (2 words) to 3 French words: Attention spreads.

### E. Calculating Similarity (Slide 14)

*   How is similarity between decoder state `hᵢ₋₁` and encoder output `ēⱼ` computed?
    *   **Additive Attention (Bahdanau et al., 2015):**
        *   Concatenate decoder state and encoder embedding, then pass through a feedforward network (FFNN).
        *   `similarity(hᵢ₋₁, ēⱼ) = FFNN([hᵢ₋₁; ēⱼ])` (typically `vᵀ tanh(W₁hᵢ₋₁ + W₂ēⱼ)`).
    *   **Multiplicative Attention (Luong et al., 2015 - more common):**
        *   Compute dot product (or a scaled variant) between decoder state and encoder embedding.
        *   `similarity(hᵢ₋₁, ēⱼ) = (hᵢ₋₁ ⋅ ēⱼ) / √d` (where `d` is embedding size, for scaling).
        *   Dividing by `√d` helps maintain variance if embeddings have high dimensionality.

### F. How Attention Works (Full Flow) (Slide 15)

*   Once similarity weights `wᵢⱼ` are calculated (e.g., via softmax of dot products), they are used to weight the embeddings `ēⱼ` from the encoder.
*   Context vector for multiplicative attention: `zᵢ = Σⱼ softmax((hᵢ₋₁ ⋅ ēⱼ) / √d) ⋅ ēⱼ`.

### G. Generalizing the Notation: Queries, Keys, Values (Slides 16-17)

*   Multiplicative attention: `zᵢ = Σⱼ softmax((qᵢ ⋅ kⱼ) / √d) ⋅ vⱼ` (generalized).
*   **Query (qᵢ):** What is being looked up (e.g., current decoder state `hᵢ₋₁`).
*   **Key (kⱼ):** Index of what to find (e.g., encoder hidden state `ēⱼ`).
*   **Value (vⱼ):** Stored information at the key (e.g., encoder hidden state `ēⱼ`, can be different from key).
*   Query, Key, and Values are often linear transformations of original vectors:
    *   `qᵢ = W_q hᵢ₋₁`
    *   `kⱼ = W_k ēⱼ`
    *   `vⱼ = W_v ēⱼ`
    *   `W_q, W_k, W_v` are learnable weight matrices.
*   **Hypothetical Example (Slide 17):**
    *   **Query:** Need an adjective describing a person.
    *   **Key:** Adjectives describing people & animals.
    *   **Value:** Word is "friendly", Italian translation "simpatico". (The attention mechanism would "retrieve" this value based on query-key similarity).

---

## III. Aside: Deep Learning (Slides 18-21)

### A. What is Deep Learning? (Slide 19)

*   Deep Neural Networks are NNs with **MANY layers**.
*   **Why many layers?**
    *   Massively improves performance.
    *   Allows the network to learn a **hierarchy of useful features** automatically (earlier layers learn simple features, later layers combine them into complex ones).
*   **Downsides?**
    *   Need **lots of training data**.
    *   Need large **computing resources** (GPUs).
    *   Can be unstable & **harder to train**.

### B. Why is Deep Learning so Popular? (Slide 20)

*   **Amazing performance** on many tasks, especially with complicated raw data (images, text).
*   Greatly **simplifies training pipeline** (little/no need for manual pre-processing/feature engineering).
*   Allows for **transfer learning** (reuse knowledge from previously trained systems).
*   **Possible due to:**
    *   **Hardware advancements:** GPUs for fast matrix multiplication.
    *   **Huge amounts of data:** For learning complex features.
    *   **Clever architectures:** Convolutional, recurrent, self-attention networks.
    *   **Improved training procedures:** Fast gradient descent, batch normalization, dropout.
    *   **Toolkits:** PyTorch/TensorFlow performing automated differentiation (`∇Loss`).

### C. Why is Deep Learning Important for Text? (Slide 21)

*   Achieves **state-of-the-art performance** for most text processing tasks (classification, summarization, generation, translation, etc.).
*   Until 2017, deep architectures often involved stacking many layers of LSTMs (e.g., ELMO).
*   In 2017, a new architecture, the **Transformer**, emerged.
    *   Paper: "Attention Is All You Need" (Vaswani et al., 2017) ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)).
    *   Makes use of a stack of **self-attention** networks.

---

## IV. Self-Attention (Slides 22-27)

### A. Motivating Self-Attention: Slow Training of RNNs (Slides 22-23)

*   **Lessons from Deep Learning (Slide 23):** Deeper models work better.
*   **Problem with Training RNNs:**
    *   Information must propagate sequentially from the first encoder position to the last decoder position.
    *   Gradient information must pass back along the entire sequence to update parameters.
*   **Sequential Calculation:**
    *   Cannot be fully parallelized.
    *   Training time is linear in text length O(n).
    *   Difficult to learn very deep networks with many layers.
*   **During Training:** We know the desired output and have attention passing info directly. Can we speed up training?

### B. Removing the RNN (Slide 24)

*   **During training:** We know the entire output, no need to generate one term at a time. Attention passes information directly.
*   **Crazy Idea:** What if we remove recurrent links from encoder/decoder?
    *   Just have an NN encoder/decoder without the recurrent part.
*   **Creates two problems:**
    1.  What should be used for the query in the attention mechanism (if not the decoder's previous state)?
    2.  We lose all information about the **order of words**.
*   **Possible Solutions:**
    1.  Use current output of encoder as query (instead of decoder context) - *but what would that mean?* (This leads to self-attention).
    2.  Add **position information** directly to the data/embeddings (e.g., learn positional embeddings).

### C. So What is Self-Attention? (Slide 25)

*   A mechanism for:
    *   Combining word embedding vectors to produce **new word embedding vectors**.
    *   Each new high-level embedding is a **weighted average** of word embeddings below it (from the same sequence).
*   **Weights are computed based on:**
    *   **Similarity between embeddings** in their respective positions within the same sequence.
    *   Model parameters control this process, learning how best to compute the weights.
    *   *Diagram: "This self attention idea is cool" - each word's new embedding is a weighted sum of all words' initial embeddings in the sentence.*

### D. So What is Self-Attention - Exactly? (Slide 26)

*   Mechanism for **updating a sequence of embedding vectors** based on the weighted average of incoming embedding vectors from the same sequence.
    *   `zᵢ = Σⱼ softmax((qᵢ ⋅ kⱼ) / √d) ⋅ vⱼ`
*   Weights `wᵢⱼ` depend on the similarity between embeddings at position `i` (query) and position `j` (key) in the same sequence.
*   More specifically, at each position `i`, compute:
    *   **Query (qᵢ):** Linear transform of embedding `ēᵢ` at position `i`. (`qᵢ = W_q ēᵢ`)
    *   **Key (kⱼ):** Linear transform of embedding `ēⱼ` at position `j`. (`kⱼ = W_k ēⱼ`)
    *   **Value (vⱼ):** Linear transform of embedding `ēⱼ` at position `j`. (`vⱼ = W_v ēⱼ`)

### E. What is Self-Attention Trained To Do? (Slide 27)

*   Self-attention models are trained to either:
    1.  **Recover missing words (Masked Language Modeling - MLM):**
        *   Input text is corrupted by randomly masking tokens.
        *   Model learns to predict the original masked tokens based on surrounding unmasked context.
    2.  **Predict the next word (Causal/Autoregressive Language Modeling):**
        *   Model learns to predict the next word in a sequence given the previous words.
        *   Used for text generation.

---

## V. Transformer Architecture (Slides 28-33, 35-41)

### A. Original Transformer (Slide 29)

*   From 2017 ("Attention is all you need") paper.
*   Was brilliant, looked complicated because it contained both an **encoder and a decoder** stack.
*   **Internal architecture is surprisingly simple:**
    *   Basic self-attention module contains:
        *   **Multiple attention heads** working in parallel.
        *   A **feedforward network (FFN)**.
        *   **Residual connections** and **layer normalization**.
    *   Architecture is word **position agnostic**.
        *   So **positional encoding** is added to the input embeddings at the bottom layer.

### B. Inside Transformer (Stacking) (Slide 30)

*   The basic self-attention module (Transformer block) is stacked on itself **many times** (e.g., 12+ layers).
*   Allows the **semantics of each token to build up** over multiple steps of processing.
*   **Note:** Transformers are MUCH **faster to train** than stacks of RNNs because computations within a layer can be parallelized (RNNs require sequential gradient iteration).

### C. Motivating Self-Attention (Why it makes sense) (Slides 31-33)

*   **Context-Dependent Meanings (Slide 32):**
    *   Words take on different meanings depending on their context.
    *   Self-attention allows a word's representation to **depend on its context** within the same sentence.
    *   It learns a **weighting function** over lower-level embeddings of context terms to refine a word's own embedding.
    *   *Example: "I arrived at the **bank** after crossing the street." (financial bank) vs. "I arrived at the **bank** after crossing the river." (river bank). Self-attention helps differentiate.*
*   **Coreference Resolution (Slide 33):**
    *   Complicated tasks like coreference resolution can be handled by multiple layers of self-attention.
    *   *Example:* "The animal didn't cross the street because **it** was too tired." (it -> animal) vs. "The animal didn't cross the street because **it** was too wide." (it -> street).
    *   More attention is placed on "animal" in the first sentence and "street" in the second when resolving "it". This is possible if "it" initially has some embedding related to "tired" (for the first) or "wide" (for the second), and similarity is computed.

### D. Transformer Architecture - Structure & Implementation (Slides 34-41)

*   **Transformer Stack (Slide 35):**
    *   **Input module:** Creates initial embedding for each token (token embedding + positional encoding).
    *   **Transformer blocks:** Many stacked on top of each other. Each block modifies the embedding at each token position.
        *   *Same parameters* are used within a self-attention/FFN sub-layer to process each token position, but *different parameters* exist for each layer in the stack.
    *   **Output module:** Recovers the predicted word (e.g., linear layer + softmax).
*   **The Transformer Block (Slide 36):**
    *   Modifies incoming embedding by adding information from:
        1.  A **self-attention block**.
        2.  A **Feed-Forward Neural Network (FFNN)**.
    *   Both sub-layers have residual connections (`x + SubLayer(x)`) and layer normalization (`LayerNorm`).
    *   `LayerNorm(ẽ + z̃)` where `ẽ` is input embedding and `z̃` is output of self-attention/FFN.
*   **Inside the Transformer Block (Slide 37):**
    *   **Self-attention block:** Contains multiple self-attention **heads** (e.g., 8-16).
        *   Each head works with a fraction of the embedding size (`d/h`). Outputs are concatenated.
    *   **Feed-Forward Network (FFNN) layer (MLP):**
        *   Typically a single hidden layer with a fan-out of around 4 (e.g., input `d`, hidden `4d`, output `d`).
        *   `FFNN(z̃) = W₂ ReLU(W₁z̃ + b₁) + b₂`.
*   **Scaled Dot-Product Attention (Single Head Detail) (Slide 38):**
    *   **Query (Q):** What we are looking up (embedding of the current position).
    *   **Key (K):** What we are looking for (embedding of other words after linear transformation).
    *   **Value (V):** What will be updated (embedding of other words after linear transformation, used to form weighted sum).
    *   Each (Q, K, V) is produced by a linear mapping from the original embedding.
    *   `Attention(Q,K,V) = softmax( (QKᵀ) / √dₖ ) V`.
    *   `dₖ` is the dimension of keys (and queries). Scaling by `√dₖ` normalizes dot products.
*   **Self-Attention via Matrix Multiplications (Slides 39-40):**
    *   Calculations for a single layer are a series of matrix multiplications/manipulations.
    *   **All token positions are updated in parallel.** All attention heads are computed at the same time.
    *   **Steps:**
        1.  Compute Query (Q), Key (K), and Value (V) matrices for all token positions.
        2.  Multiply QKᵀ (for autoregressive models, apply a mask so future tokens have zero similarity).
        3.  Apply softmax to rows of the resulting similarity matrix.
        4.  Multiply attention weights by the Value matrix.
        5.  For multi-head attention, concatenate each head's output and multiply by an output weight matrix `W⁰`.
    *   (More info: [http://www.columbia.edu/~jsl2239/transformers.html](http://www.columbia.edu/~jsl2239/transformers.html))
*   **Video Explanation (Slide 41):**
    *   "Attention in transformers, visually explained | Chapter 6, Deep Learning (Grant Sanderson)"
    *   [https://www.youtube.com/watch?v=eMlx5fFNoYc](https://www.youtube.com/watch?v=eMlx5fFNoYc)

---

## VI. Generating Input for Transformer (Slides 42-47)

### A. Tokenization: Word or Character-level? (Slide 43)

*   **Choice depends on:**
    *   Computational trade-offs (expressivity vs. sequence length/inference complexity).
    *   Language type (e.g., Chinese logograms at syllable level; DNA sequences).
*   **Words as tokens:**
    *   Much larger number of tokens (e.g., 1M for English Twitter).
    *   No explicit modeling of morphology (`dist(apple, apples)` could be same as `dist(apple, elephant)`).
*   **Characters as tokens:**
    *   Longer sequence length.
    *   Burden on model to learn word structure; embeddings may contain disjunctive information.
    *   Much less interpretable.
*   *Diagram shows BytePair Encoding (BPE) and Character n-grams (FastText) as intermediate options.*

### B. Transformers use Sub-word Tokens (Byte-Pair Encoding - BPE) (Slide 44)

*   Break words into sub-word tokens using data-driven methods.
*   **Byte-Pair Encoding (BPE):**
    1.  Find frequent character sequences.
    2.  Iteratively replace the most frequent consecutive characters (or sub-word units) by new merged symbols.
*   Common prefixes/suffixes become vocabulary elements.
*   Example:
    *   `though they think that the thesis is thorough enough`
    *   `replace th -> θ`: `θough θey θink θat θe θesis is θorough enough`
    *   `replace ou+g+h -> ə`: `θə θey θink θat θe θesis is θorə enə`
    *   `replace θe -> ψ`: `θə ψy θink θat ψ ψsis is θorə enə`
*   Input: `'I like playing football.'` -> Word: `'I', 'like', 'playing', 'football', '.'` -> Sub-word: `'I', 'like', 'play', '#ing', 'foot', '#ball', '.'`

### C. Embedding Positional Information (Slides 45-47)

*   **Need for Positional Information (Slide 46):**
    *   Transformer architecture is symmetric (agnostic to input order).
    *   Training on reordered features learns the same model.
    *   But text meaning depends on word order (e.g., "rat in the white house" ≠ "white rat in the house").
*   **Positional Encoding using Sinusoids (Slide 47):**
    *   Inform model about word order.
    *   Sentence: "I want my mommy" -> `(“I”, 1), (“want”, 2), (“my”, 3), (“mommy”, 4)`.
    *   `Embedding["my",3] = embedding["my"] + embedding[position_3]`.
    *   Instead of binary encoding for position, use **sinusoids** for floating point embedding vectors:
        *   `PE(pos, 2i) = sin(pos / 10000^(2i/d_model))`
        *   `PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))`
        *   `pos` is position, `i` is dimension index, `d_model` is embedding dimension.
    *   This creates unique positional vectors that can be added to token embeddings.

---

## VII. Why Does Stacked Self-Attention Work So Well? (Slides 48-51)

### A. What is Stacked Self-Attention Learning? (Slide 49)

*   Visualizations help interpret what is learned.
*   Some attention heads simply aggregate information or attend to a previous token.
*   **Others learn language relationships** (Clark et al., 2019).
    *   *Demo link for BERT visualization:* [https://colab.research.google.com/drive/1PEHWRHrvxQvYr9NFRC-E_fr3xDq1htCj](https://colab.research.google.com/drive/1PEHWRHrvxQvYr9NFRC-E_fr3xDq1htCj)
    *   *Examples shown:* Head attending to next token, noun modifiers, prepositional attachment.

### B. Why is Stacked Self-Attention So Useful? (Slide 50)

*   Argued that Transformers effectively learn to build a **dependency parse tree** over concepts in the text (Hewitt et al., 2019).
*   *Example:*
    *   "The store was out of food. ____"
    *   "The chef who ran to the store was out of food. ____"
    *   To predict the next sentence, need to know *who* is out of food. Self-attention can learn these dependencies.

### C. But What Does the FFNN Do? (Slide 51)

*   The two blocks (self-attention and FFNN) provide complementary functionality:
    *   **Self-attention builds patterns** from input text (e.g., identifies noun phrases like "The Eiffel Tower", "fastest animal on earth").
    *   **Feed-forward layer looks up facts** for entities.
        *   Paper: "Transformer Feed-Forward Layers Are Key-Value Memories" ([https://arxiv.org/abs/2012.14913](https://arxiv.org/abs/2012.14913)).
        *   *Example:* Input `<Tower>`, self-attention identifies it as part of `<height of Eiffel Tower>`, FFNN looks up/associates `<300 metres>`.

---

## VIII. BERT vs GPT (Slides 52-56)

### A. Story of Two Architectures (BERT & GPT) (Slide 53)

*   **Original Transformer (2017):** Designed for translation, had encoder & decoder.
*   Subsequent models often used only one part:
*   **BERT (Bidirectional Encoder Representations from Transformers):**
    *   (Devlin et al., 2019 - Google).
    *   Uses the **encoder** stack of the Transformer.
    *   **Autoencoder:** Recovers (potentially corrected) input at the top of each column.
    *   Learned by **masking random tokens** (MLM) and recovering them.
    *   Great for **representing text** (e.g., for building classifiers).
*   **GPT (Generative Pretrained Transformer):**
    *   (Radford et al., 2019 - OpenAI, for GPT-2).
    *   Uses the **decoder** stack of the Transformer (with masked self-attention).
    *   **Autoregressive:** Predicts the next token at the top of each column.
    *   Learned by **masking future tokens** (standard language modeling).
    *   Great for **generating text**.

### B. Pretraining of BERT and GPT (Slide 54)

*   **BERT:**
    *   Pretrained by **masking out** random words in the input using a special `[MASK]` token.
    *   Model must recover all words, including masked ones.
    *   Trained on Wikipedia and a corpus of books.
*   **GPT-2:**
    *   Pretrained by masking **future words** in the sequence and predicting the next word at each point (causal language modeling).
    *   Trained on 40GB of web text (Reddit user-rated content).
    *   *Note:* Model quality depends on pre-training data quality ("garbage in => garbage out").

### C. Transformer Sizes & # of Parameters (Slide 55)

*   Transformers come in multiple sizes, depending on:
    *   Number of self-attention layers.
    *   Size of the embedding used at each layer.
    *   Number of parallel attention heads.
*   **Typical BERT sizes:** Base (110M params), Large (340M params).
*   **Typical GPT-2 sizes:** Largest (1.5B params, 50,257 vocab).
*   **More parameters generally result in:**
    *   Better performance.
    *   Longer training times and larger memory requirements.
*   BERT/GPT typically use a context size of 512 or 1024 tokens.

### D. Related Models (Slide 56)

*   **Variants on BERT:**
    *   **RoBERTa (Facebook):** Modifies training objective, more data, larger batches.
    *   **XLNet:** BERT with autoregressive modeling (GPT-2 like elements).
    *   **DistilBERT:** Smaller (40%), faster (60%) BERT, retains ~97% accuracy.
    *   Many other variations for specific domains (e.g., medical) or tasks (e.g., QA).
*   **Encoder-Decoder Model (full Transformer):**
    *   **T5 (Text-To-Text Transfer Transformer):** Uses encoder+decoder, clever relative positional encoding. Particularly useful for translation or other text2text problems.
*   *Note: Domain-specific and task-specific text2text models are often worth investigating.*

---

## IX. What Can Transformers Be Used For? (Slides 57-63)

### A. Text Classification with BERT (Slides 58-61)

*   **Traditional Classifier Building (Slide 58):**
    1.  Decide on feature types (n-grams, etc.).
    2.  Decide how to process them (stemming, IDF, POS, embeddings).
*   **BERT removes the manual feature extraction step.**
*   Performance improvements likely due to unsupervised pre-training (language modeling) and not discarding word order.
*   **How is BERT Fine-tuned for Classification? (Slide 59):**
    *   **Pre-training:** A special `[CLS]` token is added to the start of all text (loss function ignored its output).
    *   **Fine-tuning:** Teach the model to produce the **class label** at the output corresponding to the `[CLS]` token. Instead of predicting a missing word, the model is fine-tuned to predict the class label.
*   **What Else Can We Do? Transfer Learning Benefits (Slide 60):**
    *   Model comes **pre-trained** on enormous data.
    *   **Fine-tune** on specific tasks with little data.
    *   **Big improvements in performance:** Especially with small training corpora due to leveraging LM pre-training.
    *   **Multi-linguality:** Multilingual BERT pretrained on 104 languages! Can train a classifier on English docs and use it on Italian ones (transfer learning across languages).
*   **Using BERT for Classification in Practice (Slide 61):**
    *   Download large pre-trained LM (choose type, size, layers for fine-tuning).
    *   **Advantage:** No need for manual feature engineering.
    *   **Cons vs. Simple Linear Classifier:**
        *   Hard limit on length of text (e.g., <1000 tokens, may need to chunk).
        *   Much longer and more effort to train (needs fast hardware like GPUs, learning rate tuning).
        *   Model will be big (more memory, slower predictions).
        *   Predictions are **less interpretable** (though explainability techniques like LIME exist).

### B. Text Generation with GPT-type Models (Slides 62-63)

*   **GPT-2 was a fantastic text generator (Slide 63):**
    *   Conditioning on a (large) piece of text, GPT-2 could generate new content.
    *   Sounded **highly believable**.
    *   Could be **nonsensical** (disagree with common sense). Newer models are better but still invent occasional untruths.
*   **Text generation is very important:**
    *   Building block for many applications, from question answering to chatbots.

---

## X. Conclusions (Slides 64-65)

*   **Sequence2Sequence Models:**
    *   Utilize RNNs in an encoder-decoder framework.
    *   Improved state-of-the-art for many NLP tasks (translation, QA, summarization).
    *   Made use of attention architecture to handle long sequences.
*   **Transformer Models:**
    *   Replaced recurrent links in RNNs (with attention) with **self-attention**.
    *   Stack many self-attention and feedforward blocks in a deep architecture.
    *   Provide state-of-the-art performance for text classification (e.g., BERT) and text generation (e.g., GPT).

---
```
