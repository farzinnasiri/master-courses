# Lecture Notes: Agentic AI

**Course:** Natural Language Processing
**Lecturer:** Mark Carman
**Textbook References:**
*   "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition" by Daniel Jurafsky and James H. Martin
*   "Machine Learning and Security: Protecting Systems with Data and Algorithms" by Clarence Chio & David Freeman

---

## Lecture Contents (Slide 2)

*   What is Agentic AI?
*   Design Patterns
*   Toolkits for Agentic AI
*   Most common application: RAG systems
*   Configuring and Evaluating RAG systems

---

## I. What is Agentic AI? (Slide 3)

*   **According to Wikipedia:**
    *   **Agentic AI** is a class of **artificial intelligence** that focuses on autonomous systems that can make decisions and perform tasks without human intervention.
    *   These independent systems automatically respond to conditions to produce process results.
    *   The field is closely linked to agentic automation (agent-based process management systems) when applied to process automation.
    *   Applications include software development, customer support, cybersecurity, and business intelligence.

---

## II. Agentic AI Design Patterns (Slides 4-5)

Popular architectures for using LLMs in workflow:

*   **Reflection Pattern:**
    *   LLM checks its own output.
    *   Can be used to police behavioral requirements (e.g., ensure no offensive content or secret information leakage).
*   **Tool Use Pattern:**
    *   LLM makes use of external tools (e.g., calculators, retrieval engines) and processes their results.
*   **Re-act (REasoning and ACTing) Pattern:**
    *   LLM reasons about available tools.
    *   Calls tools that perform actions on the world to achieve goals (e.g., a call center operator using a customer database tool).
*   **Planning Pattern:**
    *   LLM generates a plan to achieve a complicated goal.
    *   The plan involves executing multiple tasks.
    *   LLM must control workflow execution and replan if any task fails.
*   **Multi-agent Pattern:**
    *   Multiple LLMs interact with each other as agents.
    *   Each agent has specific capabilities and contributes to a larger goal.

*(Reference: [https://blog.dailydoseofds.com/p/5-agentic-ai-design-patterns](https://blog.dailydoseofds.com/p/5-agentic-ai-design-patterns))*

---

## III. Toolkits for Agentic AI (Slide 6)

*   **LangChain:**
    *   Open-source framework that simplifies the development of LLM-powered applications.
    *   Documentation: [https://python.langchain.com/docs/introduction/](https://python.langchain.com/docs/introduction/)
*   **Related Libraries:**
    *   **LangGraph:** For building complex workflows (graphs of LLM calls and tools).
    *   **LangSmith:** For monitoring and debugging LLM application workflows.
    *(Diagram shows LangChain architecture components: Architecture (LangChain, LangGraph), Components (Integrations), Deployment (LangGraph Platform), and LangSmith for monitoring & debugging.)*

---

## IV. How Can an LLM Be Used as an Agent? How Can It Control Tools? (Slides 7-9)

### A. How LLMs Make Use of Tools (Slide 8)

*   **Instruction Tuning:** Models are often instruction-tuned on examples of tool use.
*   **Declaration of Tools:** Simply declare at the start of the conversation (or in the system prompt) what tools are available and how to use them.
*   **Special Syntax:** A specific syntax (often JSON-like or XML-like) is used in the conversation to format:
    *   The LLM's call to a tool (e.g., `Action: Wikipedia, Action Input: Number of moons of Mars`).
    *   The response from the tool (e.g., `Observation: Mars has two moons...`).
    *   The tool is treated as just another actor in the dialog.
*   The LLM generates a "Thought" process, then the "Action" and "Action Input", receives an "Observation", and continues this loop until it can provide a "Final Answer".

### B. How LLMs Make Use of Tools (cont.) (Slide 9)

*   **Behind the Scenes:** The agentic AI toolkit (e.g., LangChain) parses the LLM's structured output (e.g., the JSON specifying the action and input).
*   The toolkit then calls the actual tool (e.g., a Python function for `wikipedia_tool` or `calculator_tool`).
*   The tool's output is returned to the LLM as an "Observation" to continue the reasoning cycle.
    *(Source: [https://github.com/langchain-ai/langchain/discussions/13127](https://github.com/langchain-ai/langchain/discussions/13127))*

---

## V. If LLMs Can Be Used as Agents, What Does the Future Hold? (Slides 10-11)

*   **No need for humans anymore?** (A provocative question).
*   These models are getting better and better, working on ever more complicated tasks.
*   **Example of Advanced Capability (Slide 11):**
    *   A research group (Sakana AI) managed to automatically generate a research paper for a top-level Machine Learning conference (ICLR) that was accepted by reviewers.
    *   Reference: [https://sakana.ai/ai-scientist-first-publication/](https://sakana.ai/ai-scientist-first-publication/)
    *(Image: Robots in a lab, performing experiments and writing an academic paper).*

---

## VI. Engineering and Evaluating Retrieval Augmented Generative (RAG) Models (Slides 12-15)

### A. Engineering RAG Systems (Slide 13)

*   Many choices to be made when building a RAG model:
    *   What documents to include in the corpus?
    *   How to chunk the documents?
    *   Should images/tables be embedded too?
    *   What embedding model should be used? How many dimensions?
    *   Is reranking of retrieved results (e.g., with BERT) needed?
    *   How many relevant results to include in the prompt to the generative LLM?
    *   If the generative model already knows the answer (parametric knowledge), should it skip retrieval?
*   These choices affect the overall system performance in terms of:
    *   Accuracy of the results.
    *   Speed of retrieval.
    *   Memory requirements of the index.

### B. Evaluating RAG Systems (Slide 14)

*   Involves:
    *   Determining how correct the output is.
    *   If an evaluation dataset is available, compare generated outputs with ground truth outputs.
*   **Problem:** Desired (ground truth) and actual (generated) output often aren't exactly the same textually but may contain the same information.
    *   *Example: Question: "What is the capital of Narnia?" Ground-truth and Generated answers are paraphrases.*
*   **Metrics for Semantic Similarity:**
    *   **Exact-match:** Checks if predicted text is exactly the same as ground-truth. (Score: 0 in example)
    *   **BLEU-4:** Compares based on n-gram overlap (common in Machine Translation). (Score: 0.206)
    *   **METEOR:** Allows for synonyms, stemming, rephrasing. (Score: 0.42)
    *   **ROUGE-L:** Uses longest common subsequence of overlapping words (common in Summarization). (Score: 0.492)
    *   **BERTScore:** Uses a BERT model to produce a similarity score between documents. (Score: 0.905)

### C. Judge and Jury (LLM-as-Evaluator) (Slide 15)

*   **LLM-as-a-Judge:**
    *   Use an LLM to judge whether the generated answer and the ground-truth answer are semantically the same.
*   **LLM-as-a-Jury:**
    *   If one LLM isn't trusted, ask a number of different LLMs to vote on whether the texts agree.

---

## VII. Conclusions (Slides 16-17)

*   In this lecture, we discussed:
    *   **Agentic AI design patterns:** Reflection, Tool use, Re-act, Planning, Multi-agent.
    *   (Implicitly covered: From LLMs to Chatbots, Prompting LLMs, Open-source LLMs, RAG systems, Evaluation of RAG systems from previous context and slide content).

---

