# Lecture Notes: Spoken Conversation & Dialog

**Course:** Natural Language Processing
**Lecturer:** Mark Carman
**Textbook Reference:** "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition" by Daniel Jurafsky and James H. Martin.

---

## Lecture Contents (Slide 2)

*   Properties of Dialog
*   Conversational Analysis
*   What are Conversational Agents?
*   Open-domain Chatbots
*   Task-oriented Dialog Agents
*   Evaluating Dialog Systems
*   Ethical Questions

---

## I. Properties of Spoken Dialog (Slides 3-4)

### Example of SPOKEN Dialog (Slide 4)

*   Conversation between travel agent (A) and client (C).
*   **Turns:** Each takes turns to speak (single word or multiple sentences).
*   **Turn-taking:** Speakers negotiate when to take the floor.
*   **Interruptions:** Client interrupts agent to make a correction; agent stops talking.
*   **Spoken Chatbots Must Determine When to Talk:**
    *   Requires **end-point detection** (when user has stopped talking).
    *   Challenging because people pause mid-utterance.
    *   Humans detect when someone is about to finish and start immediately.
    *   Traditional agents wait for speaker to stop before processing.
    *   **Barge-in detection** needed to recognize and handle user interruptions.
    *   *Example dialog illustrates these points.*

---

## II. Traditional Conversational Analysis (Slides 5-8)

*   Covers: Speech Acts & Common Ground, Conversational Structure & Initiative, Inference.

### A. Conversations as Actions (Speech Acts) (Slide 6)

*   **Insight:** Each turn in dialogue is a kind of **action** (Wittgenstein, Austin).
*   **Speech Acts (Dialogue Acts):**
    *   **Constatives:** Commit speaker to something being the case (answering, claiming, confirming, denying, stating).
        *   *Example:* "I need to travel in May." (User stating a constraint).
    *   **Directives:** Attempt by speaker to get addressee to do something (advising, asking, forbidding, inviting, ordering, requesting).
        *   *Example:* "Turn up the music!" (System issuing a DIRECTIVE).
        *   *Example:* "What day in May do you want to travel?” (Agent politely commanding client to answer - DIRECTIVE).
    *   **Commissives:** Commit speaker to some future course of action (promising, planning, vowing, betting, opposing).
    *   **Acknowledgments:** Express speaker’s attitude regarding hearer with respect to action (apologizing, greeting, thanking, accepting an acknowledgment).
        *   *Example:* "Thanks" (User issuing an ACKNOWLEDGMENT).

### B. Establishing Common Ground (Slide 7)

*   Participants in joint activity need to establish **common ground**: a common/agreed understanding of the world state.
*   **Principle of Closure:** When performing an action, an agent requires evidence it succeeded.
    *   Example: Elevator grounds user request by lighting the button after it is pressed.
*   Speech involves conveying information; speakers **ground** each other’s utterances by acknowledging that the hearer has understood.
*   **Grounding is important for chatbots:**
    *   Bad: `System: Did you want to review profile? User: No. System: What’s next?`
    *   Good: `System: Did you want to review profile? User: No. System: Okay, what’s next?` ("Okay" makes it clear the system understood).
*   **Humans ground utterances by:**
    *   Explicitly stating "OK".
    *   Repeating what the other person says.
    *   Starting a response with "And" (implying connection to prior turn).

### C. Conversation Structure, Initiative & Inference (Slide 8)

*   **Conversational Structure:** Local structure between adjacent speech acts.
    *   **Adjacency pairs:** question + answer, proposal + acceptance/rejection, compliment + downplayer.
    *   **Sub-dialogues (side sequences):** e.g., a correction.
        *   *Example: Client asks about day of week mid-booking, agent answers, then conversation returns to booking.*
*   **Conversational Initiative:**
    *   Some conversations are controlled by one person (e.g., reporter interviewing).
    *   Most human conversations have mixed initiative (A leads, then B leads, etc.).
    *   NLP systems often default to simpler styles (can be frustrating):
        *   **User initiative:** User asks, system responds.
        *   **System initiative:** System asks questions to fill a form, user can't change direction.
*   **Inference:** An important aspect of dialogue.
    *   Client may not directly answer an agent's question but expect the agent to draw inferences.
    *   *Example: `Agent: And, what day in May did you want to travel? Client: OK, uh, I need to be there for a meeting that’s from the 12th to the 15th.` (Client implies travel dates via meeting dates).*

---

## III. What are Conversational Agents? (Slides 9-11)

### A. Conversational What? (Slide 10)

*   **Conversational agents** are also known as: dialogue systems, dialogue agents, chatbots.
*   **Used for:**
    *   **Voice interfaces:** Play music, control lights, set timers, get weather.
    *   **Entertainment:** Chatting can be fun.
    *   **Clinical uses (mental health):** Chatting can be therapeutic.
    *   **Access services:** Book hotels, buy books.
*   **Examples:** Personal assistants on phones/smart speakers (SIRI, Alexa, Cortana, Google Assistant).

### B. Kinds of Conversational Agents (Slide 11)

*   Traditionally two broad categories:
    1.  **Open-domain Chatbots:**
        *   Carry on extended conversations, aiming to mimic unstructured human-human conversation.
        *   Mostly designed for fun, possibly also for therapy.
    2.  **Task-oriented Dialogue Systems:**
        *   Goal-based agents used to solve specific tasks (e.g., booking a restaurant, maintaining a shopping list).
        *   Interfaces to personal assistants, cars, appliances, robots.

---

## IV. Open-domain Chatbots (Slides 12-29)

### A. Chatbot Architectures (Slide 13)

*   **Rule-based:**
    *   Used a fixed set of rules to generate responses.
    *   Examples:
        1.  Pattern-action rules: ELIZA (1966).
        2.  Plus mental model: PARRY (1971).
*   **Corpus-based (Data-driven):**
    *   Mine large datasets of human conversations to either retrieve or generate responses.

### B. Rule-Based Chatbots: ELIZA & PARRY (Slides 14-17)

*   **ELIZA (Weizenbaum, 1966) (Slide 15):**
    *   Simulated a Rogerian psychotherapist.
    *   Seemed to give linguistically coherent responses and remember things from earlier in the discourse.
    *   **Eliza's trick:** Used simple transformation rules based on keywords; reflected statements back at the patient; needed very little world knowledge.
*   **Eliza’s Simple Transformation Rules (Slide 16):**
    *   Organized by keywords. Each keyword has a pattern and a list of possible transformations.
    *   If multiple patterns match, the most specific rule (highest keyword importance) is chosen.
    *   If no keyword matches, a non-committal response is used (e.g., "PLEASE GO ON ...").
    *   **Memory trick for "MY":** Randomly select a transform for "MY", store output, and use it later if no other keyword matches.
*   **PARRY (Colby, 1971) (Slide 17):**
    *   Computational model of schizophrenia (paranoid type).
    *   Similar pattern-response structure to Eliza but with a richer control structure.
    *   Included a **model of mental state** (variables for Anger, Fear, Mistrust).
    *   **Procedure:** Mental state variables change based on user input (e.g., insults increase Anger). Response depends on current mental state.
    *   First system to pass a version of the **Turing test in 1972** (psychiatrists couldn't distinguish PARRY interviews from those with human paranoid schizophrenics).

### C. Aside: Emotion Classes (Slides 18-20)

*   **What emotion classes should we be using? (Slide 18)**
*   **Basic Emotion Classes (Slide 19):**
    *   **Ekman’s 6 basic emotions:** Happiness, Surprise, Fear, Sadness, Anger, Disgust.
    *   **Plutchik’s wheel:** 8 basic emotions in opposing pairs (joy-sadness, anger-fear, trust-disgust, anticipation-surprise).
    *   **Spatial model (Valence-Arousal):** Emotion as a point in 2D/3D space.
        *   **Valence:** Pleasantness of the stimulus.
        *   **Arousal:** Intensity of emotion provoked.
        *   (Dominance: Degree of control exerted by stimulus - sometimes added for 3D).
*   **Beyond Simple Emotion Classes (Scherer's Typology) (Slide 20):**
    *   **Emotion:** Relatively brief response to an event (angry, sad, joyful).
    *   **Mood:** Subjective low-intensity feeling, long duration, often no apparent cause (cheerful, gloomy).
    *   **Interpersonal stance:** Affective stance towards another person (distant, warm, supportive).
    *   **Attitudes:** Enduring preferences/predispositions (liking, hating).
    *   **Personality traits:** Emotionally laden, stable dispositions (nervous, hostile).

### D. Lessons from Eliza and Parry (Slide 21)

*   Creating chatbots that interact relatively convincingly is **not actually that difficult**.
*   Very simple rule-based systems can work quite well.
*   **Anthropomorphism, Privacy & Ethical Implications:**
    *   People became **deeply emotionally involved** with ELIZA.
    *   Privacy implications of storing conversations.
    *   Users might be misled about the computer's understanding.
    *   Later studies showed humans **develop relationships** with technological artifacts.
    *   When designing chatbots, consider potential **harms** as well as **benefits**.

### E. Risks of Anthropomorphisation (Slide 22)

*   Explored in movies (e.g., "Her" (2013) - protagonist falls in love with virtual assistant).
*   More relevant with improvements in text generation.
*   Can lead to **serious risks to mental health** (e.g., reports of users encouraged towards harmful actions by AI chatbots).

### F. Corpus-Based Chatbots (Slides 23-29)

*   **Overview (Slide 23):** Produce context-appropriate responses by either:
    *   **Retrieving** a response from a large corpus of conversations.
    *   **Generating** a response using a language model, given the dialogue context.
*   **Examples of Corpus-Based Models [2020] (Slide 24):**
    *   **Microsoft Asia’s XiaoIce:** Chatted in Chinese, mainly extracted turns from past human conversations.
    *   **Facebook’s BlenderBot:** Generated responses with a Language Model.
*   **Corpus-Based Chatbots Require Corpora (Slide 25):**
    *   Data-intensive: require hundreds of millions or billions of words.
    *   **Sources of conversational data:**
        *   Transcripts of telephone conversations.
        *   Movie dialogues.
        *   Paid crowd-worker conversations.
        *   Pseudo-conversations from public posts on social media.
    *   Raises questions about removing Personally Identifiable Information (PII).
*   **Response by Retrieval or Generation (Slide 26):**
    *   **Retrieval-based:** Given user query and corpus, find the response most similar to the query (using lexical or semantic search).
        *   *Diagram shows query embedding `h_q` and response candidate embedding `h_r` similarity calculation.*
    *   **Generation-based:** Generate response by conditioning on encoded query using an encoder-decoder or decoder-only model trained on conversational data. For dialog, query also includes previous responses.
*   **Problem: Generative Chatbots Can Get Repetitive/Boring (Slide 27):**
    *   Tendency to produce dull responses ("I’m OK", "I don’t know"), shutting down conversation.
    *   **Modifications:**
        *   Use **diversity-enhanced beam search** instead of greedy decoding.
        *   Train with **diversity-focused training objectives**.
        *   Add **minimum length constraints**.
*   **Modern Chatbots Use RAG Models (Slide 28):**
    *   Response by **retrieving and refining knowledge**.
    *   Can generate responses from **informative text** rather than just prior dialogue turns.
    *   To respond to "Tell me something about Beijing":
        *   XiaoIce: Collects sentences from lectures/news, uses IR to find relevant ones.
        *   Can augment LM with **Retrieval-Augmented Generation (RAG)**:
            *   Use IR to retrieve Wikipedia passages.
            *   Concatenate retrieved sentences to dialogue context (with separator token).
            *   Provide as context to LM, which learns to incorporate the text into its response.
*   **Chatbots: Advantages and Disadvantages (Slide 29):**
    *   **Pros:** Fun, good for narrow/scriptable applications.
    *   **Cons:** Don't "really" understand, appearance of understanding can be problematic, rule-based ones are expensive/brittle, IR-based ones can only mirror training data.
    *   **Next step:** Integrating chatbot abilities into frame-based agents.

---

## V. Task-Oriented Dialog Agents (Slides 30-38)

### A. Overview (Slide 31)

*   **Goal-oriented:** Help user solve a task (setting timer, travel reservation, playing song, buying product).
*   Often use **frame-based architecture** with:
    *   **Frames:** Actions/functions the agent can perform.
    *   **Slots:** Variables/arguments for those functions.
    *   **Values:** Assigned to variables (initially unknown).
    *   *Example: `buy_book(name="Alice in Wonderland", author="Lewis Carol", ...)`*

### B. Frame-Based Architecture (Slides 32-34)

*   **Concept (Slide 33):** Proposed in 1977 (GUS - Genial Understanding System).
    1.  Knowledge structure representing user intentions.
    2.  Action **frames**, each with **slots** and **values**.
*   A **frame** is an action that can be performed.
    *   Contains a set of **slots** to be filled with information of a given type.
    *   Each slot is associated with a **question** to the user.
    *   *Example slots for flight booking: ORIGIN, DEST, DEP DATE, DEP TIME, AIRLINE.*
*   **Genial Understanding System (GUS) Example (Slide 34):**
    *   Transcript shows GUS filling slots for a flight booking.
    *   System asks questions to fill slots; user might fill many slots at once.
    *   When frame is filled, DB query is run or booking made.
    *   **Complex abilities:** Handled coreference ("the first one"), temporal reasoning ("Friday in the evening"), implicit constraints ("must be in San Diego before 10am").

### C. Multiple Frames & Frame Detection (Slide 35)

*   Systems usually contain **multiple frames** (e.g., car reservations, hotel reservations, route info, airfare info).
*   **Frame detection:** System must detect which slot of which frame the user is currently trying to fill and switch dialogue control accordingly.
*   **Slots can have condition-action rules:**
    *   E.g., for plane booking frame: once `destination` is specified, use it as default `StayLocation` for hotel booking. Once `departure date` is specified, calculate `ArrivalDay`.

### D. GUS: Natural Language Understanding (NLU) (Slide 36)

*   NLU component extracts 3 things from user utterances:
    1.  **Domain classification:** Is user talking about airlines, alarm clock, calendar?
    2.  **User intent determination:** What task/goal? (find movie, show flight, remove appointment).
    3.  **Slot filling:** Extract values to instantiate the task frame.
    *   *Examples: "Show me morning flights from Boston to SF on Tuesday." -> DOMAIN: AIR-TRAVEL, INTENT: SHOW-FLIGHTS, SLOTS filled.*

### E. Rule-Based Slot-Filling & Template-Based Generation (Slide 37)

*   Common in industry:
    *   Use **handwritten rules** (e.g., regular expressions) to fill slots for a recognized intent (e.g., SET-ALARM intent: `wake me (up) (at) TIME`).
    *   Use **templates** for pre-built response strings:
        *   Fixed: "Hello, how can I help you?"
        *   Contain variables: "What time do you want to leave CITY-ORIG?"

### F. Summary: Simple Frame-Based Architecture (Slide 38)

*   **Positives:** High precision, can provide coverage if domain is narrow.
*   **Negatives:** Can be expensive/slow to create rules, can suffer from recall problems (brittle).

---

## VI. Dialog-State (Belief-State) Architecture (Slides 39-43)

### A. Overview (Slide 39)

*   Sophisticated version of frame-based architecture.
*   Has dialogue acts & better generation capabilities.
*   Basis for many industrial systems.
*   Often uses ML for slot-understanding.

### B. Dialogue-State Architecture Components (Slide 40)

1.  **Natural Language Understanding (NLU):** Extracts slot fillers from user’s utterance using ML.
2.  **Dialogue State Tracker (DST):** Maintains current dialog state: user’s recent dialogue act, set of slot-filler constraints from user.
3.  **Dialogue Policy:** Decides what system should do/say next.
    *   GUS policy: Ask questions until frame is full.
    *   More sophisticated: Know when to ask clarification questions, etc.
4.  **Natural Language Generation (NLG):** Produces more natural, less templated utterances.

### C. Mapping Text to Dialogue Acts (Slide 41)

*   Combine ideas of **speech acts** and **grounding** into a single, language-independent dialog representation.
*   Example tag set for restaurant recommendation: `HELLO(a=x,b=y,...)`, `INFORM(a=x,b=y,...)`, `REQUEST(a=x,...)`, `CONFIRM(a=x,...)`, etc.
*   Tags label sample dialogue, showing content of each act (e.g., user `inform(food=italian, near=museum)`).

### D. Slot Filling with Classifiers & Sequence Labelers (Slide 42)

*   **Classifier approach:** Train a classifier to map words/phrases to semantic frame-fillers (requires large labeled dataset).
*   **Sequence labeler approach:** Label each input word with a slot tag (e.g., using BIO tagging).
    *   Extracted values must be **normalized** (e.g., "San Francisco" -> "SFO").
*   **Sequence-to-sequence model:** Can be trained to directly produce desired slot-value output.
*   Also need **domain and intent detection** (e.g., classify sequence as "airline_travel+flight_search").

### E. Dialogue State Tracking & Corrections (Slide 43)

*   Dialogue-act detection and slot-filling are often performed jointly.
    *   Example: "I'd like Cantonese food near the Mission district" -> `inform(food=cantonese, area=mission)`.
*   Simple DSTs run slot-filler after each sentence.
*   **Detecting Correction Acts:**
    *   If system misrecognizes utterance, user might make a **correction**.
    *   Done by repeating, rephrasing, or saying "no" to a confirmation.
    *   Corrections are hard for systems to understand (often misrecognized themselves).
    *   **Hyperarticulation** (exaggerated prosody) is a factor: "I said BAL-TI-MORE, not Boston".
    *   Features for detecting corrections: lexical cues ("no", "correction"), semantic similarity, phonetic overlap, prosody, ASR confidence.

---

## VII. Dialog Policy and Generation (Slides 44-50)

### A. Dialogue Policy (Slide 45)

*   At each turn, system predicts action based on:
    *   **Entire conversation history:** `Âᵢ = argmax P(Aᵢ | A₁,U₁,...,Aᵢ₋₁,Uᵢ₋₁)`.
    *   Or **current dialogue state** (filled slots) and last system/user utterances: `Âᵢ = argmax P(Aᵢ | Frameᵢ₋₁,Aᵢ₋₁,Uᵢ₋₁)`.
*   Dialogue systems make errors, so they use mechanisms to ensure understanding:
    *   **Confirming** understandings with the user.
    *   **Rejecting** utterances the system has likely misunderstood.

### B. Confirmation & Rejection Strategy (Slide 46)

*   **Confirmation:**
    *   **Explicit:** `S: Do you want to leave from Baltimore? U: Yes.` (Easier for user to correct, but awkward/long).
    *   **Implicit:** `S: Which city do you want to leave from? U: Baltimore. S: When do you want to travel to Berlin?` (System implies understanding by moving on).
*   **Rejection:** System expresses lack of understanding (e.g., "I’m sorry, I didn’t understand that.").
    *   **Progressive prompting:** If utterance rejected multiple times, prompt with more detail or guide user.

### C. Confidence-Based Confirmation (Slide 47)

*   ASR or NLU systems assign a **confidence value** to their understanding.
*   Confidence estimated based on: acoustic log-likelihood, prosodic features, ratio of best to second-best interpretation scores.
*   Use confidence thresholds to determine action:
    *   `< α` (low confidence): Reject.
    *   `≥ α` (above threshold): Confirm explicitly.
    *   `≥ β` (high confidence): Confirm implicitly.
    *   `≥ γ` (very high confidence): Don't confirm at all.

### D. Natural Language Generation (NLG) (Slide 48)

*   Modeled in two stages in information-state architecture:
    1.  **Content planning (what to say):** Dialogue policy decides speech act and attributes (slots/values).
    2.  **Sentence realization (how to say it):** Generate natural language (answer or confirmation).
    *   *Example input: `recommend(restaurant_name=Au Midi, neighborhood=midtown, cuisine=french)` -> Output: "Au Midi is in Midtown and serves French food."*

### E. Sentence Realization (Slide 49)

*   Training data is hard to come by (don't see each specific entity in each situation).
*   Improve generalization by **delexicalization**:
    *   Replacing words in training set that represent slot values with generic placeholder tokens.
    *   E.g., `[restaurant_name]` is in Midtown and serves `[cuisine]` food.
*   Train encoder-decoder models to map frames (semantic representations) to delexicalized sentences.
*   Finally, **relexicalize** by inserting actual slot values.

### F. Generating Clarification Questions (Slide 50)

*   System repeats parts of user utterance to clarify which aspect needs clarification.
*   *Example: `User: What do you have going to UNKNOWN WORD on the 5th? System: Going where on the 5th?`*
*   Methods:
    *   Rules: Replace "going to UNKNOWN WORD" with "going where".
    *   Classifiers that guess which slots were misrecognized.

---

## VIII. Evaluating Dialog Systems (Slides 51-54)

### A. Overview (Slide 51)

*   **Open-domain chatbots:**
    *   Mainly require **human evaluation**.
    *   **Participant evaluation:** User who talked to chatbot assigns score.
    *   **Observer evaluation:** Annotator reads transcript and assigns score.
*   **Task-based dialogue:**
    *   Mainly evaluated by measuring **performance on task**.

### B. Evaluating Chatbots (Human Evaluation) (Slide 52)

*   **8 dimensions of quality (See et al., 2019):** Avoiding repetition, interestingness, making sense, fluency, listening, inquisitiveness, humanness, engagingness.
*   **Participant evaluation:** How repetitive? Did it make sense? Did you enjoy talking?
*   **Observer evaluation (annotators compare 2 conversations):**
    *   **Engagingness:** Who would you prefer to talk to for a long conversation?
    *   **Interestingness:** Which speaker is more interesting?
    *   **Humanness:** Which speaker sounds more human?
    *   **Knowledgeable:** Which speaker is more knowledgeable?

### C. Automatic Evaluation is an Open Problem (Slide 53)

*   Automatic methods (like BLEU for MT) generally not used for chatbots as they correlate poorly with human judgments.
*   **Possible approach: Adversarial evaluation** (Turing Test inspired):
    *   Train a classifier to distinguish human vs. machine responses.
    *   More successful system is better at fooling the classifier.
*   Recently, using **ChatGPT (LLMs) for evaluating** dialog quality (LLM-as-a-Judge).

### D. Evaluating Task-Based Agents (Slide 54)

*   **Evaluate by task success:**
    1.  **End-to-end evaluation (task success):** Was the task completed correctly? (e.g., meeting added to calendar).
    2.  **Slot error rate for sentence:** `# incorrect/missing slots / # slots for sentence`.
*   **Or via user study (PARADISE framework - Walker et al., 2000):**
    *   TTS Performance, ASR Performance, Task Ease, Interaction Pace, User Expertise, System Response, Expected Behavior, Future Use.
*   **Other useful metrics:**
    *   **Efficiency:** Elapsed time, number of turns/queries/corrections.
    *   **Quality:** Number of ASR rejections, number of barge-ins.

---

## IX. Ethical Implications of Chatbots (Slides 55-59)

### A. Ethical Design (Slide 56)

*   Ethical issues are crucial in AI agents.
*   Mary Shelley's *Frankenstein* raised these concerns over 200 years ago (creating agents without ethical consideration).
*   **Ethical Issues:**
    *   **Safety:** Systems abusing users, distracting drivers, giving bad medical advice.
    *   **Representational harm:** Systems demeaning particular social groups.
    *   **Privacy:** Information leakage.

### B. Safety (Slide 57)

*   Very important for many applications:
    *   **Chatbots for mental health:** Extremely important not to say the wrong thing.
    *   **In-vehicle conversational agents:** Must be aware of environment, not distract driver.

### C. Abuse & Representation Harm (Microsoft Tay Example) (Slide 58)

*   **Lesson of Microsoft Tay (2016):**
    *   Experimental Twitter chatbot, 18-24 y.o. female persona.
    *   Designed to learn from users (IR-based interaction).
    *   Taken down after only **16 hours**.
    *   Immediately turned offensive (obscene tweets, harassment, propaganda, conspiracy theories).
    *   Reflected racism/misogyny present on Twitter.
*   **Bias in training data:**
    *   Abusive content in datasets used to train systems.
    *   Detectors find large amounts of biased/abusive content in dialogue datasets.
    *   Much effort spent trying to reduce biased content in training data.

### D. Privacy Concern: Training on User Data (Slide 59)

*   **Accidental information leakage:**
    *   Example: "Computer, turn on the lights [user answers phone] Hi, yes, my password is ..."
    *   Shown that this leakage can indeed occur.
*   **Intentional information leakage:**
    *   Dialogue systems designed to send user data to developer or advertiser.
    *   Important to consider **privacy-preserving dialogue systems**.

---

## X. Conclusions (Slides 60-61)

*   In this lecture, we discussed:
    *   Properties of dialog: speech acts and conversational structure.
    *   Open-domain chatbots: rule-based (Eliza, Parry) and data-driven (corpus-based, RAG) chatbots.
    *   Task-oriented dialog agents: frame-based and dialog-state architectures (NLU, DST, Policy, NLG).
    *   Evaluation of chatbots (human-centric, task-based metrics).
    *   Ethical concerns for chatbots (safety, bias, privacy, anthropomorphism).

---

