Okay, here are the lecture notes for "Classifying Text" based on the slides you provided:

---

# Lecture Notes: Classifying Text

**Course:** Natural Language Processing
**Lecturer:** Mark Carman
**Textbook Reference:** "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition" by Daniel Jurafsky and James H. Martin.
**(Additional for Linear Models Section):** "Machine Learning and Security: Protecting Systems with Data and Algorithms" by Clarence Chio & David Freeman

---

## Contents (Slide 2)

1.  Quick Revision: Supervised Machine Learning
2.  Text Classification
3.  Extracting Features from Text
4.  Linear Models
5.  Evaluating a Text Classifier
6.  Conclusions

---

## I. Quick Revision: Supervised Machine Learning (Slides 3-12)

### A. Machine Learning (ML) (Slide 4)

*   **Definition:** Techniques aimed to make machines "act more intelligent" by generalizing from past data to predict future data.
*   **Standard Definition (Tom M. Mitchell):** "A computer program is said to learn from **experience E** with respect to some class of **tasks T** and a **performance measure P**, if its performance at tasks in T, as measured by P, improves because of experience E."
    *   *Example:* A robot learning to express emotions (see Guardian article on GPT-3).

### B. Example ML Task: Categorizing Fish (Slides 5-6)

*   **Scenario:** On a fishing boat, a worker separates fish from a conveyor belt into different types.
*   **Goal:** Build a system to do this task automatically.
*   **Approach:**
    1.  **Install Sensors:** Camera to capture images of fish.
    2.  **Compute Features:** From each image, extract features like:
        *   `X₁`: length
        *   `X₂`: width
        *   `X₃`: colour
    3.  **Learn Rules:** From historical (labeled) data, learn classification rules:
        *   `if (X₁/X₂ < 1.4 and X₃=blue) => bass`
        *   `else if (X₁/X₂ > 2.1 and X₃=pink) => tuna`
        *   `else => unknown`

### C. Supervised Learning (Slide 7)

*   Each training instance (e.g., a fish) is a vector in some **feature space**.
*   Each training instance has been **labeled** with a class (e.g., type of fish).
*   **Task:** **Partition the space** to be able to make predictions for new, unlabeled vectors.
    *   *Diagram:* Shows data points in a 2D feature space (X₁, X₂) separated into classes.

### D. In Practice (Slide 8)

*   Data usually overlaps, so classes are often **not linearly separable**.
*   Instances are described by **many features**, with some dimensions being better at distinguishing classes than others.

### E. Types of Classification MODELS (Slide 9)

*   All classifiers divide up the feature space.
*   The **boundary** can be **linear** or **non-linear**.
*   **Linear models:**
    *   Include Naïve Bayes, Logistic Regression, Support Vector Machine (SVM).
*   **Non-linear models:**
    *   Include SVM with Radial Basis Function (RBF) kernel, Gradient Boosted Decision Trees, Neural Networks.

### F. Training a Model (Slide 10)

*   **Model:**
    *   A parameterized formula (e.g., `y = θᵀx`).
    *   Used to predict labels for new instances.
*   **Learning Algorithm:**
    *   Takes as input training instances and corresponding ground truth labels.
    *   Searches for **parameters** of the model (θ*) that minimize prediction error (loss) on training labels.
    *   Has its own settings called **hyper-parameters**.
    *   *Diagram:* Shows `Training Instances` + `Training Labels` -> `Learning Algorithm (with Hyper-parameters)` -> `Model (θ*)`. And `Instances` -> `Model (θ)` -> `Predicted Labels`.

### G. Hyperparameters and Overfitting (Slide 11)

*   **Hyperparameters:** Parameters of the learning algorithm that can control the **complexity of the model**.
*   **Examples of hyper-parameters:** #parameters in model, #iterations over training set, learning rate, maximum size of parameters.
*   **Overfitting:**
    *   Training error usually improves with model complexity.
    *   However, **test performance** (error on unseen data) reaches a peak and then degrades as the model **overfits** the training set by learning spurious patterns.
    *   **Goal:** Find the model with the best **generalization performance**.
    *   *Diagram:* Shows `Performance` vs. `Model Complexity`. `Training data` performance keeps improving. `Test data` performance peaks (best model) then degrades.

### H. Preventing Overfitting (Slide 12)

*   Choose hyper-parameter values to prevent overfitting.
    *   Can't use training data error (doesn't indicate generalization).
    *   Can't use test data (needed later for unbiased final testing).
    *   Solution: Hold-out a portion of the training set as a **validation set** for interim evaluation.
*   **Validation Dataset:**
    *   Used to evaluate different training runs/models trained with different hyper-parameter settings.
    *   Train model variants, evaluate on validation set.
    *   Keep hyper-parameter values that show the best generalization performance on the validation set.
    *   *Diagram:* Similar to slide 11, but shows `Validation data` curve peaking. Illustrates `Training set`, `Validation set`, `Test set` split.

---

## II. Text Classification (Slides 13-16)

### A. What is text classification? (Slide 14)

*   The process of training a model to classify documents into predefined categories.
    *   *Diagram:* Document -> Classifier -> Categories (Science fiction?, Romance?, Crime drama?).

### B. Why classify text? (Slide 15)

*   An **extremely common task** with many applications:
    *   Spam/phishing detection
    *   Authorship identification (e.g., Satoshi Nakamoto)
    *   Sentiment analysis (e.g., customer reviews)
    *   Offensive content detection
    *   Web search query intent
    *   Personalized news feeds
    *   Identifying criminal behavior online (fraud, grooming)
    *   Routing communication (e.g., company emails)
    *   Task identification in spoken interfaces (e.g., "Hey Alexa, tell me a joke")

### C. Types of text classification problems (Slide 16)

*   **Binary classification:** Output is binary.
    *   E.g., Spam detection, sentiment analysis: "I absolutely love this product..." => POSITIVE.
*   **Ordinal regression:** Output is an ordinal scale.
    *   E.g., Product review star rating: "Hotel room was filthy..." => 1_STAR.
*   **Multi-class classification:** Output is one category from multiple.
    *   E.g., Topic categorization, routing: "Hi, My internet connection has been dodgy..." => REPAIRS_DEPT.
*   **Multi-label classification:** Output is a set of categories.
    *   E.g., News articles: "Donald Trump invited Tiger Woods for a round of golf..." => POLITICS, SPORT.

---

## III. Extracting Features from Text (Slides 17-26)

### A. Overview (Slide 18)

*   Text can be **arbitrarily long** and has no fixed size, so it cannot be given directly to a model.
*   **Features** must first be extracted from the text.
    *   *Diagram:* Text -> Feature Extraction -> Feature Vector (e.g., `<12, 105, 32, -5, ..., 27, -42, 11>`) -> Classifier -> Category.

### B. Feature Details (Slide 19)

*   **Features:** Signals in the document that are useful for predicting its category.
*   Need to convert text data into a **vector of features**.
*   **If training data is scarce:**
    *   **Syntax-based features:** E.g., # of capitalized words.
    *   **Part-of-speech based features:** E.g., # of verbs vs. proper nouns.
    *   **Reading-difficulty based features:** E.g., average length of words/sentences.
*   **Most common features:** The **words themselves**.
    *   The vocabulary of a document provides the most important signal.
    *   The # of occurrences of words provides further information.
    *   **Bag-of-words (BOW) model:**
        *   Represents documents as vectors of word counts.
        *   Results in a massively sparse representation (long vector with many zeros).

### C. Motivating Bag-of-Words (BOW) (Slide 20)

*   Consider customer review extracts:
    *   Positive: "...zany characters and **richly** applied satire, and some **great** plot twists"
    *   Negative: "It was **pathetic**. The **worst** part..."
    *   Positive: "...**awesome** caramel sauce... I **love** this place!"
    *   Negative: "...**awful** pizza and **ridiculously overpriced**..."
*   Much of the **useful information** for classifying documents is present in the **vocabulary** of the document.

### D. Why not just one-hot encode? (Slide 21)

*   To create a fixed-dimension feature vector from text:
    *   One could truncate documents at a fixed length and treat them as a sequence of categorical variables.
*   Encoding categorical variables using **one-hot encoding** produces *n* binary features per variable (where *n* is the size of the vocabulary).
*   A sequence of 500 tokens with a vocabulary of 50,000 would produce 500 * 50,000 = 25 million columns (features).
*   This leads to **too many features** given a typically small amount of training data.
*   **BOW solution:** Sum all one-hot encodings for tokens in a document together.
    *   Reduces the #columns to the size of the vocabulary.
    *   **Throws away all word order information** but retains critical vocabulary information.

### E. Aside: Word Frequencies (Statistical Laws) (Slides 22-24)

*   **Heap's Law (Slide 23):** Vocabulary size `V(l)` grows approximately with the square root of document/collection length `l`.
    *   `V(l) ∝ l^β`, where `β ≈ 0.5`.
*   **Zipf's Law (Slide 23):** A token's frequency `ctf_t` is approximately proportional to the inverse of its rank `rank(t)`.
    *   `ctf_t ∝ 1 / rank(t)^s`, where `s ≈ 1`.
*   **Monkeys, Typewriters & Vocabularies (Slide 24):**
    *   Heap's law derives from Zipf's law and can be explained by a random typing model (infinite monkey theorem).
    *   **Key point:** Vocabulary of a document/collection grows **slowly** compared to its length.
    *   Examples:
        *   Switchboard phone conv.: 2.4M tokens, 20k vocab.
        *   Shakespeare: 884k tokens, 31k vocab.
        *   COCA: 440M tokens, 2M vocab.
        *   Google N-grams: 1T tokens, 13M+ vocab.

### F. Bag-of-Words (BOW) Representation Details (Slide 25)

*   Document vocabulary is very small compared to the vocabulary of the entire collection.
*   Terms present in a document usually characterize its content well.
*   BOW representation can use counts of occurrences or binary presence/absence.
*   Results in a **very sparse representation**.
*   **Completely ignores word order.**
    *   Extension: Including **n-grams** (bigrams, trigrams) can increase performance but greatly increases the number of dimensions, requiring more data.

### G. BOW Representation Implications (Slide 26)

*   Usually, we have far fewer documents (examples) than vocabulary terms (features).
    *   This is not an ideal situation for a classifier.
    *   Multiple settings of model parameters can explain the observed class labels equally well.
*   Therefore, **strong regularization** is needed to guide the learner and prevent overfitting.

---

## IV. Tokenizing & Preprocessing Text for Building Classifiers (Slides 27-35)

### A. Simple Tokenizers in Python (Slide 28)

*   **Scikit-learn (default tokenizer):**
    *   Uses simple regular expression: `token_pattern = '(?u)\b\w\w+\b'`
    *   `\b` matches word boundaries.
    *   `\w` matches any 'word' character (`[a-zA-Z0-9]`).
*   **NLTK (Natural Language Toolkit):**
    *   Uses a more complicated regular expression to catch various types of tokens (abbreviations, currency, percentages, ellipses).
    *   Example: `text = 'That U.S.A. poster-print costs $12.40...'`
    *   Becomes: `['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']`

### B. Common Pre-processing of Text for Classifiers (Slide 29)

*   **Prior to tokenization:**
    *   Remove mark-up (e.g., HTML tags).
    *   Lowercase the text (reduces vocabulary size, but can lose info like 'WHO' vs 'who').
    *   Remove punctuation.
*   **After tokenization:**
    *   Remove stopwords (extremely high-frequency words).
    *   Remove very low-frequency words (insufficient information to determine correlation with class).
*   **Less common activities when building classifiers:**
    *   Stemming or lemmatization (to reduce vocabulary size).
    *   Spelling correction.

### C. Aside: Spelling Correction (Slides 30-35)

*   **Probabilistic Spelling Correction (Slide 31):**
    *   Consider "acress" (observed, not in vocab), intended "actress"?
    *   Ideal: `P(correct|observed) = #(correct & observed) / #(observed)` (from a large corpus of misspellings). This corpus usually doesn't exist.
    *   Can use **string edit distance** (insertions, deletions, substitutions, transpositions).
    *   Problem: `actress, caress, cress, access, across, acres` might have similar edit distances, but some words (like `actress`) are inherently more likely.
*   **Bayes' Rule for Spelling Correction (Slide 32):**
    *   `P(correct|observed) = P(observed|correct) * P(correct) / P(observed)`
    *   Since `P(observed)` is constant for all candidates, `P(correct|observed) ∝ P(observed|correct) * P(correct)`
        *   `P(correct)`: **Prior probability** of the corrected word (how frequent is it in English?).
        *   `P(observed|correct)`: **Likelihood of correction** (how likely the author mistyped `observed` when intending `correct`? This is the channel/error model).
*   **Parameter Estimation (Slide 33):**
    *   **Prior `P(correct)`:** Estimate from word frequencies in a large corpus (e.g., N=44 million words). A smoothed estimate is used, e.g., `P(c) = (freq(c) + 0.5) / (N + 0.5 * |V|)`.
    *   **Likelihood `P(observed|correct)`:** Estimate from a confusion matrix of common errors (e.g., how often 'e' is substituted for 'o' when 'across' was intended but 'acress' was typed).
*   **Context is Needed (Slide 34):**
    *   A simple Bayesian model might predict "acres" (44%) over "actress" (37%) for "acress" based on prior and error model.
    *   However, context like "...stellar and versatile **acress** whose combination..." clearly indicates "actress".
*   **Making use of context leads to Naïve Bayes Classifier (Slide 35):**
    *   Look at preceding words (e.g., "versatile actress" vs. "versatile acres").
    *   Replace unigram `P(correct)` with bigram `P(previous|correct)P(correct)`.
    *   The probability becomes `P(correct|observed, previous) ∝ P(observed|correct) P(previous|correct) P(correct)`.
    *   This is a **Naïve Bayes model** with two features: the observed (incorrect) word and the previous word in the sentence.

---

## V. Linear Classification Models (Slides 36-58)

### A. Overview (Slide 37)

*   Due to the very high number of dimensions in BOW representations, **linear models** are often used with text.
*   Linear models estimate one parameter per vocabulary word, making them **highly interpretable**.
*   Three most popular linear models for text:
    *   **Naive Bayes (NB):** Traditionally used with text.
    *   **Logistic Regression (LR):** Works well if regularized.
    *   **Support Vector Machines (SVM):** Naturally regularized and designed for high-dimensional data.

### B. Decision Boundaries: Hyperplanes (Slides 38-39)

*   Linear classification algorithms find linear decision boundaries.
*   This boundary is an **oriented hyperplane** in an n-dimensional vector space.
*   Given a feature vector `x = (x₁, ..., xₙ)` and model parameters `θ`, the hyperplane equation is: `θ·x − b = 0`.
    *   `θ·x = Σ θᵢxᵢ` is the dot product.
    *   `θ` is an n-dimensional vector orthogonal to the hyperplane.
    *   `b` is an offset (distance of hyperplane from origin; if `|θ|₂=1`, distance is `b`, otherwise `b/|θ|₂`). Often `b = -θ₀`.

### C. Linear Classifiers: Multinomial Naïve Bayes (NB) (Slides 40-45)

*   **"Naïve" Assumption (Slide 41):**
    *   NB is one of the oldest & simplest text classifiers.
    *   It assumes that word occurrences are **statistically independent** of each other given the class label.
    *   This means words provide independent information about the class.
    *   This assumption simplifies parameter calculation.
    *   The assumption **does not hold in practice** (words are highly correlated), but predictions are often good (though the model tends to be overconfident).
*   **Multinomial NB Classifier Formulation (Slide 42):**
    *   To estimate `P(spam | text_content)`:
        *   Use Bayes' Rule: `P(spam | text) = P(text | spam)P(spam) / P(text)`.
        *   Ignore `P(text)` (denominator) as it's constant: `P(spam | text) ∝ P(text | spam)P(spam)`.
        *   Apply Naïve assumption: `P(text | spam) = P(word₁ | spam) * P(word₂ | spam) * ...`
        *   So, `P(spam | text) ∝ P(hi | spam) P(mark | spam) ... P(I | spam) ... P(spam)`.
    *   Estimate probabilities from training data:
        *   `P(wordᵢ | spam) ≈ (count(wordᵢ in spam_docs)) / (total_words_in_spam_docs)` (for multinomial) or `(count(spam_docs_with_wordᵢ)) / (count(spam_docs))` (for Bernoulli). Slide implies latter.
        *   `P(spam) ≈ (number of spam emails) / (total number of emails)`.
*   **Smoothing Probability Estimates (Slide 43):**
    *   If a word (e.g., 'inheritance') appears only in spam messages, `P(inheritance|not-spam)` would be 0, making the entire probability for the 'not-spam' class zero if 'inheritance' is present.
    *   Avoid this by **smoothing**: add a pseudo-count `α` (e.g., `α=1` for Laplace smoothing).
    *   `P(word | class) = (count(word in class_docs) + α) / (count(class_docs) + α * |Vocabulary|)` (Laplace for Bernoulli NB). Slide shows: `P(inheritance | not-spam) = (#not-spam emails containing ‘inheritance’ + α) / (number of not-spam emails + 2α)`. *(This 2α implies α for presence and α for absence, or a binary feature context).*
*   **Is Independence a Big Deal? (Slide 44):**
    *   For spam classification, the assumption that `P('save' | spam) = P('save' | 'cheap', spam)` is incorrect as words like 'cheap' and 'save' are correlated in spam.
    *   However, this simplification makes estimation and prediction easier.
    *   In practice, NB often learns effective spam detectors despite the flawed assumption.
*   **NB Pros & Cons (Slide 45):**
    *   **Advantages:**
        *   Very **fast** to estimate (one pass over training data, no complex parameter search).
        *   Reliable predictor even with little data (stable classifier).
        *   If the conditional independence assumption holds, it's the optimal classifier.
    *   **Disadvantages:**
        *   Doesn't perform quite as well on large data as other classifiers (redundant features are counted multiple times).
        *   Predicted probabilities are often **not well-calibrated** (overconfident due to violation of independence).

### D. Linear Classifiers: Logistic Regression (LR) (Slides 46-49)

*   **Distance from Hyperplane (Slide 47):**
    *   The further a point is from the decision boundary, the more certain the prediction.
    *   Signed distance of a point `x` from hyperplane: `s(x) = θ·x − b`.
    *   Positive score => likely spam; Negative score => likely not-spam.
*   **From Distances to Probabilities (Slide 48):**
    *   Instead of just classifying, estimate the probability of a class (e.g., `P(fraud | x)`), interpreted as confidence.
    *   Need a function mapping distance `s(x) ∈ (-∞, +∞)` to probability `P ∈ [0,1]`.
    *   Standard function: **logistic curve (sigmoid function)**: `sigmoid(s) = 1 / (1 + e⁻ˢ) = eˢ / (eˢ + 1)`.
    *   Takes value 0.5 at the decision boundary (`s=0`).
    *   The slope (speed of probability change) depends on the magnitude of `θ`.
*   **Logistic Regression: Pros & Cons (Slide 49 - actual slide 56):**
    *   **Pros:**
        *   Produces **well-calibrated probability estimates**.
        *   Can be trained efficiently and scales well to large numbers of features.
        *   Explainable since each feature's contribution to the final score is additive.
    *   **Cons:**
        *   Assumes feature values are **linearly related to log odds**. If this assumption is strongly violated, the model will perform poorly.

### E. Linear Classifiers: Support Vector Machines (SVM) (Slides 50-58)

*   **Maximising the Margin (Slide 51 - actual slide 58):**
    *   If data for 2 classes is linearly separable, many decision boundaries are possible.
    *   SVM chooses the one that **maximizes the margin**.
    *   **Margin (γ):** The distance from the hyperplane to the closest points (on either side).
    *   This makes the classifier robust and generalize well.
*   **Support Vectors (Slide 52 - actual slide 59):**
    *   Points lying exactly on the margin are called **support vectors**.
    *   They constrain/define the location of the boundary and prevent the margin from getting bigger.
    *   In a *d*-dimensional space, there are at least *d+1* support vectors.
    *   **Difference from Logistic Regression:** Position of the SVM hyperplane depends *only* on these closest points (the convex hull of data points). Adding/moving internal points doesn't affect the boundary.
*   **SVMs & Loss Function (Slide 53 - actual slide 60):**
    *   A basic SVM is a linear classifier that finds the hyperplane best separating two classes.
    *   The difference from LR and NB lies in the **loss function** used to find parameters:
        *   LR uses **negative log-likelihood**: penalizes points proportionally to the probability of incorrect prediction (including those correctly classified but close to boundary).
        *   SVM uses **hinge loss**: only penalizes points that are on the wrong side of the margin or within the margin.
*   **Maximising the Margins (Mathematical Formulation) (Slides 54-55 - actual slides 61-62):**
    *   Signed distance to hyperplane: `s(x) = w₀ + Σ wᵢxᵢ`.
    *   For points on margin: `|s(x)| = |w₀ + Σ wᵢxᵢ| = γ`.
    *   Assuming `w₀=0` (hyperplane through origin), then for margin points `|w·x| = γ`.
    *   Generally, for all data points `xⱼ` with class `yⱼ ∈ {+1,-1}`: `yⱼ(w·xⱼ + w₀) ≥ γ`.
    *   To maximize `γ`, we can fix `γ=1` and minimize `||w||²` (i.e., `Σ wᵢ²`) subject to `yⱼ(w·xⱼ + w₀) ≥ 1`.
    *   This is equivalent to minimizing loss: `(1/2)||w||² + C Σ εⱼ`, where `εⱼ = max(0, 1 - yⱼ(w·xⱼ + w₀))` is the error (hinge loss) for prediction `(xⱼ,yⱼ)`.
*   **Non-separable Data (Slide 56):**
    *   If data is not linearly separable, points on the wrong side of the margin are penalized based on their distance from it.
    *   The objective function `O(w,b) = (1/2)Σwⱼ² + CΣεᵢ` remains almost unchanged.
    *   `εᵢ` is the distance (error) for the i-th support vector (misclassified or within margin).
    *   `C` is a hyperparameter: trade-off between maximizing margin size and minimizing classification errors.
*   **Hinge Loss (Slide 57 - actual slide 64):**
    *   The error function for SVM is the **hinge loss**: `L(xᵢ,yᵢ) = max(0, 1 - yᵢ(w·xᵢ + w₀))`.
    *   Applies no cost to correctly classified points outside the margin.
    *   Increases linearly with distance for points on the wrong side of the margin.
    *   The objective function can be seen as `(Regularization Term) + (Error Term)`.
*   **Summary: Linear SVM (Slide 58 - actual slide 65):**
    *   Basic SVM is a linear classifier, finds a hyperplane.
    *   Difference from LR is the loss function:
        *   LR (log-likelihood): penalizes points based on probability of misclassification, including those on the correct side if not confident.
        *   SVM (hinge loss): penalizes points linearly with distance from margin, only for those on the wrong side or very close to it.

---

## VI. Conclusions on Linear Classifiers (Slides 59-60)

*   We've seen 3 linear classification techniques: Naive Bayes, Logistic Regression, Support Vector Machines.
*   When using a **bag-of-words representation** of text:
    *   **Linear classifiers** are often sufficient for most problems.
    *   The data is high dimensional, so dependencies between features don't usually need to be explicitly modeled by the classifier itself (the BOW model itself is a strong simplification).

---

## VII. Evaluating Text Classifiers (Slides 61-64)

### A. Training and Testing (Slide 62)

*   Test a trained model by comparing its **predicted labels** with **ground truth labels** for instances from a held-out **test set**.

### B. Evaluating a Binary Text Classifier (Slide 63)

*   **Confusion Matrix:**
    *   Rows: True Class (Pos, Neg)
    *   Columns: Predicted Class (Pos, Neg)
    *   Cells: TP (True Positives), FN (False Negatives), FP (False Positives), TN (True Negatives).
*   **Common Metrics:**
    *   **Accuracy:** `(TP + TN) / (TP + FP + TN + FN)` (% of correct predictions).
    *   **Precision:** `TP / (TP + FP)` (% of positive predictions that were correct).
    *   **Recall (Sensitivity):** `TP / (TP + FN)` (% of actual positive instances found by the model).
    *   **F-measure (F₁-score):** `2 * (Precision * Recall) / (Precision + Recall)` (harmonic mean of precision and recall).
    *   **AuC (Area under the ROC Curve):** Often used as a single measure that doesn't depend on the specific confidence threshold used for predictions.

### C. Evaluating Multi-class Classifiers (Slide 64)

*   If there are *n* classes, the confusion matrix will be *n x n*.
*   Calculate a precision/recall value for each class by considering it as the positive class in a one-versus-all setting.
*   Combine each class's Precision and Recall values into a single measure:
    *   **Macro-average:** Average the per-class P/R values, weighting each class the same.
    *   **Micro-average:** Compute P/R from the sum of individual TPs, FPs, FNs over all classes. This weights each class by the number of its data points.

---

## VIII. Conclusions on Text Classification (Slides 65-66)

*   Traditionally, text classification uses a **bag-of-words representation** of text documents.
*   **Linear classifiers** such as Support Vector Machines or regularized Logistic Regression are commonly and effectively used with BOW representations.

---
