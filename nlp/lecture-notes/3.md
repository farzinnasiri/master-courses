Okay, here are the lecture notes for "Searching Text" and "Clustering Text" based on the slides you provided.

---

# Lecture Notes: Searching Text

**Course:** Natural Language Processing
**Lecturer:** Mark Carman
**Textbook Reference:** "An Introduction to Information Retrieval" by Christopher D. Manning, Prabhakar Raghavan, & Hinrich Schütze

---

## Contents (Slide 2)

1.  What is Information Retrieval?
2.  Term weighting
3.  Building Indices
4.  Crawling
5.  Training a Reranker
6.  Evaluating search
**Textbook:** "An Introduction to Information Retrieval" by Christopher D. Manning, Prabhakar Raghavan, & Hinrich Schütze
    *   Free online: [https://nlp.stanford.edu/IR-book/information-retrieval-book.html](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)

---

## I. What is Information Retrieval? (Slides 3-7)

### A. Definition (Slide 4)

*   Information Retrieval (IR) is the task of **finding content** (documents, images, video, etc.) that is **useful (i.e., relevant)** to a user's **information need**.
*   Examples of information needs:
    *   "My inauguration crowd was the biggest ever. Period! Somebody find me some pictures!" (Image search for Trump inauguration)
    *   "My fingers look pretty big to me. They can’t be shorter than the average, can they?" (Searching for data on finger length)
    *   "Hmm. Those banana and apple pancakes look so good! I wonder how you make them …" (Recipe search)

### B. From Information Needs to Query Keywords (Slide 5)

*   From information needs, we extract **query keywords**.
*   The system then looks for documents containing those keywords.
*   **Process illustrated:**
    *   Information Need: "Hmm. Those banana and apple pancakes look so good! I wonder how you make them …"
    *   Query Keywords: `banana`, `apple`, `pancakes`, `make`
    *   Large collection (billions of documents) indexed by search engine.
    *   Filtering:
        *   Millions of docs contain "banana".
        *   Thousands of docs contain "banana" + "apple".
        *   Hundreds of docs contain "banana" + "apple" + "pancakes".
        *   Tens of docs contain "banana" + "apple" + "pancakes" + "make" (mostly relevant documents).

### C. Is Web Search Hard? (Slide 6)

*   **Not entirely straightforward:**
    *   Typical adult vocabulary: 20-35 thousand words.
    *   Typical document length: < 1000 words (so document vocabulary is small).
*   **Heavy lifting by vocabulary matching:**
    *   Assuming vocabulary is well-distributed.
    *   Need fast indexes to find documents with selected keywords.
*   **Challenges:**
    *   What if **no document** contains all query terms?
    *   What if **many documents** contain all query terms?
*   **Solutions:**
    *   Assign scores to keywords based on their **discriminative** power.
    *   Expand document representation (e.g., page importance/PageRank) and train an ML model.
    *   *Note:* Average document length for web crawl (ClueWeb09) is only 756 words.

### D. Is Retrieval Just Text Classification? (Slide 7)

*   **Why not just train a classifier?**
    *   Concatenate query and document bag-of-words into a single feature vector.
    *   Predict the probability that the user finds the document relevant to the query.
*   **Problems with this approach:**
    *   **Linear classifiers are unsuitable:** They wouldn't capture interactions between query and document terms.
    *   **Non-linear models (with pairwise interactions):**
        *   For a vocabulary of 100k, could require up to 10 billion parameters for all pairwise interactions.
        *   Needs huge amounts of training data: (query, document, relevance_label).
        *   Retrieval would be very slow if needing to iterate over all documents to calculate a score for each.
*   **Future Note:** The idea of treating retrieval as a classification/regression problem will be revisited (Learning to Rank).

---

## II. Term Weighting (Slides 8-19)

*Some terms are more discriminative than others, making them more useful for identifying relevant documents.*

### A. Query Term Subsets (Slide 9)

*   **Scenario:** Searching for "giant tree frogs". No single document in Wikipedia contains all 3 keywords.
*   **Strategy:** Drop one keyword at a time to find subsets:
    *   ("giant", "tree"): many documents.
    *   ("giant", "frog"): a couple of documents.
    *   ("tree", "frog"): a few documents.
*   **Relevance:** The smallest set of documents (from the most specific keyword subset) is likely the most relevant.
*   **Goal:** Develop a principled way to extend this idea into an efficient ranking algorithm.

### B. Motivating IDF (Inverse Document Frequency) Weighting (Slide 10)

*   Estimate how many documents would be returned for a given query term subset.
*   Rank documents by how **unlikely** it was to see so many query terms in them.
*   **Probability of a random document `d'` containing keywords `q = {t₁, ..., t|q|}`:**
    *   Assuming term independence: `P(q ⊆ d') = Π_{t∈q} P(t ∈ d') = Π_{t∈q} (dft / N)`
    *   Where:
        *   `dft`: Document frequency of term `t` (number of docs in corpus containing `t`).
        *   `N`: Total number of documents in the corpus.
*   **Ranking by unlikelihood (1/P):**
    *   Score by `1/P`. Taking the logarithm makes the score additive:
    *   `score(d) = -log Π_{t∈q∩d} P(t ∈ d') = Σ_{t∈q∩d} log (N / dft)`
    *   This is the basis for **Inverse Document Frequency (IDF)**.

### C. Information Theory & Odds Variant (Slide 11)

*   **Inverse Document Frequency (IDF):**
    *   Weights each term `t` by `idft = log (N / dft)`.
*   **Standard Information Theory:**
    *   Information gained from observing term `t`: `info(t) = -log P(t)`.
    *   Amount of information = surprise at observing the term.
*   **Common IDF Variant (using odds):**
    *   Uses odds of observing term: `odds(t) = P(t) / (1 - P(t))`.
    *   Resulting document score (often with smoothing):
        `score(d) = Σ_{t∈q} log ( (N - dft + 0.5) / (dft + 0.5) )`
    *   Smoothing (adding 0.5) prevents terms with small frequencies from dominating.
    *   Little difference between IDF formulations unless a term is very common (`dft > N/2`).

### D. Term Weighting – TF-IDF (Slides 12-14)

*   **IDF weights vocabulary terms, but documents contain more info than just vocabulary (Slide 13):**
    *   Some documents contain the same query term **many times** and are more likely to be relevant.
*   **TF-IDF combines Term Frequency (TF) with IDF:**
    *   Simplest option: `score(q,d) = Σ_{t∈q} tft,d * log (N / dft)`
    *   Where `tft,d` = # of occurrences of term `t` in document `d` (term frequency).
*   **Motivation for TF:**
    *   Instead of `P(term in doc)`, consider `P(term appears k times)`.
    *   `P(t,k) ≈ P(next-token=t)ᵏ`
    *   `P(next-token=t) = ctft / Σ_{t'} ctft'` (where `ctft` is collection term frequency).
    *   This leads to a score like `- Σ_{t∈q} tft,d * log(ctft / Σ_{t'} ctft')`.
    *   Replacing collection frequency (`ctft`) with document frequency (`dft`) gives something close to TF-IDF and can be more robust.
*   **Variants of TF-IDF (Slide 14):**
    *   TF-IDF performs well but assumes a linear relationship between term frequency and document score.
    *   **Is this linear assumption valid?** Probably not. Doubling occurrences of a term shouldn't necessarily double the score. Score should improve, but not linearly (diminishing returns).
    *   **Common alternative (logarithmic TF):**
        *   `log(1 + tft,d)` or `max(0, 1 + log(tft,d))` (to handle `tft,d=0`).

### E. Length Normalization (Slides 15-18)

*   **Need to normalize for document length (Slide 16):**
    *   Longer documents naturally have larger vocabularies and are more likely to contain query terms.
    *   But they are not necessarily more useful.
    *   Shorter documents with the same term count should be preferred.
*   **How to normalize?**
    *   Divide by document length (L₁ norm) - done in some Language Modeling approaches.
    *   More common: Use L₂ normalization.
*   **Cosine Similarity between TFIDF Vectors (Slide 17):**
    *   Vector Space Model treats TF-IDF values for all terms as a vector representation `d = (tf₁,idf₁, ..., tfₙ,idfₙ)`.
    *   Similarity is computed based on the **angle** between query and document vectors.
    *   `sim(d₁, d₂) = (d₁ ⋅ d₂) / (||d₁|| ⋅ ||d₂||)`
        *   Where `d₁` = query, `d₂` = document.
        *   `||x|| = √(Σ xᵢ²)` (Euclidean length / L₂ norm).
    *   Cosine similarity normalizes by vector length (L₂ norm), not token count (L₁ norm).
*   **Alternative Length Normalization: Pivoted Length Normalisation (PLN) (Slide 18):**
    *   **Idea:** Generally, longer documents *do* contain more information, but full normalization loses all length information.
    *   PLN parameterizes L₁ normalization around the average document length (`Lave`).
    *   Formula: `tft,d / Ld`  ->  `tft,d / (b*Ld + (1-b)*Lave)`
        *   `Ld = Σ_{t'} tft',d` (length of doc `d`)
        *   `Lave = (1/N) Σ_d Ld` (average doc length)
        *   `0 ≤ b ≤ 1` (smoothing parameter).

### F. Term Weighting – BM25 (Okapi BM25) (Slide 19)

*   Pivoted length normalization leads to the venerable **Okapi BM25** ranking formula.
*   `RSVd = Σ_{t∈q} [log (N / dft)] * [(k₁ + 1)tft,d / (k₁((1-b) + b(Ld/Lave)) + tft,d)]`
    *   `k₁`, `b` are parameters to be set (tuned).
        *   `k₁` controls TF scaling (how quickly TF saturates).
        *   `b` controls length normalization.
*   **Why "BM25"?** BM = Best Match; it was literally the 25th formula they tried.
*   **GOTO method for term-based text retrieval:**
    *   Has stood the test of time.
    *   **Nice properties:**
        *   Term importance (TF component) asymptotes, so a term repeated massively won't dominate the score.
    *   Parameters control dependence on document length. Default values (`k₁` between 1.2 & 2, `b`=0.75) are often ok but can be improved with validation.

---

## III. Index Structures (Slides 20-22)

### A. Under the Hood of a Search Engine (Slide 21)

*   Retrieval measures must be calculated **fast** (delay affects attention).
    *   Search engines respond in tenths of a second.
*   **Inverted Indices:** Building blocks of search engines.
    *   Made up of **Posting Lists**: `TermID => List_of_DocumentIDs (containing the term)`.
    *   Often store additional info like term positions or frequencies.
    *   Use integer compression algorithms for fast decompression and reduced space.
*   **Calculating retrieval function involves joins over posting lists:**
    *   Documents in posting lists are often sorted (e.g., by term count/TF) for **early termination** of results list computation.
    *   **Index pruning** techniques remove documents that would never be retrieved for a certain query.
    *   Ref: [http://engineering.nyu.edu/~suel/queryproc/](http://engineering.nyu.edu/~suel/queryproc/)

### B. Positional Indices (Slide 22)

*   Documents are more likely relevant if query terms appear **close together**.
    *   Most indices record locations (positions) of terms in documents to calculate proximity.
*   Words at the **start of a webpage** are generally more important.
*   **Statistically significant bigrams/trigrams:**
    *   Found using pointwise mutual information.
    *   Often indexed with their own posting lists (treated as single terms).
    *   Example: "white house" (official residence) vs. "white" and "house" appearing separately.

---

## IV. Crawling (Slides 23-24)

### A. Web Crawlers (Slide 23)

*   Scour the web by following hyperlinks to find pages to add to the index.
*   **Efficient crawling requires:**
    *   Learning to **prioritize URLs** effectively.
    *   Determining how frequently to **re-visit** a website to update the index.
*   **Web scale crawlers need to be robust:**
    *   Handle all types of content, including generated pages.
    *   Perform content-based **duplicate page detection** (many URLs can map to the same content).
    *   Often use a distributed crawler architecture with a centralized URL list.
*   **Respect robots.txt files:**
    *   A text file in the root directory of a website tells crawlers what content they can/cannot crawl.

### B. Crawlers - Aside: Dark Web (Slide 24)

*   **Dark Web:** Anonymous web built around TOR (The Onion Router) gateways.
    *   Often associated with illicit content.
*   Crawling the Dark Web is interesting because there's no DNS linking URLs to IP addresses behind TOR gateways.

---

## V. Learning to Rerank (LTR) (Slides 25-32)

*Once an initial set of potentially relevant documents has been found, why not rerank them based on all available information to improve/maximize overall quality of search results?*

### A. Why Rerank? (Slide 26)

*   For web search, many indicative features can be used beyond simple keyword matching:
    *   Multiple **retrieval functions** (BM25, Embeddings-based, BERT, etc.).
    *   Different **document parts/views** (titles, anchor-text, bookmarks).
    *   **Query-independent features** (PageRank, HITS, spam scores).
    *   **Personalized information** (user click history).
    *   **Context features** (location, time, previous query in session).
*   Search engines (e.g., Google) combine **hundreds of signals**.
    *   Google's major ranking aspects (2017): incoming links, content, technical quality, past user CTR.
*   **Rank learning provides an automated & coherent method:**
    *   For combining diverse signals into a single retrieval score.
    *   While optimizing a measure users care about (e.g., NDCG, ERR).

### B. Generating Dataset for LTR (Slides 27-32)

*   **Process:**
    1.  **Start with a query** (e.g., "Tourism Amsterdam") (Slide 27).
    2.  **Generate initial ranking** using a keyword-based ranker (e.g., BM25 scores) (Slide 27).
    3.  **Truncate ranking @ k** to get candidates for re-ranking (e.g., top 8 docs) (Slide 28).
    4.  **Calculate feature values** for each candidate document (e.g., bm25_title, anchortext score, PageRank, ...) (Slide 29).
    5.  **Normalize each feature** at the query level (e.g., min-max normalization) to make values comparable across different queries (Slide 30).
    6.  **TRAINING: Provide ground-truth relevance labels** for each query-document pair (manual labeling by human annotators) (Slide 31).
        *   Each row (query-doc pair, feature vector, relevance label) becomes a training instance.
    7.  **Repeat for all queries** in the training set to build a large dataset (Slide 32).

---

## VI. Evaluating Search Results (Slides 33-35)

### A. Gathering Relevance Judgments (Slide 34)

*   Search engines employ human **raters** to annotate search results with relevance information.
    *   Google has detailed guidelines for its Quality Raters.
*   Other organizations (e.g., Wikipedia) may collect judgments directly from users ("Would this document have been relevant to query...?").
*   Usually, models are **not trained directly from click data** because it can cause a feedback loop.
*   **Latest approach:** Using Large Language Models (LLMs) to judge relevance.

### B. Metrics for Evaluating Search Results (Slide 35)

*   **Traditional Measures:**
    *   **Precision at depth k (P@k):** `#{Relevant docs in top k} / k`. (Percentage of top k results that are relevant).
    *   **Recall at depth k (R@k):** `#{Relevant docs in top k} / #{Relevant docs in total}`. (Percentage of all relevant documents found in top k).
    *   **F-measure at depth k (F₁@k):** `2 * (P@k * R@k) / (P@k + R@k)`. (Harmonic mean of P@k and R@k).
*   **More Recent Measures:**
    *   **MAP (Mean Average Precision):**
        *   "Average Precision" (AveP) is the average of P@k values at all rank positions containing relevant documents for a single query. `AveP = (Σ_{k=1 to n} (P(k) * rel(k))) / (number of relevant documents)`.
        *   MAP is the mean of AveP scores over a set of queries.
        *   Estimates area under the precision-recall curve.
    *   **NDCG@k (Normalized Discounted Cumulative Gain):**
        *   More faithful to user experience by discounting lower-ranked documents (users care more about top results).
        *   Normalized at the query level (scores between 0 and 1).
        *   `DCG@k = Σ_{i=1 to k} (relᵢ / log₂(i+1))`
        *   `NDCG@k = DCG@k / IDCG@k` (where IDCG is DCG of the ideal ranking).
        *   Formula on slide: `NDCG(Q, k) = (1/|Q|) Σ_{j=1 to |Q|} Z_{kj} Σ_{m=1 to k} (2^{R(j,m)} - 1) / log₂(1+m)` (This is a common formulation where `R(j,m)` is relevance score of doc at rank `m` for query `j`, `Z_kj` is normalization factor).
*   **Note:** P@k and NDCG@k are usually the most important measures for retrieval.

---

## VII. Formulating Learning-to-Rank Problem (Slides 36-40)

### A. Two-Stage (Re)ranking Process (Slide 37)

*   **Training Phase:**
    1.  Training Queries -> Document Collection -> Top k Retrieval (initial ranker).
    2.  Results from (1) -> Labeling (get relevance judgments).
    3.  Results from (1) -> Feature Extraction.
    4.  Labeled Data + Features -> Training Set.
    5.  Training Set -> Learning f(x) (train the reranking model).
*   **Usage Phase:**
    1.  Live Queries -> Document Collection -> Top k Retrieval (initial ranker).
    2.  Results from (1) -> Feature Extraction.
    3.  Features + Parameters of f(x) (from training) -> Applying f(x) (rerank).
    4.  Output: Ranked Lists of Docs.

### B. Rank Learning – Treat as Regression Problem (Slide 38)

*   LTR can be treated as a simple **regression problem**:
    *   Predict the relevance label based on feature values.
    *   Standard regression techniques can be applied.
*   **Caveat:** Optimizing a pointwise regression objective (like MSE) is **suboptimal** for ranking.
    *   It doesn't directly optimize the order or preference for documents at the *top of the list*.
    *   It might end up predicting well scores for less relevant documents overall, rather than focusing on the top-k quality.
    *   *Graph:* Shows "Regressing Relevance Labels" with `Relevance = 2^label - 1` vs. `Feature Value`.

### C. Loss Functions in Learning to Rank (Slide 39)

*   During learning, the loss function can be defined in 3 ways:
    *   **Pointwise (e.g., MSE):** Considers each document independently.
        *   `total_loss = Σ_{i=1 to m} Σ_{j=1 to n_qi} loss(f(x_i^j), y_i^j)`
        *   `f()` = predicted score, `y` = relevance label, `x` = feature vector.
    *   **Pairwise (e.g., # incorrectly ordered pairs):** Considers pairs of documents and aims to get their relative order correct.
        *   `total_loss = Σ_{i=1 to m} Σ_{j=1 to n_qi} Σ_{k=j+1 to n_qi} loss(f(x_i^j), f(x_i^k), y_i^j, y_i^k)`
    *   **Listwise (e.g., NDCG):** Considers the entire list of documents for a query and directly optimizes a list-based ranking metric.
        *   `total_loss = Σ_{i=1 to m} loss(f(x_i^1), ..., f(x_i^{n_qi}), y_i^1, ..., y_i^{n_qi})`

### D. LambdaMART (Slide 40)

*   A **listwise** rank learner that uses **boosted regression trees**.
*   **Name comes from:**
    *   **Lambda (λ):** An approximation of the loss gradient (specifically, the "lambda gradients" are derived from changes in NDCG or other listwise metrics).
    *   **MART:** Multiple Additive Regression Trees.
*   **Performance:** Performs very well in practice and has become a default/baseline learner in many applications.

---

## VIII. Conclusions (Slide 41-42)

In this lecture, we have:
*   Introduced classic **term weighting schemes** (TF-IDF, BM25) and query-document **similarity measures** (cosine similarity) used in information retrieval.
*   Discussed the basics of **web crawling**.
*   Introduced the **learning-to-rank (reranking) methodology** for combining many relevance signals into a single retrieval score.

---
---

# Lecture Notes: Clustering Text

**Course:** Natural Language Processing
**Lecturer:** Mark Carman
**Date:** 3 March 2025
**Textbook Reference:** "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition" by Daniel Jurafsky and James H. Martin.

---

## Lecture Content (Slide 2)

1.  Why cluster text?
2.  Similarities between documents
3.  Basic clustering algorithms
4.  Topic Modelling
5.  Visualisation via dimensionality reduction

---

## I. Why Cluster Text? (Slides 3-5)

### A. What is Text Clustering? (Slide 4)

*   Process of grouping documents into **coherent subgroups**.
*   Based on some **distance function** or **similarity measure**.
*   For text documents, this usually means grouping by:
    *   **Same topic:** E.g., sports news vs. politics vs. technology.
    *   **Same emotion:** E.g., angry, disappointed, elated, hopeful.
    *   **Same problem to be fixed:** E.g., complaints about faulty thermostat, printer issues.
    *   **Same complaint/praise:** E.g., hotel reviews.
    *   **Same author:** E.g., Shakespeare, Dylan, Eminem.
    *   **Same text generator:** E.g., for detecting spam emails.
    *   **Same source document:** E.g., for de-duplicating content during a web crawl.
*   *Diagrams:* Illustrate documents as points in a 2D space, initially scattered, then grouped into distinct clusters.

### B. Example Clustering Applications (Slide 5)

*   **News Aggregators (e.g., Google News):**
    *   Use clustering to **summarise/characterise** a collection:
        *   What **topics** are being discussed?
        *   How **prevalent** is each topic?
        *   What topics are **trending**?
        *   Which articles are most **exemplary** (central to a topic)?
*   **Security Applications (e.g., phishing, malware detection):**
    *   Determine which groups of communications/files might be malicious (based on a few identified instances).
    *   Use clusters to discover useful **features** for describing malicious/benign documents.
    *   Identify what aspects distinguish the groups.

---

## II. Representing Documents & Measuring Similarity (Slides 6-7)

### A. Document Similarity (Slide 7)

*   To cluster documents, we need to measure similarity between them based on their content.
    *   This is subjective (e.g., is Stephen King closer to Agatha Christie or J.K. Rowling?).
*   **Traditional Representation:** **tf-idf vectors** (scaled bag-of-words).
    *   Sparse representation (most values in the vector are zero).
    *   Similarity is based on the number of **shared words**, scaled by the **importance** of those words (how rare are they? - IDF).
*   **Assumption:** Similarity doesn't depend on the length of documents.
    *   Therefore, compute **cosine similarity** between vectors.
    *   `sim(d₁, d₂) = (d₁ ⋅ d₂) / (||d₁|| ⋅ ||d₂||)` (dot product between length-normalized vectors).
    *   `||x|| = √(Σ xᵢ²)` (L₂ norm).

---

## III. Clustering Techniques/Algorithms (Slides 8-21)

### A. Types of Clustering Algorithms (Slide 9)

*   Many different algorithms are in common use:
    *   k-Means / k-Medoids
    *   Agglomerative Hierarchical Clustering
    *   Density-based clustering (e.g., DBSCAN)
    *   Spectral clustering
    *   Topic Models
*   All these are implemented in `sklearn` (scikit-learn): [sklearn.cluster](https://scikit-learn.org/stable/modules/clustering.html)
*   This lecture will concentrate on those most used with text: **k-Means & Topic Models**.
*   *Diagrams:* Show different ways data points can be clustered (globular, hierarchical/dendrogram, density-based with noise).

### B. k-Means Clustering (Slides 10-13)

*   **Overview (Slide 11):**
    *   A "go-to" clustering method: simple, fast, and relatively robust.
    *   **How it works:**
        *   Searches for exactly **k clusters** (k is user-defined).
        *   Represents each cluster by its **centroid** (mean of the points in the cluster).
        *   Centroid: `μ_C = (1/|C|) Σ_{x∈C} x`
        *   Uses **Euclidean distance**: `d(x,y) = √(Σ (xᵢ - yᵢ)²)`.
*   **Advantages (Slide 11):**
    *   Scales well to large collections.
    *   Requires no pairwise distance calculations between all points (only point-to-centroid).
*   **Disadvantages (Slide 11):**
    *   Searches for **globular clusters**.
    *   Implicitly assumes **Euclidean distance metric**, which is not ideal for text (cosine similarity is preferred for tf-idf).
    *   The number of clusters (`k`) must be specified in advance.
*   **k-Means Process (Slide 12):**
    1.  **Initialize** `k` centroids randomly.
    2.  **Assign** each data point to the closest centroid.
    3.  **Recompute** centroids by averaging data points in each new cluster.
    4.  **Repeat** steps (2) & (3) until convergence (centroids stop moving significantly).
    *   The algorithm minimizes the variance of clusters (sum of squared distances between points and their cluster centroids): `min Σ_{i=1 to k} Σ_{x∈Cᵢ} d(x, μ_Cᵢ)²`.
*   **Potential Problems with k-Means (Slide 13):**
    *   Choosing the "right" value of `k` is critical (can use the "elbow method" to help select `k`).
    *   The algorithm can converge on a **local minimum** (run the algorithm multiple times with different random initializations).
    *   **Scaling effects:** The similarity measure (and thus feature scaling) affects the clusters found. For text, tf-idf weighting and document length normalization are important pre-steps.
    *   *Diagrams:* Illustrate `too small k` (clusters merge), `too big k` (clusters split), `local minimum` (suboptimal groups), `different scaling` (different clusters).

### C. k-Medoids (Slides 14-15)

*   Represents each cluster by its **medoid** rather than its centroid.
*   **Medoid:** One of the actual data points from the dataset that is "most central" (e.g., point with the smallest average distance to all other points in the cluster).
*   **Algorithm Iteration:**
    1.  Reassign data points to the cluster with the closest medoid.
    2.  Recompute medoids for each cluster.
*   **Advantage:**
    *   Can be used with other distance metrics, not just Euclidean.
    *   Medoid is a real document, providing a realistic representation of the cluster.
*   **Disadvantage:**
    *   Much **higher computational complexity** than k-Means.
    *   Needs to calculate distances between pairs of data points (or candidate medoids and points), which can be `O(n²)` in naive implementations or still more complex than k-Means.

### D. Agglomerative Hierarchical Clustering (Slides 16-18)

*   **Overview (Slide 17):**
    *   Builds a **hierarchy** of clusters called a **dendrogram**.
    *   Agglomerative clustering builds clusters by **merging groups** in a bottom-up fashion:
        1.  Assign each document to its own group (cluster).
        2.  Merge the 2 groups (clusters) that are **most similar**.
        3.  Repeat until only one group (cluster) remains.
*   **Linking Criteria (Slide 18):**
    *   Needs to compute distances **between groups** of documents.
    *   Uses a **linkage criterion**:
        *   **Complete-linkage:** Maximum distance between any two points in the two groups.
        *   **Single-linkage:** Minimum distance between any two points in the two groups.
        *   **Average-linkage:** Average distance between all pairs of points (one from each group).
    *   The linking criterion influences the shape of clusters found:
        *   Complete- or average-linkage tend to find tight, globular clusters.
        *   Single-linkage can find long, thin ("chain-like") clusters.
*   **Advantages (Slide 18):**
    *   Works with *any* distance metric / similarity function.
    *   The dendrogram provides information about the underlying structure of the data and allows for choosing different numbers of clusters by "cutting" the dendrogram at different levels.
*   **Disadvantage (Slide 18):**
    *   High time complexity (typically `O(n² log n)` or `O(n³)` depending on implementation and linkage) makes it unsuitable for very large datasets.

### E. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) (Slides 19-21)

*   **What is it? (Slide 20):**
    *   The number of clusters is **not defined by the user** but inferred from the data.
    *   A density-based algorithm: clusters are found in high-density regions, separated by low-density regions.
*   **Parameters:**
    *   `ε` (epsilon): Radius of the neighborhood around each point.
    *   `minPoints`: Minimum number of points required within `ε` to form a dense region (core point).
*   **How it works? (Point Classification) (Slide 20):**
    *   The # of data points in an `ε`-neighborhood is a density estimate. `minPoints` is a threshold on this density.
    *   Algorithm classifies each point as:
        *   **Core point:** Has ≥ `minPoints` neighbors (including itself) inside its `ε`-radius.
        *   **Border point:** Not a core point, but is within `ε` of a core point.
        *   **Noise point:** Neither a core nor a border point.
    *   Clusters are formed by connecting core points that are density-reachable from each other, and then including their border points.
*   **Pros and Cons (Slide 21):**
    *   **Advantages:**
        *   No need to choose the number of clusters in advance.
        *   Identifies and ignores noise/outlier instances.
        *   Can find arbitrarily (oddly) shaped clusters.
    *   **Disadvantages:**
        *   Performance depends critically on chosen parameters (`ε`, `minPoints`). (Usually, `ε` is set as a function of the average distance to `minPoints` nearest neighbors – e.g., using a k-distance graph).
        *   May not work well if clusters have very different densities.

---

## IV. Topic Modelling (Slides 22-27)

### A. What are Topic Models? (Slide 23)

*   Perform **soft clustering** of documents: allows documents to belong to multiple clusters (topics) with different probabilities.
*   Provide a **low-dimensional representation** of documents compared to the high-dimensional vocabulary space.
*   Each cluster is referred to as a **"topic"**:
    *   Described by a **distribution over words**.
    *   Terms in a topic should be coherent (semantically related, discussing a single theme).
*   **Topic vector:** Probability distribution over words for a given topic (e.g., `P(elephant | topic3) = 0.09`).
*   **Document vector:** Probability distribution over topics for a given document (e.g., `P(topic3 | doc7) = 0.1`).
*   *Diagrams:* Show "hard clustering" (documents belong to one cluster) vs. "soft clustering" (documents can belong to multiple overlapping clusters). Bar charts illustrate `P(word|topic)` and `P(topic|document)`.

### B. Matrix Decomposition (Slide 24)

*   Topic Modeling is a form of matrix decomposition.
*   It decomposes a **terms-documents count matrix** (or tf-idf matrix).
*   Into 2 smaller matrices: **terms-topics** and **topics-documents**.
    *   `Terms x Documents ≈ (Terms x Topics) x (Topics x Documents)`
*   A much smaller number of parameters need to be estimated: `V*T + T*D <<< V*D` (where `V`=vocab size, `D`=doc count, `T`=topic count).
*   **Most famous technique: Latent Dirichlet Allocation (LDA)**
    *   Named for using a Dirichlet prior when estimating parameters.
    *   Related to Non-negative Matrix Factorization (NMF).
    *   Also related to Latent Semantic Indexing (LSI), which applies Singular Value Decomposition (SVD) to a TF-IDF matrix.

### C. What is Topic Modelling Useful For? (Slide 25)

*   Factorization/clustering **generalizes** observed term counts:
    *   Makes document representations more **dense & useful**.
    *   Allows calculating more **meaningful distances** between documents (semantic similarity rather than just lexical overlap).
*   Deals with problems of:
    *   **Polysemy:** A word with multiple meanings (e.g., "bank" - financial institution or edge of a river?). Topic models can associate "bank" with different topics depending on context.
    *   **Synonymy:** Different words with similar meanings (e.g., "cancer" = "oncology"?). Documents about "cancer" and "oncology" might be grouped into the same topic.
    *   **Short documents:** Limited vocabulary makes similarity hard; topic models can infer underlying topics.
*   Sometimes useful for **visualizing collections** (often after further dimensionality reduction like t-SNE).

### D. Details of LDA Generative Model (Slide 26)

*   **Generative model for Latent Dirichlet Allocation (LDA):** Describes how documents are assumed to be generated.
    1.  For each topic, choose word proportions (a distribution over words, `φ_z ~ Dirichlet(β)`).
    2.  For each document, choose topic proportions (a distribution over topics, `θ_d ~ Dirichlet(α)`).
    3.  For each word position in a document:
        1.  Choose a **topic** `z` based on the document's topic proportions (`z_i ~ Discrete(θ_d)`).
        2.  Choose a **word** `w` based on the chosen topic's word proportions (`w_i ~ Discrete(φ_z)`).
*   **Estimating parameters for the topic model:**
    *   Requires **iteratively** updating parameters (e.g., using Gibbs sampling).
        1.  Update topic probabilities for each document.
        2.  Update word probabilities for each topic.
    *   Place Bayesian **priors** on parameters (Dirichlet priors) to avoid over-fitting.
    *   Use sampling techniques (like Gibbs sampling) to find a good (though possibly local) maximum of the posterior probability.
    *   **Hyperparameters `α` and `β`** determine the prior on the topic/document distributions (how concentrated or sparse they are expected to be).

### E. Applications of Topic Models (Slide 27)

*   Useful for **visualizing collections**:
    *   Represent topics by their most frequent/characteristic terms.
    *   Common also to show changes in topic prevalence or content over time.
*   *Diagram:* t-SNE visualization of LDA topics from tweets, where clusters of points are labeled with representative terms.

---

## V. Conclusions (Slides 28-29)

*   There are many applications of text clustering.
*   Many clustering techniques (k-Means, Hierarchical, DBSCAN, etc.) can be applied to tf-idf vectors representing documents.
*   Topic modeling (e.g., LDA) builds a lower-dimensional representation of documents by performing soft clustering, identifying latent topics as distributions over words and representing documents as distributions over these topics.

---
