<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Natural Language Processing - Study Guide</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" rel="stylesheet">
    <style>
        :root {
            --primary-color: #4B5563;
            --secondary-color: #60A5FA;
            --background-color: #F9FAFB;
            --text-color: #1F2937;
            --code-background: #F3F4F6;
            --highlight-color: #DBEAFE;
            --border-color: #E5E7EB;
        }
        
        html {
            scroll-behavior: smooth;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: var(--background-color);
            color: var(--text-color);
            line-height: 1.6;
        }
        
        .sidebar {
            height: calc(100vh - 60px);
            overflow-y: auto;
            position: sticky;
            top: 60px;
        }
        
        .content {
            max-width: 1200px;
            margin: 0 auto;
        }
        
        code {
            background-color: var(--code-background);
            padding: 2px 4px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        
        pre {
            background-color: var(--code-background);
            border-radius: 6px;
            padding: 1rem;
            overflow-x: auto;
            margin: 1rem 0;
        }
        
        .nav-link {
            transition: background-color 0.2s ease;
        }
        
        .nav-link:hover {
            background-color: var(--highlight-color);
        }
        
        .nav-link.active {
            background-color: var(--highlight-color);
            border-left: 4px solid var(--secondary-color);
        }
        
        .search-results {
            max-height: 300px;
            overflow-y: auto;
        }
        
        .section-header {
            border-bottom: 2px solid var(--border-color);
            padding-bottom: 0.5rem;
        }
        
        .example-box {
            background-color: var(--highlight-color);
            border-left: 4px solid var(--secondary-color);
            padding: 1rem;
            border-radius: 0 6px 6px 0;
            margin: 1rem 0;
        }
        
        .collapsible-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }
        
        .concept-card {
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
            transition: transform 0.2s ease;
        }
        
        .concept-card:hover {
            transform: translateY(-3px);
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        
        /* For PDF export optimization */
        @media print {
            .sidebar, .search-container, .mobile-menu-button {
                display: none !important;
            }
            
            .content-container {
                margin-left: 0 !important;
                width: 100% !important;
            }
            
            .section {
                page-break-inside: avoid;
                margin-bottom: 2rem;
            }
            
            html, body {
                width: 100%;
                margin: 0;
                padding: 0;
            }
            
            .hidden-print {
                display: none !important;
            }
        }
    </style>
</head>
<body>
    <!-- Header -->
    <header class="bg-white shadow fixed w-full z-10">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-3 flex justify-between items-center">
            <div class="flex items-center">
                <button id="mobile-menu-button" class="md:hidden mr-3 text-gray-600 hover:text-gray-900">
                    <i class="fas fa-bars text-xl"></i>
                </button>
                <h1 class="text-xl font-bold text-gray-800">Natural Language Processing</h1>
            </div>
            <div class="search-container relative">
                <div class="flex items-center bg-gray-100 rounded-lg px-3 py-2">
                    <i class="fas fa-search text-gray-500 mr-2"></i>
                    <input id="search-input" type="text" placeholder="Search topics..." class="bg-transparent border-none focus:outline-none text-sm w-48 md:w-64">
                </div>
                <div id="search-results" class="search-results absolute right-0 top-full mt-2 bg-white rounded-lg shadow-lg p-3 w-72 hidden z-10">
                    <!-- Search results will be inserted here -->
                </div>
            </div>
        </div>
    </header>

    <div class="container mx-auto pt-16 flex">
        <!-- Sidebar Navigation -->
        <aside id="sidebar" class="sidebar bg-white shadow w-64 transform -translate-x-full md:translate-x-0 fixed md:relative z-10 transition-transform duration-300 ease-in-out">
            <nav class="p-4">
                <div class="mb-6">
                    <h2 class="text-lg font-bold text-gray-700 mb-2">Contents</h2>
                    <ul class="nav-list">
                        <li>
                            <a href="#introduction" class="nav-link block px-3 py-2 rounded-md text-sm">1. Introduction to NLP</a>
                        </li>
                        <li>
                            <a href="#classifying-text" class="nav-link block px-3 py-2 rounded-md text-sm">2. Classifying Text</a>
                        </li>
                        <li>
                            <a href="#searching-text" class="nav-link block px-3 py-2 rounded-md text-sm">3. Searching Text</a>
                        </li>
                        <li>
                            <a href="#language-models" class="nav-link block px-3 py-2 rounded-md text-sm">4. Language Models and Embeddings</a>
                        </li>
                        <li>
                            <a href="#sequence-classifiers" class="nav-link block px-3 py-2 rounded-md text-sm">5. Sequence Classifiers and Labellers</a>
                        </li>
                        <li>
                            <a href="#seq2seq" class="nav-link block px-3 py-2 rounded-md text-sm">6. Sequence2Sequence Models & Transformers</a>
                        </li>
                        <li>
                            <a href="#applications" class="nav-link block px-3 py-2 rounded-md text-sm">7. Applications of Transformers</a>
                        </li>
                    </ul>
                </div>

                <div>
                    <h2 class="text-lg font-bold text-gray-700 mb-2">Resources</h2>
                    <ul>
                        <li>
                            <a href="#references" class="nav-link block px-3 py-2 rounded-md text-sm">References & Textbooks</a>
                        </li>
                        <li>
                            <a href="#glossary" class="nav-link block px-3 py-2 rounded-md text-sm">NLP Glossary</a>
                        </li>
                    </ul>
                </div>
            </nav>
        </aside>

        <!-- Main Content Area -->
        <main class="content-container w-full md:w-3/4 lg:w-4/5 p-6 md:ml-64">
            <div class="content">
                
                <!-- Introduction Section -->
                <section id="introduction" class="mb-12 section">
                    <h2 class="text-2xl font-bold mb-4 section-header">1. Introduction to NLP</h2>
                    
                    <div class="mb-6">
                        <p class="mb-4">Natural Language Processing (NLP) is a field at the intersection of computer science, artificial intelligence, and linguistics. It focuses on the interaction between computers and human language, particularly how to program computers to process and analyze large amounts of natural language data.</p>
                        
                        <div class="concept-card">
                            <h3 class="text-lg font-semibold mb-2">What is Natural Language Processing?</h3>
                            <p>NLP enables machines to understand, interpret, and generate human language in a valuable way. It bridges the gap between human communication and computer understanding.</p>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Core NLP Tasks</h3>
                        
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Text Classification</h4>
                                <p>Categorizing text documents into predefined classes (e.g., spam detection, sentiment analysis).</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Information Retrieval</h4>
                                <p>Finding relevant information or documents in response to a query (e.g., search engines).</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Named Entity Recognition</h4>
                                <p>Identifying entities such as persons, organizations, locations in text.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Machine Translation</h4>
                                <p>Automatically translating text from one language to another.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Question Answering</h4>
                                <p>Providing specific answers to natural language questions.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Text Summarization</h4>
                                <p>Creating concise summaries of longer documents while preserving key information.</p>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">NLP Pipeline</h3>
                        <p class="mb-3">Most NLP systems follow a common processing pipeline:</p>
                        
                        <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                            <ol class="list-decimal ml-5 space-y-2">
                                <li><strong>Text Preprocessing</strong>: Cleaning text, normalizing, handling special characters</li>
                                <li><strong>Tokenization</strong>: Breaking text into words, phrases, symbols, or other meaningful elements</li>
                                <li><strong>Normalization</strong>: Converting text to a more uniform format (lowercasing, stemming, lemmatization)</li>
                                <li><strong>Feature Extraction</strong>: Converting text into numerical features machines can process</li>
                                <li><strong>Model Application</strong>: Using machine learning models for the specific NLP task</li>
                                <li><strong>Post-processing</strong>: Refining and presenting results</li>
                            </ol>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Challenges in NLP</h3>
                        
                        <div class="example-box">
                            <h4 class="font-medium mb-2">Ambiguity in Language</h4>
                            <p class="mb-2">Words and sentences can have multiple meanings:</p>
                            <ul class="list-disc ml-5">
                                <li><strong>Lexical ambiguity</strong>: "Bank" can refer to a financial institution or the side of a river</li>
                                <li><strong>Syntactic ambiguity</strong>: "I saw the man with the telescope" (Who has the telescope?)</li>
                                <li><strong>Referential ambiguity</strong>: "After John hit Bob, he fell down" (Who fell down?)</li>
                            </ul>
                        </div>
                        
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mt-4">
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Contextual Understanding</h4>
                                <p>Understanding context is critical but challenging for machines (e.g., sarcasm, humor, cultural references).</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Linguistic Variations</h4>
                                <p>Differences in dialects, slang, idioms, and grammatical structures across languages and regions.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Out-of-Vocabulary Words</h4>
                                <p>Handling words not seen during training, including neologisms and domain-specific terminology.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">World Knowledge</h4>
                                <p>NLP systems often lack the common sense and world knowledge that humans use to understand language.</p>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Evolution of NLP Approaches</h3>
                        
                        <div class="overflow-auto">
                            <table class="min-w-full bg-white border border-gray-300">
                                <thead>
                                    <tr>
                                        <th class="py-2 px-4 border-b text-left">Era</th>
                                        <th class="py-2 px-4 border-b text-left">Approach</th>
                                        <th class="py-2 px-4 border-b text-left">Characteristics</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td class="py-2 px-4 border-b">1950s-1980s</td>
                                        <td class="py-2 px-4 border-b">Rule-based systems</td>
                                        <td class="py-2 px-4 border-b">Manually crafted linguistic rules and patterns</td>
                                    </tr>
                                    <tr>
                                        <td class="py-2 px-4 border-b">1980s-2000s</td>
                                        <td class="py-2 px-4 border-b">Statistical NLP</td>
                                        <td class="py-2 px-4 border-b">Probabilistic models trained on large corpora (e.g., n-gram models, HMMs)</td>
                                    </tr>
                                    <tr>
                                        <td class="py-2 px-4 border-b">2000s-2010s</td>
                                        <td class="py-2 px-4 border-b">Machine Learning</td>
                                        <td class="py-2 px-4 border-b">Feature engineering with traditional ML algorithms (SVMs, MaxEnt models)</td>
                                    </tr>
                                    <tr>
                                        <td class="py-2 px-4 border-b">2010s-Present</td>
                                        <td class="py-2 px-4 border-b">Deep Learning</td>
                                        <td class="py-2 px-4 border-b">Neural networks with word embeddings, RNNs, LSTMs, and transformer-based models</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>

                    <div>
                        <h3 class="text-xl font-semibold mb-3">Practical NLP Applications</h3>
                        
                        <div class="grid grid-cols-1 md:grid-cols-3 gap-4">
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Virtual Assistants</h4>
                                <p>Siri, Alexa, Google Assistant use NLP to understand and respond to user queries</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Customer Service</h4>
                                <p>Chatbots and automated support systems to handle customer inquiries</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Healthcare</h4>
                                <p>Medical records analysis, symptom checking, clinical decision support</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Finance</h4>
                                <p>Sentiment analysis for market prediction, fraud detection, compliance monitoring</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Education</h4>
                                <p>Automated essay scoring, personalized learning, language learning tools</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Social Media</h4>
                                <p>Content moderation, trend analysis, personalized content recommendation</p>
                            </div>
                        </div>
                    </div>
                </section>
                
                <!-- Classifying Text Section -->
                <section id="classifying-text" class="mb-12 section">
                    <h2 class="text-2xl font-bold mb-4 section-header">2. Classifying Text</h2>
                    
                    <div class="mb-6">
                        <p class="mb-4">Text classification is the task of assigning predefined categories to text documents. It's one of the fundamental tasks in NLP with applications ranging from spam detection to sentiment analysis and topic categorization.</p>
                        
                        <div class="concept-card">
                            <h3 class="text-lg font-semibold mb-2">What is Text Classification?</h3>
                            <p>Text classification involves automatically categorizing text documents into one or more predefined classes based on their content. The classification can be binary (two classes), multi-class (multiple mutually exclusive classes), or multi-label (a document can belong to multiple classes simultaneously).</p>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Common Text Classification Tasks</h3>
                        
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Sentiment Analysis</h4>
                                <p>Determining whether a piece of text expresses positive, negative, or neutral sentiment.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Spam Detection</h4>
                                <p>Identifying whether an email or message is spam or legitimate.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Topic Categorization</h4>
                                <p>Classifying documents into topical categories (e.g., sports, politics, technology).</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Language Identification</h4>
                                <p>Determining which language a text is written in.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Intent Recognition</h4>
                                <p>Identifying the intention behind a user query (e.g., booking, information seeking).</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Toxicity Detection</h4>
                                <p>Identifying harmful, offensive, or inappropriate content.</p>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Text Classification Pipeline</h3>
                        
                        <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                            <ol class="list-decimal ml-5 space-y-3">
                                <li>
                                    <strong>Data Collection and Preparation</strong>
                                    <p>Gather labeled text data for training and testing. The quality and representativeness of this data will significantly impact the performance of your classifier.</p>
                                </li>
                                <li>
                                    <strong>Text Preprocessing</strong>
                                    <p>Clean and normalize text data:</p>
                                    <ul class="list-disc ml-5">
                                        <li>Lowercasing</li>
                                        <li>Removing punctuation and special characters</li>
                                        <li>Removing stopwords (common words like "the", "and", "is")</li>
                                        <li>Stemming or lemmatization (reducing words to their root form)</li>
                                        <li>Handling numbers, URLs, and other special tokens</li>
                                    </ul>
                                </li>
                                <li>
                                    <strong>Feature Extraction</strong>
                                    <p>Convert text into numerical features that algorithms can process:</p>
                                    <ul class="list-disc ml-5">
                                        <li>Bag of Words (BoW): Simple word count or presence</li>
                                        <li>TF-IDF (Term Frequency-Inverse Document Frequency): Weighted word frequencies</li>
                                        <li>N-grams: Sequences of N consecutive words</li>
                                        <li>Word embeddings: Dense vector representations (Word2Vec, GloVe)</li>
                                    </ul>
                                </li>
                                <li>
                                    <strong>Model Selection and Training</strong>
                                    <p>Choose and train a classification algorithm on the feature-transformed data.</p>
                                </li>
                                <li>
                                    <strong>Evaluation</strong>
                                    <p>Assess model performance using metrics like accuracy, precision, recall, F1-score, and confusion matrices.</p>
                                </li>
                                <li>
                                    <strong>Deployment and Monitoring</strong>
                                    <p>Put the model into production and continuously monitor performance.</p>
                                </li>
                            </ol>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Feature Extraction Techniques</h3>
                        
                        <div class="example-box mb-4">
                            <h4 class="font-medium mb-2">Bag of Words (BoW) Example</h4>
                            <p class="mb-2">Consider these two short documents:</p>
                            <ul class="list-disc ml-5 mb-3">
                                <li>Document 1: "I love natural language processing"</li>
                                <li>Document 2: "Natural language processing is fascinating"</li>
                            </ul>
                            <p class="mb-2">The vocabulary is: {"I", "love", "natural", "language", "processing", "is", "fascinating"}</p>
                            <p class="mb-2">BoW representation:</p>
                            <ul class="list-disc ml-5">
                                <li>Document 1: [1, 1, 1, 1, 1, 0, 0] (counts of each word in the vocabulary)</li>
                                <li>Document 2: [0, 0, 1, 1, 1, 1, 1]</li>
                            </ul>
                        </div>
                        
                        <div class="example-box mb-4">
                            <h4 class="font-medium mb-2">TF-IDF Example</h4>
                            <p class="mb-2">TF-IDF weights terms based on their frequency in a document and rarity across documents.</p>
                            <p>For the term "natural" in the above documents:</p>
                            <ul class="list-disc ml-5">
                                <li>Term Frequency (TF): How often "natural" appears in each document (normalized)</li>
                                <li>Inverse Document Frequency (IDF): log(Total # of documents / # of documents containing "natural")</li>
                                <li>TF-IDF = TF × IDF</li>
                            </ul>
                            <p class="mt-2">Common terms like "the" get lower weights, while distinctive terms get higher weights.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4 class="font-medium mb-2">Word Embeddings</h4>
                            <p class="mb-2">Word embeddings map words to dense vector representations where semantically similar words are close in the vector space.</p>
                            <p class="mb-2">For example, in a well-trained embedding space:</p>
                            <ul class="list-disc ml-5">
                                <li>vector("king") - vector("man") + vector("woman") ≈ vector("queen")</li>
                                <li>Words like "happy", "joyful", and "cheerful" would be close together</li>
                            </ul>
                            <p>Popular word embedding models include Word2Vec, GloVe, and FastText.</p>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Classification Algorithms</h3>
                        
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Naive Bayes</h4>
                                <p>A probabilistic classifier based on Bayes' theorem with strong independence assumptions between features. Simple and efficient, particularly for text classification.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Support Vector Machines (SVM)</h4>
                                <p>Finds the hyperplane that best separates different classes. Effective in high-dimensional spaces like those created by text features.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Decision Trees & Random Forests</h4>
                                <p>Decision trees split data based on feature values. Random forests combine multiple trees to improve performance and reduce overfitting.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Logistic Regression</h4>
                                <p>A linear model for classification that uses the logistic function to predict the probability of a class. Simple, interpretable, and often effective.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Neural Networks</h4>
                                <p>Multi-layer perceptrons, CNNs, RNNs, and transformer-based models that can learn complex patterns in text data.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">k-Nearest Neighbors (k-NN)</h4>
                                <p>Classifies a document based on the majority class of its k nearest neighbors in the feature space.</p>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Naive Bayes Classifier</h3>
                        <p class="mb-3">Naive Bayes is particularly popular for text classification due to its simplicity and effectiveness.</p>
                        
                        <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                            <h4 class="font-medium mb-2">The Naive Bayes Formula for Text Classification</h4>
                            <p class="mb-2">For a document d and class c:</p>
                            <p class="mb-3">P(c|d) ∝ P(c) × ∏ P(term_i|c)</p>
                            <p class="mb-2">Where:</p>
                            <ul class="list-disc ml-5">
                                <li>P(c|d) is the probability of class c given document d</li>
                                <li>P(c) is the prior probability of class c</li>
                                <li>P(term_i|c) is the probability of term_i occurring in class c</li>
                            </ul>
                            <p class="mt-3">The "naive" part comes from the assumption that all terms are conditionally independent given the class, which is not true in practice but works surprisingly well.</p>
                        </div>
                        
                        <div class="example-box mt-4">
                            <h4 class="font-medium mb-2">Naive Bayes Example</h4>
                            <p class="mb-2">Task: Classify an email as spam or not spam.</p>
                            <p class="mb-2">Training data:</p>
                            <ul class="list-disc ml-5 mb-3">
                                <li>60% of emails are not spam, 40% are spam</li>
                                <li>The word "money" appears in 5% of non-spam emails and 70% of spam emails</li>
                                <li>The word "friend" appears in 65% of non-spam emails and 10% of spam emails</li>
                            </ul>
                            <p class="mb-2">For a new email containing "money" and "friend":</p>
                            <p class="mb-1">P(spam|email) ∝ P(spam) × P(money|spam) × P(friend|spam)</p>
                            <p class="mb-1">P(spam|email) ∝ 0.4 × 0.7 × 0.1 = 0.028</p>
                            <p class="mb-1">P(not spam|email) ∝ P(not spam) × P(money|not spam) × P(friend|not spam)</p>
                            <p class="mb-1">P(not spam|email) ∝ 0.6 × 0.05 × 0.65 = 0.01950</p>
                            <p class="mt-2">Since P(spam|email) > P(not spam|email), the email would be classified as spam.</p>
                        </div>
                    </div>

                    <div>
                        <h3 class="text-xl font-semibold mb-3">Evaluation Metrics</h3>
                        
                        <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200 mb-4">
                            <h4 class="font-medium mb-2">Common Classification Metrics</h4>
                            
                            <div class="overflow-auto">
                                <table class="min-w-full border border-gray-300">
                                    <thead>
                                        <tr>
                                            <th class="py-2 px-4 border-b text-left">Metric</th>
                                            <th class="py-2 px-4 border-b text-left">Formula</th>
                                            <th class="py-2 px-4 border-b text-left">Description</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td class="py-2 px-4 border-b">Accuracy</td>
                                            <td class="py-2 px-4 border-b">(TP + TN) / (TP + TN + FP + FN)</td>
                                            <td class="py-2 px-4 border-b">Proportion of correctly classified instances</td>
                                        </tr>
                                        <tr>
                                            <td class="py-2 px-4 border-b">Precision</td>
                                            <td class="py-2 px-4 border-b">TP / (TP + FP)</td>
                                            <td class="py-2 px-4 border-b">Proportion of positive identifications that were actually correct</td>
                                        </tr>
                                        <tr>
                                            <td class="py-2 px-4 border-b">Recall</td>
                                            <td class="py-2 px-4 border-b">TP / (TP + FN)</td>
                                            <td class="py-2 px-4 border-b">Proportion of actual positives that were correctly identified</td>
                                        </tr>
                                        <tr>
                                            <td class="py-2 px-4 border-b">F1-Score</td>
                                            <td class="py-2 px-4 border-b">2 × (Precision × Recall) / (Precision + Recall)</td>
                                            <td class="py-2 px-4 border-b">Harmonic mean of precision and recall</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                            
                            <p class="mt-3">Where:</p>
                            <ul class="list-disc ml-5">
                                <li>TP = True Positives (correctly predicted positive class)</li>
                                <li>TN = True Negatives (correctly predicted negative class)</li>
                                <li>FP = False Positives (incorrectly predicted positive class)</li>
                                <li>FN = False Negatives (incorrectly predicted negative class)</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4 class="font-medium mb-2">When to Use Different Metrics</h4>
                            <ul class="list-disc ml-5">
                                <li><strong>Balanced Classes</strong>: Accuracy is generally suitable</li>
                                <li><strong>Imbalanced Classes</strong>: Precision, recall, F1-score are better</li>
                                <li><strong>Cost of False Positives is High</strong>: Focus on precision (e.g., spam filtering)</li>
                                <li><strong>Cost of False Negatives is High</strong>: Focus on recall (e.g., disease detection)</li>
                                <li><strong>Need Balance Between Precision and Recall</strong>: Use F1-score</li>
                            </ul>
                            <p class="mt-2">For multi-class problems, these metrics can be calculated using micro-averaging (aggregate the contributions of all classes) or macro-averaging (compute the metric independently for each class and take the average).</p>
                        </div>
                    </div>
                </section>
                
                <!-- Searching Text Section -->
                <section id="searching-text" class="mb-12 section">
                    <h2 class="text-2xl font-bold mb-4 section-header">3. Searching Text</h2>
                    
                    <div class="mb-6">
                        <p class="mb-4">Information Retrieval (IR) is the process of finding material (usually documents) that satisfies an information need from large collections (usually stored on computers). Search engines like Google, Bing, and academic search systems are examples of IR systems.</p>
                        
                        <div class="concept-card">
                            <h3 class="text-lg font-semibold mb-2">What is Information Retrieval?</h3>
                            <p>Information Retrieval focuses on finding relevant documents from a collection in response to a user query. It's about finding the right information at the right time in the right format to satisfy a user's information need.</p>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">The Information Retrieval Process</h3>
                        
                        <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                            <ol class="list-decimal ml-5 space-y-3">
                                <li>
                                    <strong>Document Collection</strong>
                                    <p>Gather and store the corpus of documents to be searched.</p>
                                </li>
                                <li>
                                    <strong>Indexing</strong>
                                    <p>Build efficient data structures (indexes) to allow quick searching.</p>
                                </li>
                                <li>
                                    <strong>Query Processing</strong>
                                    <p>Interpret and process the user's search query.</p>
                                </li>
                                <li>
                                    <strong>Matching & Ranking</strong>
                                    <p>Find documents that match the query and rank them by relevance.</p>
                                </li>
                                <li>
                                    <strong>Result Presentation</strong>
                                    <p>Present search results to the user in a useful format.</p>
                                </li>
                            </ol>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Building a Search Index</h3>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Inverted Index</h4>
                            <p class="mb-3">The fundamental data structure in information retrieval is the inverted index, which maps from terms to the documents containing them.</p>
                            
                            <div class="example-box">
                                <h5 class="font-medium mb-2">Inverted Index Example</h5>
                                <p class="mb-2">Consider these documents:</p>
                                <ul class="list-disc ml-5 mb-3">
                                    <li>Document 1: "The quick brown fox"</li>
                                    <li>Document 2: "Quick brown foxes jump"</li>
                                    <li>Document 3: "The lazy dog sleeps"</li>
                                </ul>
                                <p class="mb-2">A simplified inverted index might look like:</p>
                                <ul class="list-none ml-0 space-y-1">
                                    <li><strong>the</strong>: {Doc 1, Doc 3}</li>
                                    <li><strong>quick</strong>: {Doc 1, Doc 2}</li>
                                    <li><strong>brown</strong>: {Doc 1, Doc 2}</li>
                                    <li><strong>fox</strong>: {Doc 1}</li>
                                    <li><strong>foxes</strong>: {Doc 2}</li>
                                    <li><strong>jump</strong>: {Doc 2}</li>
                                    <li><strong>lazy</strong>: {Doc 3}</li>
                                    <li><strong>dog</strong>: {Doc 3}</li>
                                    <li><strong>sleeps</strong>: {Doc 3}</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Index Construction Process</h4>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <ol class="list-decimal ml-5 space-y-2">
                                    <li><strong>Document Collection</strong>: Gather all documents to be indexed</li>
                                    <li><strong>Tokenization</strong>: Split text into tokens (roughly words)</li>
                                    <li><strong>Normalization</strong>: Convert tokens to standard form (lowercase, remove punctuation)</li>
                                    <li><strong>Stemming/Lemmatization</strong>: Reduce words to their root forms</li>
                                    <li><strong>Stopword Removal</strong>: Filter out common words (optional)</li>
                                    <li><strong>Index Creation</strong>: Build the inverted index data structure</li>
                                    <li><strong>Optimization</strong>: Compress and optimize the index for efficient storage and retrieval</li>
                                </ol>
                            </div>
                        </div>

                        <div>
                            <h4 class="text-lg font-medium mb-2">Positional Index</h4>
                            <p class="mb-3">A positional index extends the inverted index by storing the positions of each term within documents. This enables phrase queries (e.g., "quick brown fox") and proximity searches (terms near each other).</p>
                            
                            <div class="example-box">
                                <h5 class="font-medium mb-2">Positional Index Example</h5>
                                <p class="mb-2">For Document 1: "The quick brown fox"</p>
                                <ul class="list-none ml-0 space-y-1">
                                    <li><strong>the</strong>: {Doc 1: [0]}</li>
                                    <li><strong>quick</strong>: {Doc 1: [1]}</li>
                                    <li><strong>brown</strong>: {Doc 1: [2]}</li>
                                    <li><strong>fox</strong>: {Doc 1: [3]}</li>
                                </ul>
                                <p class="mt-2">With this index, we can verify that "quick brown" appears as a phrase in Document 1 because "quick" is at position 1 and "brown" is at position 2 (adjacent).</p>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Term Weighting and Ranking</h3>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">TF-IDF Weighting</h4>
                            <p class="mb-3">Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic that reflects how important a word is to a document in a collection.</p>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">The TF-IDF Formula</h5>
                                <p class="mb-2">TF-IDF(t, d, D) = TF(t, d) × IDF(t, D)</p>
                                <p class="mb-2">Where:</p>
                                <ul class="list-disc ml-5">
                                    <li><strong>TF(t, d)</strong>: Term Frequency of term t in document d</li>
                                    <li><strong>IDF(t, D)</strong>: Inverse Document Frequency of term t in document collection D</li>
                                </ul>
                                <p class="mt-2">Term Frequency (TF) is often calculated as:</p>
                                <ul class="list-disc ml-5">
                                    <li>Raw count: Number of occurrences of term t in document d</li>
                                    <li>Boolean: 1 if term appears, 0 if not</li>
                                    <li>Logarithmically scaled: 1 + log(TF) if TF > 0, else 0</li>
                                    <li>Normalized: TF divided by the maximum term frequency in document</li>
                                </ul>
                                <p class="mt-2">Inverse Document Frequency (IDF) is typically calculated as:</p>
                                <p>IDF(t, D) = log(N / DF(t))</p>
                                <p>Where N is the total number of documents and DF(t) is the number of documents containing term t.</p>
                            </div>
                        </div>
                        
                        <div class="example-box mb-4">
                            <h4 class="font-medium mb-2">TF-IDF Example</h4>
                            <p class="mb-2">Consider three documents and the term "algorithm":</p>
                            <ul class="list-disc ml-5 mb-3">
                                <li>Document 1: "The algorithm runs efficiently" (1 occurrence of "algorithm")</li>
                                <li>Document 2: "This algorithm is better than that algorithm" (2 occurrences)</li>
                                <li>Document 3: "Data structures are important" (0 occurrences)</li>
                            </ul>
                            <p class="mb-2">For Document 1:</p>
                            <ul class="list-none ml-5 mb-2">
                                <li>TF("algorithm", Doc1) = 1/4 = 0.25 (normalized by document length)</li>
                                <li>IDF("algorithm", D) = log(3/2) ≈ 0.176 (appears in 2 out of 3 documents)</li>
                                <li>TF-IDF("algorithm", Doc1, D) = 0.25 × 0.176 ≈ 0.044</li>
                            </ul>
                            <p class="mb-2">For Document 2:</p>
                            <ul class="list-none ml-5">
                                <li>TF("algorithm", Doc2) = 2/7 ≈ 0.286</li>
                                <li>IDF("algorithm", D) = log(3/2) ≈ 0.176</li>
                                <li>TF-IDF("algorithm", Doc2, D) = 0.286 × 0.176 ≈ 0.050</li>
                            </ul>
                            <p class="mt-2">Document 2 has a higher TF-IDF score for "algorithm" than Document 1, reflecting that "algorithm" is more important to Document 2.</p>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">Vector Space Model</h4>
                            <p class="mb-3">The Vector Space Model represents documents and queries as vectors in a high-dimensional space, where each dimension corresponds to a term in the vocabulary.</p>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">Document and Query Vectors</h5>
                                <p class="mb-2">A document d is represented as:</p>
                                <p class="mb-2">d = (w₁,ₐ, w₂,ₐ, ..., wₙ,ₐ)</p>
                                <p class="mb-2">Where wᵢ,ₐ is the weight (often TF-IDF) of term i in document d.</p>
                                <p class="mb-2">Similarly, a query q is represented as:</p>
                                <p class="mb-2">q = (w₁,q, w₂,q, ..., wₙ,q)</p>
                                <p class="mb-3">The similarity between a document and a query can be calculated using the cosine similarity:</p>
                                <p>sim(d, q) = (d · q) / (|d| × |q|)</p>
                                <p class="mt-2">Where (d · q) is the dot product of the document and query vectors, and |d| and |q| are their magnitudes.</p>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Web Search and Crawling</h3>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Web Crawling</h4>
                            <p class="mb-3">Web crawlers (or spiders) are programs that systematically browse the web, following links from page to page, collecting information, and updating an index.</p>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">Basic Crawling Algorithm</h5>
                                <pre><code>function crawl(seed_urls):
    frontier = new Queue(seed_urls)
    discovered = new Set(seed_urls)
    
    while not frontier.isEmpty() and not reached_limit():
        url = frontier.dequeue()
        
        page = download_and_process(url)
        index_page(url, page)
        
        for link in extract_links(page):
            if link not in discovered:
                frontier.enqueue(link)
                discovered.add(link)
</code></pre>
                            </div>
                        </div>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Crawling Challenges</h4>
                            
                            <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Scale</h5>
                                    <p>The web contains billions of pages that need to be crawled and indexed.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Freshness</h5>
                                    <p>Pages change frequently, requiring regular recrawling to maintain an up-to-date index.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Politeness</h5>
                                    <p>Crawlers must respect robots.txt files and avoid overwhelming servers with too many requests.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Deep Web</h5>
                                    <p>Much of the web is not accessible by following links (e.g., content behind logins, dynamically generated pages).</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Duplicate Content</h5>
                                    <p>Many web pages contain identical or near-identical content.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Crawl Traps</h5>
                                    <p>Some sites have infinite loops of links or dynamically generated URLs that can trap crawlers.</p>
                                </div>
                            </div>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">PageRank Algorithm</h4>
                            <p class="mb-3">PageRank is a link analysis algorithm that assigns a numerical weight to each page based on the number and quality of links pointing to it. It was a key innovation in Google's original search algorithm.</p>
                            
                            <div class="example-box">
                                <h5 class="font-medium mb-2">How PageRank Works</h5>
                                <p class="mb-2">The basic intuition behind PageRank:</p>
                                <ul class="list-disc ml-5">
                                    <li>A page is important if many important pages link to it</li>
                                    <li>The importance of a page is divided among its outgoing links</li>
                                </ul>
                                <p class="mb-2 mt-2">The simplified PageRank formula:</p>
                                <p class="mb-2">PR(A) = (1-d) + d × (PR(T₁)/C(T₁) + PR(T₂)/C(T₂) + ... + PR(Tₙ)/C(Tₙ))</p>
                                <p>Where:</p>
                                <ul class="list-disc ml-5">
                                    <li>PR(A) is the PageRank of page A</li>
                                    <li>PR(Tᵢ) is the PageRank of pages Tᵢ that link to A</li>
                                    <li>C(Tᵢ) is the number of outbound links from page Tᵢ</li>
                                    <li>d is a damping factor (typically around 0.85)</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div>
                        <h3 class="text-xl font-semibold mb-3">Evaluating Search Systems</h3>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Evaluation Metrics</h4>
                            
                            <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Precision</h5>
                                    <p>The fraction of retrieved documents that are relevant.</p>
                                    <p class="mt-1">Precision = Relevant ∩ Retrieved / Retrieved</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Recall</h5>
                                    <p>The fraction of relevant documents that are retrieved.</p>
                                    <p class="mt-1">Recall = Relevant ∩ Retrieved / Relevant</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">F-measure</h5>
                                    <p>The harmonic mean of precision and recall.</p>
                                    <p class="mt-1">F₁ = 2 × (Precision × Recall) / (Precision + Recall)</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Mean Average Precision (MAP)</h5>
                                    <p>The mean of average precision scores for a set of queries.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Normalized Discounted Cumulative Gain (NDCG)</h5>
                                    <p>Measures the quality of ranking by considering the position of relevant documents.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Mean Reciprocal Rank (MRR)</h5>
                                    <p>Average of the reciprocal ranks of the first relevant result for a set of queries.</p>
                                </div>
                            </div>
                        </div>
                        
                        <div class="example-box">
                            <h4 class="font-medium mb-2">Precision and Recall Example</h4>
                            <p class="mb-2">Consider a search for "machine learning tutorials":</p>
                            <ul class="list-disc ml-5 mb-3">
                                <li>There are 20 relevant documents in the collection</li>
                                <li>The system retrieves 15 documents, of which 10 are relevant</li>
                            </ul>
                            <p class="mb-2">Precision = 10/15 = 0.67 (67% of retrieved documents are relevant)</p>
                            <p>Recall = 10/20 = 0.5 (50% of all relevant documents were retrieved)</p>
                            <p class="mt-3">There's often a trade-off between precision and recall. Returning more documents typically increases recall but may decrease precision, while being more selective improves precision but may reduce recall.</p>
                        </div>
                    </div>
                </section>
                
                <!-- Language Models and Embeddings Section -->
                <section id="language-models" class="mb-12 section">
                    <h2 class="text-2xl font-bold mb-4 section-header">4. Language Models and Word Embeddings</h2>
                    
                    <div class="mb-6">
                        <p class="mb-4">Language models are statistical models that capture the probability distribution of sequences of words. Word embeddings are dense vector representations of words that capture semantic relationships. These two concepts form the foundation of modern NLP systems.</p>
                        
                        <div class="concept-card">
                            <h3 class="text-lg font-semibold mb-2">What are Language Models?</h3>
                            <p>A language model computes the probability of a sequence of words. It can predict the likelihood of a word given its context, making it useful for tasks like text generation, speech recognition, machine translation, and spelling correction.</p>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">N-gram Language Models</h3>
                        <p class="mb-3">N-gram models are simple statistical language models that predict the probability of a word based on the N-1 preceding words.</p>
                        
                        <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200 mb-4">
                            <h4 class="font-medium mb-2">N-gram Probability</h4>
                            <p class="mb-2">The probability of a sequence of words w₁, w₂, ..., wₙ can be approximated as:</p>
                            <p class="mb-3">P(w₁, w₂, ..., wₙ) = P(w₁) × P(w₂|w₁) × P(w₃|w₁,w₂) × ... × P(wₙ|w₁,...,wₙ₋₁)</p>
                            <p class="mb-2">Using the Markov assumption, an n-gram model simplifies this to:</p>
                            <p>P(wᵢ|w₁,...,wᵢ₋₁) ≈ P(wᵢ|wᵢ₋ₙ₊₁,...,wᵢ₋₁)</p>
                            <p class="mt-3">For example, a trigram (n=3) model approximates P(wᵢ|w₁,...,wᵢ₋₁) as P(wᵢ|wᵢ₋₂,wᵢ₋₁).</p>
                        </div>
                        
                        <div class="example-box mb-4">
                            <h4 class="font-medium mb-2">N-gram Example</h4>
                            <p class="mb-2">Consider estimating probabilities from this corpus:</p>
                            <p class="italic mb-3">"I like machine learning. I like natural language processing."</p>
                            <p class="mb-2">For a bigram model (n=2):</p>
                            <ul class="list-none ml-5 mb-3">
                                <li>P(like|I) = count(I like) / count(I) = 2/2 = 1.0</li>
                                <li>P(machine|like) = count(like machine) / count(like) = 1/2 = 0.5</li>
                                <li>P(natural|like) = count(like natural) / count(like) = 1/2 = 0.5</li>
                            </ul>
                            <p class="mb-2">We can use this model to compute the probability of a new sentence:</p>
                            <p class="italic mb-2">"I like machine translation"</p>
                            <p>P(I like machine translation) = P(I) × P(like|I) × P(machine|like) × P(translation|machine)</p>
                            <p class="mt-2">Note: P(translation|machine) would be 0 in this simple model because "machine translation" never appears in our corpus, which highlights the need for smoothing techniques.</p>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">Smoothing Techniques</h4>
                            <p class="mb-3">Smoothing addresses the zero-probability problem by assigning some probability to unseen n-grams.</p>
                            
                            <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Laplace (Add-1) Smoothing</h5>
                                    <p>Add one to all n-gram counts.</p>
                                    <p class="mt-1">P(wᵢ|wᵢ₋₁) = (count(wᵢ₋₁,wᵢ) + 1) / (count(wᵢ₋₁) + V)</p>
                                    <p class="mt-1">Where V is vocabulary size.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Add-k Smoothing</h5>
                                    <p>Add a fraction k instead of 1.</p>
                                    <p class="mt-1">P(wᵢ|wᵢ₋₁) = (count(wᵢ₋₁,wᵢ) + k) / (count(wᵢ₋₁) + k×V)</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Good-Turing Smoothing</h5>
                                    <p>Estimates probability based on the frequency of n-grams that appear once.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Kneser-Ney Smoothing</h5>
                                    <p>A more sophisticated approach that considers the diversity of contexts in which words appear.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Neural Language Models</h3>
                        <p class="mb-3">Neural language models use neural networks to learn distributed representations of words and predict the probability of a sequence.</p>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Advantages over N-gram Models</h4>
                            
                            <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Better Generalization</h5>
                                    <p>Neural models can generalize to unseen word combinations by learning word similarities.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">No Sparsity Problem</h5>
                                    <p>Do not suffer from the curse of dimensionality that affects n-gram models with large n.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Contextual Understanding</h5>
                                    <p>Better capture long-range dependencies and contextual information.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Shared Parameters</h5>
                                    <p>Words with similar meanings share statistical strength through learned representations.</p>
                                </div>
                            </div>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">Types of Neural Language Models</h4>
                            
                            <div class="grid grid-cols-1 gap-4">
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Feedforward Neural Network LM</h5>
                                    <p>The first neural language model proposed by Bengio et al. (2003). It uses a fixed window of previous words as input.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Recurrent Neural Network LM (RNN-LM)</h5>
                                    <p>Uses recurrent connections to model sequences of arbitrary length, maintaining a hidden state that captures information from all previous words.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">LSTM and GRU Language Models</h5>
                                    <p>Use specialized RNN architectures (Long Short-Term Memory or Gated Recurrent Units) to better capture long-range dependencies.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Transformer-based LM</h5>
                                    <p>Uses self-attention mechanisms instead of recurrence, enabling parallel processing and better handling of long-range dependencies (e.g., BERT, GPT).</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Word Embeddings</h3>
                        <p class="mb-3">Word embeddings are dense vector representations of words in a continuous vector space where semantically similar words are mapped to nearby points.</p>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Why Word Embeddings?</h4>
                            
                            <div class="example-box">
                                <h5 class="font-medium mb-2">One-Hot Encoding vs. Word Embeddings</h5>
                                <p class="mb-2"><strong>One-Hot Encoding:</strong></p>
                                <ul class="list-disc ml-5 mb-3">
                                    <li>Each word is represented as a sparse vector with a 1 at its index position and 0s elsewhere</li>
                                    <li>For a vocabulary of 50,000 words, each word is a 50,000-dimensional vector</li>
                                    <li>All words are equidistant from each other (no notion of similarity)</li>
                                </ul>
                                <p class="mb-2"><strong>Word Embeddings:</strong></p>
                                <ul class="list-disc ml-5">
                                    <li>Each word is represented as a dense vector (typically 100-300 dimensions)</li>
                                    <li>Similar words have similar vector representations</li>
                                    <li>Enables mathematical operations that yield meaningful results (e.g., king - man + woman ≈ queen)</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Word2Vec</h4>
                            <p class="mb-3">Word2Vec is a popular method for learning word embeddings using shallow neural networks. It comes in two architectures:</p>
                            
                            <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Continuous Bag of Words (CBOW)</h5>
                                    <p>Predicts a target word from its context words.</p>
                                    <div class="mt-2 text-center">
                                        <p>Context → <strong>?</strong> ← Context</p>
                                        <p class="text-sm mt-1">"The [?] runs fast"</p>
                                    </div>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Skip-gram</h5>
                                    <p>Predicts context words from a target word.</p>
                                    <div class="mt-2 text-center">
                                        <p><strong>Target</strong> → Context words</p>
                                        <p class="text-sm mt-1">"dog" → "The", "runs", "fast"</p>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="example-box mt-4">
                                <h5 class="font-medium mb-2">Word2Vec Properties</h5>
                                <p class="mb-2">Word embeddings learn relationships like:</p>
                                <ul class="list-disc ml-5">
                                    <li><strong>Analogies</strong>: "king" is to "queen" as "man" is to "woman"</li>
                                    <li><strong>Clustering</strong>: Words with similar meanings have vectors close together</li>
                                    <li><strong>Dimensions</strong>: Different dimensions often capture different semantic or syntactic features</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">Other Word Embedding Methods</h4>
                            
                            <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">GloVe (Global Vectors)</h5>
                                    <p>Combines global matrix factorization and local context window methods, focusing on word-word co-occurrence statistics from a corpus.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">FastText</h5>
                                    <p>Extends Word2Vec by treating each word as a bag of character n-grams, enabling better handling of rare words and out-of-vocabulary words.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">ELMo (Embeddings from Language Models)</h5>
                                    <p>Generates contextual word embeddings using bidirectional LSTM language models, capturing word sense based on context.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">BERT Embeddings</h5>
                                    <p>Contextual embeddings from the BERT model, which uses transformer architecture and is pre-trained on masked language modeling and next sentence prediction tasks.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Contextual Word Embeddings</h3>
                        <p class="mb-3">Unlike static embeddings (Word2Vec, GloVe) that assign the same vector to a word regardless of context, contextual embeddings represent words based on their context.</p>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Benefits of Contextual Embeddings</h4>
                            
                            <div class="example-box">
                                <h5 class="font-medium mb-2">Disambiguation Example</h5>
                                <p class="mb-2">Consider the word "bank" in different contexts:</p>
                                <ul class="list-disc ml-5">
                                    <li>"I deposited money in the <strong>bank</strong>." (financial institution)</li>
                                    <li>"I sat on the <strong>bank</strong> of the river." (riverside)</li>
                                </ul>
                                <p class="mt-2">Static embeddings would assign the same vector to "bank" in both sentences.</p>
                                <p>Contextual embeddings would produce different vectors that capture the different meanings.</p>
                            </div>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">Pre-trained Language Models for Contextual Embeddings</h4>
                            
                            <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">ELMo</h5>
                                    <p>Uses bidirectional LSTM to generate contextualized word representations.</p>
                                    <p class="mt-1 text-sm">Pre-training: Language modeling (predict next/previous word)</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">BERT</h5>
                                    <p>Uses bidirectional transformer architecture to generate deep contextualized representations.</p>
                                    <p class="mt-1 text-sm">Pre-training: Masked language modeling + Next sentence prediction</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">GPT (Generative Pre-trained Transformer)</h5>
                                    <p>Uses unidirectional transformer architecture (looks at previous words only).</p>
                                    <p class="mt-1 text-sm">Pre-training: Language modeling (predict next word)</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">RoBERTa</h5>
                                    <p>A robustly optimized version of BERT with improved training methodology.</p>
                                    <p class="mt-1 text-sm">Pre-training: Masked language modeling (no next sentence prediction)</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div>
                        <h3 class="text-xl font-semibold mb-3">Evaluating Language Models and Embeddings</h3>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Language Model Evaluation</h4>
                            
                            <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Perplexity</h5>
                                    <p>A measure of how well a language model predicts a sample.</p>
                                    <p class="mt-1">PPL(W) = 2^(-1/N * Σ log₂ P(wᵢ|w₁,...,wᵢ₋₁))</p>
                                    <p class="mt-1">Lower perplexity indicates better model performance.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Bits Per Character (BPC)</h5>
                                    <p>Similar to perplexity but applied at the character level.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Downstream Task Performance</h5>
                                    <p>How well the language model serves as a foundation for specific NLP tasks.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Human Evaluation</h5>
                                    <p>Assessing the quality of text generated by the language model.</p>
                                </div>
                            </div>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">Word Embedding Evaluation</h4>
                            
                            <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Word Similarity</h5>
                                    <p>Compare model's similarity scores between word pairs with human judgments.</p>
                                    <p class="mt-1 text-sm">Datasets: WordSim-353, SimLex-999, RG-65</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Word Analogy</h5>
                                    <p>Evaluate model's ability to solve analogies like "king:queen::man:woman".</p>
                                    <p class="mt-1 text-sm">Datasets: Google analogy dataset, BATS</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Downstream Task Performance</h5>
                                    <p>How well embeddings perform when used for tasks like named entity recognition, sentiment analysis, etc.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Intrinsic Evaluation</h5>
                                    <p>Analyze clustering, neighborhood structure, and other properties of the embedding space.</p>
                                </div>
                            </div>
                            
                            <div class="example-box mt-4">
                                <h5 class="font-medium mb-2">Word Analogy Example</h5>
                                <p class="mb-2">Given word vectors, we can solve analogies by vector arithmetic:</p>
                                <p class="mb-2">"king is to queen as man is to ???"</p>
                                <p class="mb-2">vec("king") - vec("man") + vec("woman") ≈ vec("queen")</p>
                                <p>We find the word whose vector is closest to the result of this calculation.</p>
                            </div>
                        </div>
                    </div>
                </section>
                
                <!-- Sequence Classifiers and Labellers Section -->
                <section id="sequence-classifiers" class="mb-12 section">
                    <h2 class="text-2xl font-bold mb-4 section-header">5. Sequence Classifiers and Labellers</h2>
                    
                    <div class="mb-6">
                        <p class="mb-4">Sequence classification and sequence labeling are fundamental tasks in NLP that involve making predictions for entire sequences or individual tokens within sequences, respectively.</p>
                        
                        <div class="concept-card">
                            <h3 class="text-lg font-semibold mb-2">Understanding Sequence Problems in NLP</h3>
                            <p><strong>Sequence Classification</strong>: Assigning a single label to an entire sequence (e.g., sentiment analysis for a sentence).</p>
                            <p class="mt-2"><strong>Sequence Labeling</strong>: Assigning a label to each element in a sequence (e.g., part-of-speech tagging for each word in a sentence).</p>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Common Sequence Labeling Tasks</h3>
                        
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Part-of-Speech (POS) Tagging</h4>
                                <p>Labeling each word with its grammatical category (noun, verb, adjective, etc.).</p>
                                <p class="mt-2 text-sm italic">"I like natural language processing."</p>
                                <p class="text-sm">[PRON, VERB, ADJ, NOUN, NOUN, PUNCT]</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Named Entity Recognition (NER)</h4>
                                <p>Identifying and categorizing named entities (persons, organizations, locations, etc.).</p>
                                <p class="mt-2 text-sm italic">"Apple is looking at buying U.K. startup for $1 billion."</p>
                                <p class="text-sm">[ORG, O, O, O, O, O, LOC, O, O, MONEY]</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Chunking</h4>
                                <p>Identifying phrases or constituents in a sentence (noun phrases, verb phrases, etc.).</p>
                                <p class="mt-2 text-sm italic">"[NP The quick brown fox] [VP jumps over] [NP the lazy dog]."</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Semantic Role Labeling</h4>
                                <p>Identifying the semantic relationships between predicates and arguments in a sentence.</p>
                                <p class="mt-2 text-sm italic">"[Agent John] [Predicate gave] [Theme a book] [Recipient to Mary]."</p>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Sequential Models</h3>
                        <p class="mb-3">These models explicitly take into account the sequential nature of language data.</p>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Hidden Markov Models (HMMs)</h4>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">Key Components of HMMs</h5>
                                <ul class="list-disc ml-5">
                                    <li><strong>States</strong>: The hidden labels we want to predict (e.g., POS tags)</li>
                                    <li><strong>Observations</strong>: The words in the sequence</li>
                                    <li><strong>Transition Probabilities</strong>: P(tag_i | tag_i-1) - probability of transitioning from one tag to another</li>
                                    <li><strong>Emission Probabilities</strong>: P(word_i | tag_i) - probability of a word given a tag</li>
                                    <li><strong>Initial Probabilities</strong>: P(tag_i) - probability of starting with a particular tag</li>
                                </ul>
                                <p class="mt-3">HMMs assume that:</p>
                                <ol class="list-decimal ml-5">
                                    <li>The current state depends only on the previous state (first-order Markov assumption)</li>
                                    <li>The current observation depends only on the current state</li>
                                </ol>
                            </div>
                            
                            <div class="example-box mt-4">
                                <h5 class="font-medium mb-2">HMM Example for POS Tagging</h5>
                                <p class="mb-2">For the sentence "I like fish":</p>
                                <p class="mb-2">We want to find the most likely sequence of POS tags [t₁, t₂, t₃] given the words [I, like, fish].</p>
                                <p class="mb-2">Using the Viterbi algorithm, we compute:</p>
                                <p>argmax P(t₁, t₂, t₃ | I, like, fish)</p>
                                <p class="mt-2">This involves calculating:</p>
                                <ul class="list-disc ml-5">
                                    <li>P(t₁) × P(I | t₁) for all possible values of t₁</li>
                                    <li>P(t₂ | t₁) × P(like | t₂) for all possible values of t₂</li>
                                    <li>P(t₃ | t₂) × P(fish | t₃) for all possible values of t₃</li>
                                </ul>
                                <p class="mt-2">The most likely sequence might be [PRON, VERB, NOUN].</p>
                            </div>
                        </div>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Conditional Random Fields (CRFs)</h4>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">Advantages of CRFs over HMMs</h5>
                                <ul class="list-disc ml-5">
                                    <li><strong>Discriminative vs. Generative</strong>: CRFs directly model P(tags | words) rather than P(words, tags)</li>
                                    <li><strong>Feature Flexibility</strong>: CRFs can incorporate arbitrary, potentially overlapping features</li>
                                    <li><strong>Global Normalization</strong>: CRFs normalize over the entire sequence, avoiding the label bias problem</li>
                                    <li><strong>Contextual Features</strong>: Can use features from anywhere in the input sequence, not just the current position</li>
                                </ul>
                                <p class="mt-3">CRFs are particularly effective for tasks like NER and POS tagging, especially when rich feature sets are available.</p>
                            </div>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">Recurrent Neural Networks (RNNs)</h4>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">RNNs for Sequence Labeling</h5>
                                <p class="mb-2">RNNs process sequences by maintaining a hidden state that captures information from previous timesteps:</p>
                                <p class="mb-2">h_t = f(h_{t-1}, x_t)</p>
                                <p class="mb-2">y_t = g(h_t)</p>
                                <p class="mb-3">Where:</p>
                                <ul class="list-disc ml-5">
                                    <li>h_t is the hidden state at time t</li>
                                    <li>x_t is the input at time t (e.g., word embedding)</li>
                                    <li>y_t is the output at time t (e.g., tag probability)</li>
                                    <li>f and g are functions with learned parameters</li>
                                </ul>
                            </div>
                            
                            <div class="mt-4">
                                <h5 class="text-lg font-medium mb-2">Bidirectional RNNs</h5>
                                <p class="mb-3">Bidirectional RNNs process the sequence in both forward and backward directions, allowing the model to access both past and future context.</p>
                                
                                <div class="example-box">
                                    <h5 class="font-medium mb-2">Bidirectional RNN Advantage</h5>
                                    <p class="mb-2">Consider the sentence: "The <strong>bank</strong> by the river is beautiful."</p>
                                    <p class="mb-3">To correctly tag "bank" as a natural feature rather than a financial institution, it helps to see the future context "by the river".</p>
                                    <p>A bidirectional RNN has:</p>
                                    <ul class="list-disc ml-5">
                                        <li>A forward RNN that processes the sequence from left to right</li>
                                        <li>A backward RNN that processes the sequence from right to left</li>
                                        <li>The final representation for each token combines information from both directions</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Long Short-Term Memory (LSTM) Networks</h3>
                        <p class="mb-3">LSTMs are a specialized RNN architecture designed to better capture long-range dependencies in sequence data.</p>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">LSTM Architecture</h4>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">LSTM Cell Components</h5>
                                <ul class="list-disc ml-5">
                                    <li><strong>Cell State</strong>: The memory of the cell, which can maintain information over long sequences</li>
                                    <li><strong>Forget Gate</strong>: Decides what information to throw away from the cell state</li>
                                    <li><strong>Input Gate</strong>: Decides what new information to store in the cell state</li>
                                    <li><strong>Output Gate</strong>: Decides what parts of the cell state to output</li>
                                </ul>
                                <p class="mt-3">These gates use sigmoid activations (0-1) to control the flow of information, allowing LSTMs to selectively remember or forget information over long sequences.</p>
                            </div>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">BiLSTM-CRF Architecture</h4>
                            <p class="mb-3">A common and powerful architecture for sequence labeling combines bidirectional LSTMs with a CRF layer.</p>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">BiLSTM-CRF Components</h5>
                                <ol class="list-decimal ml-5">
                                    <li><strong>Word Embedding Layer</strong>: Converts words to dense vector representations</li>
                                    <li><strong>Bidirectional LSTM Layer</strong>: Processes the sequence in both directions</li>
                                    <li><strong>Linear Layer</strong>: Maps LSTM outputs to tag scores for each possible tag</li>
                                    <li><strong>CRF Layer</strong>: Models tag dependencies to find the optimal tag sequence</li>
                                </ol>
                                <p class="mt-3">The BiLSTM captures contextual word representations, while the CRF layer ensures that the predicted tag sequence follows valid transition patterns (e.g., an adjective is more likely to be followed by a noun than a verb).</p>
                            </div>
                            
                            <div class="example-box mt-4">
                                <h5 class="font-medium mb-2">Why Combine BiLSTM and CRF?</h5>
                                <p class="mb-2">Consider NER for: "Washington visited Washington."</p>
                                <ul class="list-disc ml-5">
                                    <li>The BiLSTM provides contextual representations for each "Washington" based on its surrounding words</li>
                                    <li>The CRF layer models tag dependencies (e.g., B-PER is likely to be followed by I-PER)</li>
                                    <li>The combined model might correctly label the first "Washington" as a person (B-PER) and the second as a location (B-LOC)</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Transformer-Based Models for Sequence Labeling</h3>
                        <p class="mb-3">Modern approaches to sequence labeling often use transformer-based models like BERT, which have become state-of-the-art for many NLP tasks.</p>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Fine-tuning BERT for Sequence Labeling</h4>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">Process</h5>
                                <ol class="list-decimal ml-5">
                                    <li><strong>Input Preparation</strong>: Tokenize the text using BERT's WordPiece tokenizer</li>
                                    <li><strong>Token Embeddings</strong>: Pass tokens through the pre-trained BERT model</li>
                                    <li><strong>Output Layer</strong>: Add a classification layer on top of BERT's contextual embeddings</li>
                                    <li><strong>Fine-tuning</strong>: Train the entire model on labeled sequence data</li>
                                </ol>
                                <p class="mt-3">For subword tokenization, a common approach is to use the embedding of the first subword token for the whole word, or to align labels with subwords during training and aggregate predictions during inference.</p>
                            </div>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">BERT + CRF</h4>
                            <p class="mb-3">Similar to BiLSTM-CRF, adding a CRF layer on top of BERT can further improve performance for sequence labeling tasks.</p>
                            
                            <div class="example-box">
                                <h5 class="font-medium mb-2">BERT + CRF for NER</h5>
                                <p class="mb-2">Architecture:</p>
                                <ol class="list-decimal ml-5">
                                    <li>Input text: "Apple is looking at buying U.K. startup for $1 billion"</li>
                                    <li>BERT tokenization and processing generates contextualized embeddings</li>
                                    <li>A linear layer maps these to tag scores for each token</li>
                                    <li>The CRF layer finds the optimal tag sequence: [B-ORG, O, O, O, O, B-LOC, O, O, B-MONEY, I-MONEY]</li>
                                </ol>
                                <p class="mt-2">This approach combines BERT's powerful contextual representations with the CRF's ability to model tag dependencies.</p>
                            </div>
                        </div>
                    </div>

                    <div>
                        <h3 class="text-xl font-semibold mb-3">Evaluation Metrics for Sequence Labeling</h3>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Token-Level Metrics</h4>
                            
                            <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Accuracy</h5>
                                    <p>Proportion of tokens that are correctly labeled.</p>
                                    <p class="mt-1 text-sm">Accuracy = (Correctly labeled tokens) / (Total tokens)</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Per-Class Precision, Recall, F1</h5>
                                    <p>Evaluate performance on each class separately.</p>
                                    <p class="mt-1 text-sm">F1 = 2 × (Precision × Recall) / (Precision + Recall)</p>
                                </div>
                            </div>
                        </div>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Entity-Level Metrics (for NER)</h4>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">Evaluation Schemes</h5>
                                <ul class="list-disc ml-5">
                                    <li><strong>Exact Match</strong>: An entity is correct only if both its boundaries and type match the gold standard</li>
                                    <li><strong>Partial Match</strong>: Give partial credit for partially overlapping entity spans</li>
                                    <li><strong>Type Match</strong>: Consider only entity type, ignoring boundaries</li>
                                    <li><strong>Boundary Match</strong>: Consider only entity boundaries, ignoring type</li>
                                </ul>
                            </div>
                            
                            <div class="example-box mt-4">
                                <h5 class="font-medium mb-2">NER Evaluation Example</h5>
                                <p class="mb-2">Gold standard: "The [University of Washington]<sub>ORG</sub> is in [Seattle]<sub>LOC</sub>."</p>
                                <p class="mb-2">Prediction: "The [University]<sub>ORG</sub> of [Washington]<sub>LOC</sub> is in [Seattle]<sub>LOC</sub>."</p>
                                <p class="mb-2">Exact Match:</p>
                                <ul class="list-disc ml-5">
                                    <li>True Positives: 1 (Seattle as LOC)</li>
                                    <li>False Positives: 2 (University as ORG, Washington as LOC)</li>
                                    <li>False Negatives: 1 (University of Washington as ORG)</li>
                                    <li>Precision: 1/3 = 0.33, Recall: 1/2 = 0.5, F1: 0.4</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">Confusion Matrix Analysis</h4>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">Error Analysis Insights</h5>
                                <p class="mb-2">A confusion matrix for sequence labeling shows which tags are confused with which other tags.</p>
                                <p class="mb-3">Common error patterns in sequence labeling:</p>
                                <ul class="list-disc ml-5">
                                    <li><strong>Boundary Errors</strong>: Incorrect identification of where an entity starts or ends</li>
                                    <li><strong>Type Errors</strong>: Correct boundaries but wrong entity type</li>
                                    <li><strong>Miss Errors</strong>: Completely missing an entity</li>
                                    <li><strong>False Positive Errors</strong>: Identifying an entity where there isn't one</li>
                                </ul>
                                <p class="mt-2">Analyzing these patterns can guide model improvements, data augmentation, and feature engineering efforts.</p>
                            </div>
                        </div>
                    </div>
                </section>
                
                <!-- Sequence2Sequence and Transformers Section -->
                <section id="seq2seq" class="mb-12 section">
                    <h2 class="text-2xl font-bold mb-4 section-header">6. Sequence2Sequence Models & Transformers</h2>
                    
                    <div class="mb-6">
                        <p class="mb-4">Sequence-to-Sequence (Seq2Seq) models are a family of neural network models designed to transform one sequence into another. They're foundational for tasks like machine translation, text summarization, and question answering. Transformers represent a significant advancement in sequence-to-sequence modeling that has revolutionized NLP.</p>
                        
                        <div class="concept-card">
                            <h3 class="text-lg font-semibold mb-2">What are Sequence-to-Sequence Models?</h3>
                            <p>Sequence-to-Sequence (Seq2Seq) models take a sequence of items (words, letters, etc.) as input and produce another sequence of items as output. Unlike sequence labeling, the output sequence can have a different length than the input sequence.</p>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Core Seq2Seq Applications</h3>
                        
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Machine Translation</h4>
                                <p>Converting text from one language to another.</p>
                                <p class="mt-1 text-sm italic">Input: "Hello, how are you?"</p>
                                <p class="text-sm italic">Output: "Hola, ¿cómo estás?"</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Text Summarization</h4>
                                <p>Generating a concise summary of a longer text.</p>
                                <p class="mt-1 text-sm italic">Input: [Long article about climate change]</p>
                                <p class="text-sm italic">Output: "Global temperatures are rising at an alarming rate due to human activities."</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Question Answering</h4>
                                <p>Generating answers to questions based on context.</p>
                                <p class="mt-1 text-sm italic">Input: "Who invented the telephone?"</p>
                                <p class="text-sm italic">Output: "Alexander Graham Bell invented the telephone in 1876."</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">Dialogue Systems</h4>
                                <p>Generating contextually appropriate responses in a conversation.</p>
                                <p class="mt-1 text-sm italic">Input: "What's the weather going to be like tomorrow?"</p>
                                <p class="text-sm italic">Output: "Tomorrow will be sunny with a high of 75°F."</p>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Traditional Encoder-Decoder Architecture</h3>
                        <p class="mb-3">The classical Seq2Seq model consists of two main components: an encoder and a decoder, typically implemented using RNNs (like LSTMs or GRUs).</p>
                        
                        <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200 mb-4">
                            <h4 class="font-medium mb-2">Basic Encoder-Decoder Model</h4>
                            <ol class="list-decimal ml-5 space-y-2">
                                <li><strong>Encoder</strong>: Processes the input sequence and compresses it into a context vector (also called the thought vector)</li>
                                <li><strong>Context Vector</strong>: A fixed-size representation that aims to capture the meaning of the input sequence</li>
                                <li><strong>Decoder</strong>: Takes the context vector and generates the output sequence one element at a time</li>
                            </ol>
                        </div>
                        
                        <div class="example-box">
                            <h4 class="font-medium mb-2">Encoder-Decoder Process</h4>
                            <p class="mb-2">For the task of translating "Hello, how are you?" to Spanish:</p>
                            <ol class="list-decimal ml-5">
                                <li><strong>Encoding</strong>: The encoder RNN processes "Hello, how are you?" word by word</li>
                                <li><strong>Final Hidden State</strong>: After processing the entire input, the encoder's final hidden state becomes the context vector</li>
                                <li><strong>Decoding Start</strong>: The decoder begins with this context vector and a special "start of sequence" token</li>
                                <li><strong>Output Generation</strong>: The decoder generates "Hola", then uses this output and its hidden state to generate "¿cómo", and so on</li>
                                <li><strong>Termination</strong>: The process continues until the decoder generates an "end of sequence" token or reaches a maximum length</li>
                            </ol>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Attention Mechanism</h3>
                        <p class="mb-3">A key limitation of the basic encoder-decoder model is the bottleneck created by the fixed-size context vector, especially for long sequences. Attention mechanisms address this by allowing the decoder to focus on different parts of the input sequence at each decoding step.</p>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">How Attention Works</h4>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">Attention Mechanism Steps</h5>
                                <ol class="list-decimal ml-5 space-y-2">
                                    <li><strong>Encoder Hidden States</strong>: The encoder produces a sequence of hidden states, one for each input token</li>
                                    <li><strong>Alignment Scores</strong>: At each decoding step, compute scores between the current decoder hidden state and each encoder hidden state</li>
                                    <li><strong>Attention Weights</strong>: Convert scores to weights using softmax (weights sum to 1)</li>
                                    <li><strong>Context Vector</strong>: Compute a weighted sum of encoder hidden states using the attention weights</li>
                                    <li><strong>Combined Representation</strong>: Concatenate or otherwise combine this context vector with the current decoder hidden state</li>
                                    <li><strong>Output Prediction</strong>: Use this combined representation to predict the next output token</li>
                                </ol>
                            </div>
                        </div>
                        
                        <div class="example-box">
                            <h4 class="font-medium mb-2">Attention Example</h4>
                            <p class="mb-2">When translating "The cat sat on the mat" to French:</p>
                            <ul class="list-disc ml-5">
                                <li>When generating "Le" (The), attention might focus heavily on "The"</li>
                                <li>When generating "chat" (cat), attention would shift to focus on "cat"</li>
                                <li>This dynamic focusing allows the model to effectively handle long sequences and align corresponding parts of the input and output</li>
                            </ul>
                            <p class="mt-2">Visualizing these attention weights often reveals interesting patterns, like diagonal alignments for word-by-word translation or more complex patterns for structural transformations.</p>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">The Transformer Architecture</h3>
                        <p class="mb-3">The Transformer, introduced in the "Attention is All You Need" paper (2017), revolutionized sequence modeling by dispensing with recurrence and convolutions entirely, relying solely on attention mechanisms.</p>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Key Components of Transformers</h4>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">1. Self-Attention</h5>
                                <p class="mb-2">Unlike traditional attention which operates between encoder and decoder, self-attention allows a sequence to attend to itself, helping the model understand relationships between different positions in the sequence.</p>
                                <p class="mb-3">The self-attention mechanism computes:</p>
                                <ul class="list-disc ml-5">
                                    <li><strong>Query (Q)</strong>: What the current token is looking for</li>
                                    <li><strong>Key (K)</strong>: What other tokens offer</li>
                                    <li><strong>Value (V)</strong>: The information to retrieve if there's a match between query and key</li>
                                </ul>
                                <p class="mb-2">The attention scores are computed as:</p>
                                <p>Attention(Q, K, V) = softmax(QK^T / √d_k)V</p>
                                <p class="mt-2">Where d_k is the dimension of the keys (for scaling).</p>
                            </div>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200 mt-4">
                                <h5 class="font-medium mb-2">2. Multi-Head Attention</h5>
                                <p class="mb-2">Rather than performing a single attention function, multi-head attention runs multiple attention operations in parallel. This allows the model to jointly attend to information from different representation subspaces at different positions.</p>
                                <p class="mb-2">Multi-head attention:</p>
                                <ul class="list-disc ml-5">
                                    <li>Projects Q, K, V into h different spaces (creating h "heads")</li>
                                    <li>Applies attention independently in each space</li>
                                    <li>Concatenates the results and projects again</li>
                                </ul>
                                <p class="mt-2">This enables the model to capture different types of relationships between tokens.</p>
                            </div>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200 mt-4">
                                <h5 class="font-medium mb-2">3. Positional Encoding</h5>
                                <p class="mb-2">Unlike RNNs, Transformers process all tokens in parallel, losing the inherent order information. Positional encodings are added to the input embeddings to inject information about token positions in the sequence.</p>
                                <p class="mb-2">Typically uses sine and cosine functions of different frequencies:</p>
                                <ul class="list-disc ml-5">
                                    <li>PE(pos, 2i) = sin(pos/10000^(2i/d_model))</li>
                                    <li>PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))</li>
                                </ul>
                                <p class="mt-2">These encodings have the useful property that relative positions have similar representations for different sequence lengths.</p>
                            </div>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200 mt-4">
                                <h5 class="font-medium mb-2">4. Feed-Forward Networks</h5>
                                <p class="mb-2">Each encoder and decoder layer contains a fully connected feed-forward network applied to each position independently:</p>
                                <p>FFN(x) = max(0, xW₁ + b₁)W₂ + b₂</p>
                                <p class="mt-2">This is a simple two-layer neural network with a ReLU activation that processes each token's representation independently.</p>
                            </div>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200 mt-4">
                                <h5 class="font-medium mb-2">5. Layer Normalization and Residual Connections</h5>
                                <p class="mb-2">Each sub-layer (attention and feed-forward) is followed by:</p>
                                <ul class="list-disc ml-5">
                                    <li><strong>Layer Normalization</strong>: Normalizes the activations of the previous layer for each example to have zero mean and unit variance</li>
                                    <li><strong>Residual Connection</strong>: Adds the input to the sub-layer to its output, helping with gradient flow during training</li>
                                </ul>
                                <p class="mt-2">The combination is typically expressed as: LayerNorm(x + Sublayer(x))</p>
                            </div>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">The Complete Transformer Architecture</h4>
                            
                            <div class="example-box">
                                <h5 class="font-medium mb-2">Transformer Encoder-Decoder Structure</h5>
                                <ol class="list-decimal ml-5">
                                    <li><strong>Encoder</strong>:
                                        <ul class="list-disc ml-5">
                                            <li>N identical layers (typically 6 in the original paper)</li>
                                            <li>Each layer has two sub-layers: multi-head self-attention and feed-forward network</li>
                                            <li>Residual connections and layer normalization around each sub-layer</li>
                                        </ul>
                                    </li>
                                    <li><strong>Decoder</strong>:
                                        <ul class="list-disc ml-5">
                                            <li>N identical layers (typically 6)</li>
                                            <li>Each layer has three sub-layers: masked multi-head self-attention, multi-head attention over encoder output, and feed-forward network</li>
                                            <li>The masking in the first sub-layer prevents positions from attending to subsequent positions (preserves auto-regressive property)</li>
                                            <li>Residual connections and layer normalization around each sub-layer</li>
                                        </ul>
                                    </li>
                                    <li><strong>Final Linear and Softmax Layer</strong>: Converts decoder output to probabilities over the vocabulary</li>
                                </ol>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Transformer Training and Inference</h3>
                        
                        <div class="mb-4">
                            <h4 class="text-lg font-medium mb-2">Training Process</h4>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200">
                                <h5 class="font-medium mb-2">Transformer Training Details</h5>
                                <ul class="list-disc ml-5">
                                    <li><strong>Teacher Forcing</strong>: During training, the decoder receives the correct previous token regardless of what it predicted</li>
                                    <li><strong>Parallel Training</strong>: Unlike RNNs, transformers can process all positions of the input sequence in parallel</li>
                                    <li><strong>Loss Function</strong>: Typically cross-entropy loss on the predicted token probabilities</li>
                                    <li><strong>Label Smoothing</strong>: Often applied to prevent the model from becoming too confident</li>
                                    <li><strong>Regularization</strong>: Dropout is applied to the attention weights and to the outputs of each sub-layer</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div>
                            <h4 class="text-lg font-medium mb-2">Inference Methods</h4>
                            
                            <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-200 mb-4">
                                <h5 class="font-medium mb-2">Autoregressive Decoding</h5>
                                <p class="mb-2">At inference time, the transformer generates output tokens one by one:</p>
                                <ol class="list-decimal ml-5">
                                    <li>Start with a "start of sequence" token</li>
                                    <li>Generate probabilities for the next token</li>
                                    <li>Select a token based on these probabilities</li>
                                    <li>Add the selected token to the generated sequence</li>
                                    <li>Repeat until an "end of sequence" token is generated or the maximum length is reached</li>
                                </ol>
                            </div>
                            
                            <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Greedy Decoding</h5>
                                    <p>Always select the token with the highest probability.</p>
                                    <p class="mt-1 text-sm">Simple but can lead to suboptimal outputs.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Beam Search</h5>
                                    <p>Maintain multiple candidate sequences and expand each by all possible next tokens, keeping the top-k candidates.</p>
                                    <p class="mt-1 text-sm">More computationally expensive but often produces better results.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Sampling</h5>
                                    <p>Sample the next token from the predicted probability distribution.</p>
                                    <p class="mt-1 text-sm">Can introduce diversity in generation.</p>
                                </div>
                                
                                <div class="concept-card">
                                    <h5 class="font-medium mb-1">Top-p (Nucleus) Sampling</h5>
                                    <p>Sample from the smallest set of tokens whose cumulative probability exceeds a threshold p.</p>
                                    <p class="mt-1 text-sm">Balances quality and diversity.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="mb-6">
                        <h3 class="text-xl font-semibold mb-3">Transformer Variants and Advancements</h3>
                        
                        <div class="grid grid-cols-1 gap-4">
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">BERT (Bidirectional Encoder Representations from Transformers)</h4>
                                <p>Uses only the encoder part of the transformer and is pre-trained on masked language modeling and next sentence prediction tasks.</p>
                                <p class="mt-1 text-sm">Excels at understanding tasks like classification, named entity recognition, and question answering.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">GPT (Generative Pre-trained Transformer)</h4>
                                <p>Uses only the decoder part of the transformer and is pre-trained on next-token prediction.</p>
                                <p class="mt-1 text-sm">Excels at generative tasks like text completion, summarization, and translation.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">T5 (Text-to-Text Transfer Transformer)</h4>
                                <p>Treats all NLP tasks as text-to-text problems, using the full encoder-decoder architecture.</p>
                                <p class="mt-1 text-sm">Pre-trained on a mixture of unsupervised and supervised tasks with a unified text-to-text format.</p>
                            </div>
                            
                            <div class="concept-card">
                                <h4 class="font-medium mb-1">XLNet</h4>
                                <p>Combines the benefits of autoregressive models (like GPT) and bidirectional models (like BERT) using permutation language
