<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Comprehensive Guide to Bayesian Statistics</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.0.0/css/all.min.css">
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            color: #333;
            line-height: 1.6;
        }
        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .card {
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }
        .card:hover {
            transform: translateY(-5px);
        }
        .example-box {
            background-color: #f0f9ff;
            border-left: 4px solid #3b82f6;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        .formula {
            background-color: #f8f9fa;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
            margin: 15px 0;
            text-align: center;
            font-family: 'Cambria Math', Georgia, serif;
        }
        .concept-diagram {
            margin: 20px auto;
            max-width: 100%;
            display: block;
        }
        .section-divider {
            height: 2px;
            background: linear-gradient(to right, #3b82f6, #93c5fd, #3b82f6);
            margin: 40px 0;
            border-radius: 2px;
        }
        .note-box {
            background-color: #fff7ed;
            border-left: 4px solid #f97316;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        .tip-box {
            background-color: #ecfdf5;
            border-left: 4px solid #10b981;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        .comparison-table th {
            background-color: #e0e7ff;
            padding: 10px;
            text-align: left;
        }
        .comparison-table td {
            padding: 10px;
            border-bottom: 1px solid #e5e7eb;
        }
        .comparison-table tr:nth-child(even) {
            background-color: #f9fafb;
        }
        .toc-link {
            display: block;
            padding: 5px 0;
            color: #4b5563;
            text-decoration: none;
            transition: all 0.2s ease;
        }
        .toc-link:hover {
            color: #3b82f6;
            transform: translateX(5px);
        }
        .progress-nav {
            position: sticky;
            top: 20px;
            z-index: 10;
        }
        .scrollable-content {
            max-height: none;
            overflow-y: visible;
        }
        @media print {
            .no-print {
                display: none;
            }
            .page-break {
                page-break-after: always;
            }
        }
        /* Stick Breaking Visualization Styles */
        .stick-breaking-viz {
            width: 80%;
            margin: 20px auto;
            border: 1px solid #ccc;
            padding: 10px;
            background-color: #f9f9f9;
        }
        .stick {
            height: 30px;
            background-color: #e0e0e0; /* Represents the whole stick */
            display: flex;
            overflow: hidden;
            border-radius: 3px;
        }
        .stick-segment {
            height: 100%;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 0.8em;
            text-align: center;
            border-right: 1px solid #fff; /* Separator */
        }
        .stick-segment:last-child {
            border-right: none;
        }
        /* Example widths - illustrative */
        .w1 { width: 35%; background-color: #3b82f6; }
        .w2 { width: 25%; background-color: #10b981; }
        .w3 { width: 15%; background-color: #f97316; }
        .w4 { width: 10%; background-color: #ef4444; }
        .w-rem { width: 15%; background-color: #6b7280; font-size: 0.7em; } /* Remainder */

        /* DPM Visualization Styles */
        .dpm-viz {
            border: 1px solid #ccc;
            padding: 15px;
            margin: 20px auto;
            background-color: #f9f9f9;
            font-size: 0.9em;
            text-align: center;
        }
        .dpm-level {
            border: 1px dashed #9ca3af;
            padding: 10px;
            margin-bottom: 15px;
            border-radius: 4px;
        }
        .dpm-level h5 {
            font-weight: bold;
            color: #4b5563;
            margin-bottom: 8px;
            font-size: 1em;
        }
        .dpm-atoms, .dpm-params, .dpm-data {
            display: flex;
            justify-content: space-around;
            flex-wrap: wrap;
            margin-top: 10px;
        }
        .dpm-atom, .dpm-param, .dpm-point {
            border: 1px solid #6b7280;
            padding: 5px 8px;
            margin: 5px;
            border-radius: 3px;
            background-color: #fff;
            min-width: 40px;
        }
        .dpm-atom { background-color: #e0e7ff; } /* Atoms of G */
        .dpm-param { background-color: #ecfdf5; } /* Theta_i */
        .dpm-point { background-color: #fff7ed; } /* Y_i */
        .dpm-arrow {
            margin: 0 5px;
            color: #6b7280;
        }
    </style>
</head>
<body class="bg-gray-50">
    <div class="container pt-8 pb-16">
        <header class="text-center mb-12">
            <h1 class="text-4xl font-bold text-blue-700 mb-4">Understanding Bayesian Statistics</h1>
            <p class="text-xl text-gray-600">A Visual and Conceptual Guide</p>
        </header>

        <div class="grid grid-cols-1 md:grid-cols-4 gap-8">
            <!-- Table of Contents -->
            <div class="md:col-span-1">
                <div class="bg-white rounded-lg shadow-md p-6 progress-nav">
                    <h2 class="text-xl font-bold text-blue-700 mb-4">Contents</h2>
                    <nav class="toc">
                        <a href="#introduction" class="toc-link">Introduction to Bayesian Statistics</a>
                        <a href="#basics" class="toc-link">1. Basics of Bayesian Inference</a>
                        <a href="#simulation" class="toc-link">2. Simulation Methods</a>
                        <a href="#linear-models" class="toc-link">3. Bayesian Linear Models</a>
                        <a href="#hierarchical" class="toc-link">4. Hierarchical Models</a>
                        <a href="#model-assessment" class="toc-link">5. Model Assessment</a>
                        <a href="#survival" class="toc-link">6. Survival Analysis</a>
                        <a href="#spatial" class="toc-link">7. Spatial Models</a>
                        <a href="#nonparametrics" class="toc-link">8. Bayesian Nonparametrics</a>
                    </nav>
                </div>
            </div>

            <!-- Main Content -->
            <div class="md:col-span-3 scrollable-content">
                <!-- Introduction Section -->
                <section id="introduction" class="bg-white rounded-lg shadow-md p-8 mb-8">
                    <h2 class="text-2xl font-bold text-blue-700 mb-4">Introduction to Bayesian Statistics</h2>
                    
                    <p class="mb-4">Welcome to your guide on Bayesian statistics! This visual and conceptual guide will help you understand the fundamentals of Bayesian thinking and its applications in data analysis and decision making.</p>
                    
                    <div class="tip-box">
                        <h3 class="font-bold">Why Bayesian Statistics Matters</h3>
                        <p>Bayesian statistics provides a framework for updating our beliefs based on new evidence. Unlike traditional (frequentist) statistics, Bayesian methods allow us to:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Incorporate prior knowledge into our analyses</li>
                            <li>Express uncertainty in a more intuitive way</li>
                            <li>Make probability statements about hypotheses</li>
                            <li>Continuously update our understanding as new data arrives</li>
                        </ul>
                    </div>

                    <div class="mt-6">
                        <h3 class="text-xl font-semibold text-blue-600 mb-3">The Bayesian Perspective: A Simple Example</h3>
                        
                        <div class="example-box">
                            <h4 class="font-bold">Disease Testing Scenario</h4>
                            <p>Imagine a medical test for a rare disease that affects 1% of the population. The test is 95% accurate, meaning:</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li>If you have the disease, the test has a 95% chance of being positive</li>
                                <li>If you don't have the disease, the test has a 95% chance of being negative</li>
                            </ul>
                            <p class="mt-2">If you test positive, what's the probability you actually have the disease?</p>
                            
                            <div class="flex justify-center mt-4">
                                <div class="bg-white p-4 rounded-lg border border-blue-200">
                                    <p class="font-semibold text-center">Using Bayes' Theorem:</p>
                                    <div class="formula">
                                        P(Disease|Positive) = P(Positive|Disease) × P(Disease) / P(Positive)
                                    </div>
                                    <p class="mt-2">
                                        P(Disease|Positive) = 0.95 × 0.01 / [0.95 × 0.01 + 0.05 × 0.99] ≈ 0.16 or 16%
                                    </p>
                                </div>
                            </div>
                            
                            <p class="mt-4">Surprisingly, even with a positive test result, there's only a 16% chance you have the disease. This counterintuitive result is what makes Bayesian thinking so powerful!</p>
                        </div>
                    </div>
                </section>

                <!-- Basics of Bayesian Inference -->
                <section id="basics" class="bg-white rounded-lg shadow-md p-8 mb-8">
                    <h2 class="text-2xl font-bold text-blue-700 mb-4">1. Basics of Bayesian Inference</h2>
                    
                    <p class="mb-4">Bayesian statistics treats parameters (θ) as random variables, allowing us to express our beliefs and update them as we observe data (y). It's a formal way of learning from experience.</p>

                    <h3 class="text-xl font-semibold text-blue-600 mb-3">1.1 Bayesian Learning: Likelihood, Prior and Posterior</h3>
                    
                    <p class="mb-4">The core of Bayesian learning involves combining our initial beliefs with evidence from data:</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-6">
                        <div class="card bg-blue-50 p-4">
                            <h4 class="font-bold text-blue-700">Prior Density π(θ)</h4>
                            <p>Your initial belief about a parameter θ before seeing data y</p>
                            <div class="text-center text-3xl mt-2">
                                <i class="fas fa-brain text-blue-500"></i>
                            </div>
                        </div>
                        <div class="card bg-blue-50 p-4">
                            <h4 class="font-bold text-blue-700">Likelihood Function f(y|θ)</h4>
                            <p>How probable your data y is given different parameter values θ</p>
                            <div class="text-center text-3xl mt-2">
                                <i class="fas fa-chart-line text-blue-500"></i>
                            </div>
                        </div>
                        <div class="card bg-blue-50 p-4">
                            <h4 class="font-bold text-blue-700">Posterior Density π(θ|y)</h4>
                            <p>Your updated belief about the parameter θ after seeing data y</p>
                            <div class="text-center text-3xl mt-2">
                                <i class="fas fa-lightbulb text-blue-500"></i>
                            </div>
                        </div>
                    </div>

                    <div class="concept-diagram text-center">
                        <svg width="600" height="200" viewBox="0 0 600 200" xmlns="http://www.w3.org/2000/svg">
                            <!-- Prior Distribution -->
                            <g transform="translate(50, 100)">
                                <path d="M0,80 C20,80 40,20 80,20 C120,20 140,80 160,80" stroke="#3b82f6" fill="none" stroke-width="3"/>
                                <text x="80" y="100" text-anchor="middle" font-size="14">Prior π(θ)</text>
                            </g>
                            
                            <!-- Multiplication Sign -->
                            <text x="235" y="100" text-anchor="middle" font-size="24">×</text>
                            
                            <!-- Likelihood -->
                            <g transform="translate(260, 100)">
                                <path d="M0,80 C30,-20 80,-20 160,80" stroke="#10b981" fill="none" stroke-width="3"/>
                                <text x="80" y="100" text-anchor="middle" font-size="14">Likelihood f(y|θ)</text>
                            </g>
                            
                            <!-- Equals Sign -->
                            <text x="445" y="100" text-anchor="middle" font-size="24">∝</text>
                            
                            <!-- Posterior -->
                            <g transform="translate(470, 100)">
                                <path d="M0,50 C20,50 40,0 60,0 C80,0 100,20 120,80" stroke="#f97316" fill="none" stroke-width="3"/>
                                <text x="60" y="100" text-anchor="middle" font-size="14">Posterior π(θ|y)</text>
                            </g>
                        </svg>
                    </div>

                    <div class="example-box mt-6">
                        <h4 class="font-bold">Example: Email Spam Filter</h4>
                        <p>Imagine you're building a spam filter:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>Prior:</strong> Initially, you believe 30% of all emails are spam (P(spam) = 0.3)</li>
                            <li><strong>Likelihood:</strong> You observe that 80% of spam emails contain the word "free," while only 10% of legitimate emails do</li>
                            <li><strong>Data:</strong> You receive a new email containing the word "free"</li>
                        </ul>
                        
                        <div class="bg-white p-4 rounded-lg border border-blue-200 mt-4">
                            <p class="font-semibold">Using Bayes' Theorem to calculate the posterior probability:</p>
                            <div class="formula">
                                P(spam|"free") = P("free"|spam) × P(spam) / P("free")
                            </div>
                            <p>P(spam|"free") = 0.8 × 0.3 / [0.8 × 0.3 + 0.1 × 0.7] = 0.24 / 0.31 ≈ 0.77 or 77%</p>
                        </div>
                        
                        <p class="mt-4">After seeing the word "free," your belief that the email is spam increased from 30% to 77%.</p>
                    </div>

                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">1.2 Bayes' Theorem</h3>
                    
                    <p class="mb-4">Bayes' theorem provides the mathematical rule for updating beliefs:</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">1.2.1 Bayes' Theorem for Events</h4>
                            <div class="formula">
                                P(Hⱼ|E) = P(E|Hⱼ) × P(Hⱼ) / P(E)
                            </div>
                            <p>Where H₁, ..., Hₖ is a partition of the sample space, E is an event, and P(E) > 0. P(E) = Σ P(E|Hᵢ)P(Hᵢ).</p>
                        </div>
                        <div>
                            <h4 class="font-bold">1.2.2 Bayes' Theorem for Random Variables (Densities)</h4>
                            <div class="formula">
                                π(θ|y) = f(y|θ) × π(θ) / m(y)
                            </div>
                            <p>Where θ is a parameter vector, y is observed data, π(θ) is the prior density, f(y|θ) is the likelihood, and m(y) is the marginal likelihood of the data.</p>
                        </div>
                    </div>
                    
                    <div>
                        <h4 class="font-bold">Marginal Likelihood (Evidence)</h4>
                        <p class="mb-2">The denominator m(y) ensures the posterior integrates to 1. It's calculated by integrating the joint distribution over all possible parameter values:</p>
                        <div class="formula">
                             m(y) = ∫ f(y|θ) π(θ) dθ
                        </div>
                        <p>Often, we work with proportionality, as m(y) is constant with respect to θ:</p>
                        <div class="formula">
                             π(θ|y) ∝ f(y|θ) × π(θ)
                        </div>
                    </div>

                    <div class="note-box mt-4">
                        <h4 class="font-bold">Sequential Updating</h4>
                        <p>Bayesian inference naturally handles sequential data. The posterior π(θ|y₁) from the first dataset y₁ becomes the prior for analyzing the second dataset y₂ (assuming conditional independence). The final posterior π(θ|y₁,y₂) is the same as if both datasets were analyzed together.</p>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">1.3 Main Inferential Problems</h3>
                    
                    <p>The posterior distribution π(θ|y) contains all information about θ after observing y. We summarize it for inference:</p>

                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">1.3.1 Point and Interval Estimation</h4>
                            <p class="mb-2">Summarizing the location and spread of the posterior:</p>
                            <ul class="list-disc ml-6">
                                <li><strong>Point estimates:</strong> Common choices include the posterior mean E[θ|y], posterior median, or posterior mode (MAP - maximum a posteriori).</li>
                                <li><strong>Interval estimates (Credible Intervals):</strong> An interval C such that P(θ ∈ C | y) ≥ 1-α. Examples:
                                    <ul class="list-circle ml-4">
                                        <li>Equal-tailed interval (using quantiles).</li>
                                        <li>Highest Posterior Density Interval (HPDI): The narrowest interval containing the desired probability mass.</li>
                                    </ul>
                                </li>
                            </ul>
                            
                            <div class="example-box mt-4">
                                <p class="font-semibold">Example: Estimating Batting Average</p>
                                <p>A player gets y=7 hits in n=20 at-bats (Binomial likelihood). With a Beta(1,1) prior for the average θ, the posterior is Beta(1+y, 1+n-y) = Beta(8,14):</p>
                                <ul class="list-disc ml-6 mt-2">
                                    <li>Posterior mean: 8/(8+14) = 0.364</li>
                                    <li>95% credible interval: [0.193, 0.544]</li>
                                </ul>
                            </div>
                        </div>
                        <div>
                            <h4 class="font-bold">1.3.2 Hypothesis Testing</h4>
                            <p class="mb-2">Comparing competing hypotheses (e.g., H₀: θ ∈ Θ₀ vs H₁: θ ∈ Θ₁) using posterior probabilities or odds:</p>
                            <div class="formula">
                                Posterior Odds = (P(H₀|y) / P(H₁|y)) = BF₀₁ × (π₀ / π₁)
                            </div>
                            <p>Where π₀/π₁ is the Prior Odds and BF₀₁ is the Bayes Factor.</p>
                            <div class="formula">
                                BF₀₁ = m(y|H₀) / m(y|H₁) = [∫<sub>Θ₀</sub> f(y|θ)g₀(θ)dθ] / [∫<sub>Θ₁</sub> f(y|θ)g₁(θ)dθ]
                            </div>
                            <p>The Bayes Factor measures the evidence provided by the data y in favor of H₀ over H₁. g₀ and g₁ are prior densities under H₀ and H₁.</p>

                            <div class="tip-box mt-4">
                                <h4 class="font-bold">Interpreting Bayes Factors (Jeffreys' Scale)</h4>
                                <ul class="list-disc ml-6 mt-2 text-sm">
                                    <li>1 &lt; BF₀₁ &lt; 3: Anecdotal evidence for H₀</li>
                                    <li>3 &lt; BF₀₁ &lt; 10: Moderate evidence for H₀</li>
                                    <li>10 &lt; BF₀₁ &lt; 30: Strong evidence for H₀</li>
                                    <li>30 &lt; BF₀₁ &lt; 100: Very Strong evidence for H₀</li>
                                    <li>BF₀₁ &gt; 100: Decisive evidence for H₀</li>
                                </ul>
                                <p class="text-xs">(Note: BF₁₀ = 1/BF₀₁)</p>
                            </div>

                            <div class="example-box mt-4">
                                <p class="font-semibold">Example: Testing a New Drug</p>
                                <p>H₀: "drug not effective", H₁: "drug effective". Prior odds = 1. Data yields BF₁₀ = 20 (evidence favors H₁).</p>
                                <p>Posterior Odds (H₁ vs H₀) = BF₁₀ × Prior Odds = 20 × 1 = 20.</p>
                                <p>P(H₁|y) = Post.Odds / (1 + Post.Odds) = 20/21 ≈ 0.95.</p>
                            </div>
                        </div>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">1.4 Posterior Predictive Distributions</h3>
                    
                    <p class="mb-4">Predicting future observations (y<sub>new</sub>) given the observed data (y). This involves averaging the predictive distribution f(y<sub>new</sub>|θ) over the posterior distribution π(θ|y):</p>
                    
                    <div class="formula">
                         π(y<sub>new</sub>|y) = ∫ f(y<sub>new</sub>|θ) π(θ|y) dθ
                    </div>
                    
                    <p>The posterior predictive distribution accounts for both the uncertainty about the parameters θ (via π(θ|y)) and the inherent randomness of the process (via f(y<sub>new</sub>|θ)).</p>
                    
                    <div class="example-box mt-4">
                        <h4 class="font-bold">Example: Predicting Future Sales</h4>
                        <p>Daily sales yᵢ ~ N(θ, σ²), with known σ²=100. After observing data y, the posterior for the mean is π(θ|y) ~ N(80, 5²).</p>
                        <p class="mt-2">The posterior predictive distribution for tomorrow's sales y<sub>new</sub> is:</p>
                        <div class="formula">
                             π(y<sub>new</sub>|y) ~ N(E[θ|y], Var[θ|y] + σ²) = N(80, 5² + 10²) = N(80, 125)
                        </div>
                        <p>This means our prediction incorporates both our uncertainty about the mean and the inherent variability in daily sales.</p>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">1.5 Exchangeability</h3>
                    
                    <p class="mb-4">Exchangeability is a key concept in Bayesian statistics. A sequence of random variables is exchangeable if their joint distribution is invariant to permutation.</p>
                    
                    <div class="note-box">
                        <h4 class="font-bold">Exchangeability vs. Independence</h4>
                        <p>Independent and identically distributed (i.i.d.) random variables are exchangeable, but exchangeable variables need not be independent.</p>
                        <p class="mt-2">For example, in a sequence of coin flips:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li>If the coin has a fixed probability p of heads, the flips are i.i.d.</li>
                            <li>If p is unknown and has a prior distribution, the flips are exchangeable but not independent (they're linked through the unknown p)</li>
                        </ul>
                    </div>
                    
                    <p class="mt-4">By de Finetti's theorem, an infinite sequence of exchangeable (binary) random variables behaves as if the observations were conditionally independent and identically distributed given some underlying parameter θ, which itself has a prior distribution. This provides a philosophical justification for the Bayesian approach.</p>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">1.6 Specifying Prior Distributions</h3>
                    
                    <p class="mb-4">Choosing appropriate prior distributions is a crucial step in Bayesian analysis:</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">1.6.1 Reference Priors (Convenience Priors)</h4>
                            <p>Used when minimal prior information is available or desired, aiming to let the data dominate the posterior. They are NOT truly "non-informative".</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li>Uniform priors</li>
                                <li>Maximum entropy priors</li>
                                <li>Priors that maximize the expected information from the data</li>
                            </ul>
                            
                            <div class="example-box mt-4">
                                <p class="font-semibold">Example: Estimating a Proportion</p>
                                <p>For estimating a proportion θ, a uniform prior Beta(1,1) is a common reference prior, representing equal probability for all possible values of θ between 0 and 1.</p>
                            </div>
                            <div class="note-box mt-2">
                                <p class="text-sm">Caution: Improper priors (that don't integrate to 1, like π(θ)=c on R) can lead to improper posteriors if the likelihood isn't strong enough to make it integrable.</p>
                            </div>
                        </div>
                        <div>
                            <h4 class="font-bold">1.6.2 Jeffreys Priors</h4>
                            <p>A type of reference prior invariant to parameter transformation. Proportional to the square root of the Fisher information:</p>
                            <div class="formula">
                                π(θ) ∝ √|I(θ)|
                            </div>
                            <p>Where I(θ) = E<sub>y|θ</sub>[ -∂² log f(y|θ) / ∂θ² ] is the Fisher information.</p>
                            
                            <div class="example-box mt-4">
                                <p class="font-semibold">Example: Normal Distribution Priors</p>
                                <p>• Known σ²: Jeffreys prior for mean μ is π(μ) ∝ 1 (Uniform).</p>
                                <p>• Known μ: Jeffreys prior for variance σ² is π(σ²) ∝ 1/σ² (or π(σ) ∝ 1/σ).</p>
                            </div>
                        </div>
                    </div>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">1.6.3 Scientifically Informed Priors</h4>
                            <p>Incorporate external knowledge (e.g., from previous studies, physical constraints, expert opinion) into the model. This is generally preferred when reliable information exists.</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li>Choose a distributional family (e.g., Beta for proportion, Normal for location)</li>
                                <li>Set hyperparameters (e.g., mean, variance) to reflect the external knowledge</li>
                                <li>Always check if the chosen prior makes sense for the scientifically relevant parameters</li>
                            </ul>
                            <div class="example-box mt-4">
                                <p class="font-semibold">Example: Drug Efficacy</p>
                                <p>Based on similar drugs, scientists believe a new treatment has between 30% and 60% efficacy, with 50% being most likely. This could be modeled as a Beta(5,5) prior centered at 0.5 with most mass between 0.3 and 0.7.</p>
                            </div>
                        </div>
                        <div>
                            <h4 class="font-bold">1.6.4 Merging of Priors & Posterior Consistency</h4>
                            <p>As more data accumulates, the influence of the prior diminishes, and the likelihood dominates the posterior. Different reasonable priors will lead to increasingly similar posterior distributions.</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li><strong>Posterior Consistency:</strong> Under suitable conditions, as n → ∞, the posterior distribution concentrates around the true parameter value θ₀.</li>
                                <li>This means that eventually, enough data can overcome initial differences in prior beliefs.</li>
                                <!-- Removed pooling example as it's less central than consistency -->
                            </ul>
                        </div>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">1.7 Asymptotic Normality of the Posterior Distribution</h3>
                    
                    <p>The posterior distribution π(θ|y) often resembles a Normal distribution (Bernstein-von Mises Theorem):</p>
                    
                    <div class="formula">
                         π(θ|y) ≈ N(θ̂, [nI(θ̂)]⁻¹)
                    </div>
                    
                    <p>Where θ̂ is the Maximum Likelihood Estimator (MLE) or posterior mode, and I(θ̂) is the Fisher information evaluated at θ̂.</p>
                    
                    <div class="note-box">
                        <h4 class="font-bold">Important Implications</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li>With large enough samples, the posterior becomes less sensitive to the choice of prior</li>
                            <li>Bayesian credible intervals become approximately equal to frequentist confidence intervals</li>
                            <li>Computation can be simplified by using normal approximations</li>
                        </ul>
                    </div>
                    
                    <div class="example-box mt-4">
                        <h4 class="font-bold">Example: Coin Flipping</h4>
                        <p>With a Beta(1,1) prior on the probability of heads, after observing 100 flips with 60 heads:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li>The posterior is Beta(61,41)</li>
                            <li>This is approximately normal with mean 0.6 and variance 0.0024</li>
                        </ul>
                        <p class="mt-2">As n increases further, the normal approximation becomes even more accurate.</p>
                    </div>
                </section>

                <div class="section-divider"></div>

                <!-- Simulation Methods -->
                <section id="simulation" class="bg-white rounded-lg shadow-md p-8 mb-8">
                    <h2 class="text-2xl font-bold text-blue-700 mb-4">2. Simulation Methods for Bayesian Statistics</h2>
                    
                    <p class="mb-4">Many Bayesian models result in posterior distributions that cannot be expressed in closed form. Simulation methods allow us to draw samples from these complex distributions and make inference based on these samples.</p>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mb-3">2.1 The Monte Carlo Method</h3>
                    
                    <p class="mb-4">The Monte Carlo method is used to learn relevant features of a target distribution π through simulation. In the Bayesian framework, π is typically the posterior distribution. The method assumes we can draw independent and identically distributed samples from π: θ<sup>(1)</sup>, θ<sup>(2)</sup>, ... ~ π.</p>
                    
                    <div class="formula">
                        <p>For a parameter θ ∈ Θ ⊂ ℝ<sup>d</sup> with target distribution π, we simulate draws θ<sup>(1)</sup>, θ<sup>(2)</sup>, ..., θ<sup>(T)</sup> from the posterior π(θ|y)</p>
                    </div>
                    
                    <div class="note-box">
                        <h4 class="font-bold">Theorem 2.1: Strong Law of Large Numbers (SLLN)</h4>
                        <p>Let π be a distribution over Θ and h: Θ→ℝ be a function such that ∫<sub>Θ</sub> |h(θ)|π(dθ) < +∞. Moreover, let θ<sup>(1)</sup>, θ<sup>(2)</sup>, ... be a sequence of random variables independent and identically distributed from π. Then:</p>
                        <ol class="list-decimal ml-6 mt-2">
                            <li>The sequence of random variables h̄<sup>(T)</sup> := (1/T)∑<sub>i=1</sub><sup>T</sup> h(θ<sup>(i)</sup>) converges almost surely to ∫<sub>Θ</sub> h(θ)π(dθ) = h̄ as T→+∞.</li>
                            <li>For any p ∈ (0,1), if q<sub>p</sub> is the p-quantile of h(θ) and q<sub>p</sub><sup>(T)</sup> is the empirical p-quantile of {h(θ<sup>(1)</sup>), ..., h(θ<sup>(T)</sup>)}, then q<sub>p</sub><sup>(T)</sup> converges almost surely to q<sub>p</sub> as T→+∞.</li>
                        </ol>
                    </div>
                    
                    <p class="mb-4">The SLLN is used to implement the Monte Carlo method by computing h̄<sup>(T)</sup> for a smart choice of the function h:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>To approximate π(A), use h(θ) = I<sub>A</sub>(θ) (indicator function)</li>
                        <li>To approximate E<sub>π</sub>[θ<sub>i</sub>], use h(θ) = θ<sub>i</sub> (the i-th component of θ)</li>
                    </ul>
                    
                    <div class="note-box">
                        <h4 class="font-bold">Theorem 2.2: Central Limit Theorem (CLT)</h4>
                        <p>Let π be a distribution over Θ and h: Θ→ℝ be a function such that ∫<sub>Θ</sub> |h(θ)|π(dθ) < +∞ and 0 < Var(h(θ)) =: σ² < +∞. Moreover, let θ<sup>(1)</sup>, θ<sup>(2)</sup>, ... be a sequence of random variables independent and identically distributed from π. Then:</p>
                        <ol class="list-decimal ml-6 mt-2">
                            <li>√T(h̄<sup>(T)</sup> - h̄) converges in distribution to N(0, σ²) as T→+∞.</li>
                            <li>σ²<sup>(T)</sup> = (1/T)∑<sub>i=1</sub><sup>T</sup>(h(θ<sup>(i)</sup>) - h̄<sup>(T)</sup>)² converges almost surely to σ² as T→+∞.</li>
                        </ol>
                        <p class="mt-2">In particular, the CLT tells us that if T is large, then err<sup>(T)</sup> = h̄<sup>(T)</sup> - h̄ ≈ N(0, σ²/T). For any c > 0, the probability that |err<sup>(T)</sup>| > c becomes small when T is large.</p>
                    </div>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example 2.1: Educational Attainment and Children</h4>
                        <p>The General Social Survey collected data on educational attainment and number of children of 155 women in the 1990s. We compare women with college degrees (n₂ = 44) to those without (n₁ = 111).</p>
                        <div class="formula">
                            <p>Y<sub>11</sub>,...,Y<sub>n₁1</sub> | θ₁ ~ Poisson(θ₁)</p>
                            <p>Y<sub>12</sub>,...,Y<sub>n₂2</sub> | θ₂ ~ Poisson(θ₂)</p>
                            <p>θ<sub>j</sub> ~ Gamma(a, b), j = 1, 2  with a = 2, b = 1</p>
                        </div>
                        <p>The posterior distribution is:</p>
                        <div class="formula">
                            <p>(θ₁, θ₂) | y₁, y₂ ~ Gamma(219, 112) × Gamma(68, 45)</p>
                        </div>
                        <p>We want to compute P(θ₁ > θ₂ | y₁, y₂) using Monte Carlo with h(θ₁, θ₂) = I(θ₁ > θ₂).</p>
                        <p>With T = 1000 samples, we get h̄<sup>(T)</sup> ≈ 0.97, which matches the true value computed by numerical integration.</p>
                    </div>
                    
                    <h4 class="font-bold mt-6">2.1.1 Monte Carlo Method for the Posterior Predictive Distribution: The Augmentation Trick</h4>
                    
                    <p class="mb-4">In the Bayesian framework, the posterior predictive distribution is an important object. For conditionally i.i.d. data, its density can be written as:</p>
                    
                    <div class="formula">
                        <p>m<sub>Y<sub>n+1</sub>|Y<sub>1</sub>,...,Y<sub>n</sub></sub>(y|y<sub>1</sub>,...,y<sub>n</sub>) = ∫<sub>Θ</sub> f(y|θ) π(dθ|y<sub>1</sub>,...,y<sub>n</sub>)</p>
                    </div>
                    
                    <p class="mb-4">If we only need to evaluate the posterior predictive density via Monte Carlo method, we can approximate:</p>
                    
                    <div class="formula">
                        <p>m<sub>Y<sub>n+1</sub>|Y<sub>1</sub>,...,Y<sub>n</sub></sub>(y|y<sub>1</sub>,...,y<sub>n</sub>) ≈ (1/T) ∑<sub>i=1</sub><sup>T</sup> f(y|θ<sup>(i)</sup>)</p>
                    </div>

                    <p class="mb-4">where θ<sup>(1)</sup>, θ<sup>(2)</sup>, ..., θ<sup>(T)</sup> are i.i.d. draws from π(θ|y<sub>1</sub>,...,y<sub>n</sub>).</p>
                    
                    <div class="note-box">
                        <h4 class="font-bold">The Augmentation Trick</h4>
                        <p>To obtain i.i.d. samples from the posterior predictive distribution, we need to apply the augmentation trick.</p>
                        <p class="mt-2">In general, suppose we cannot simulate directly from the distribution L(X<sub>1</sub>), but there exists another random variable X<sub>2</sub> such that we can simulate from L(X<sub>1</sub>|X<sub>2</sub>) and L(X<sub>2</sub>). Then:</p>
                        <ol class="list-decimal ml-6 mt-2">
                            <li>For t = 1,...,T:
                                <ul class="list-disc ml-6">
                                    <li>Draw a sample x<sub>2</sub><sup>(t)</sup> ~ L(X<sub>2</sub>)</li>
                                    <li>Draw a sample x<sub>1</sub><sup>(t)</sup> ~ L(X<sub>1</sub>|X<sub>2</sub>=x<sub>2</sub><sup>(t)</sup>)</li>
                                </ul>
                            </li>
                        </ol>
                        <p class="mt-2">The resulting samples x<sub>1</sub><sup>(1)</sup>, x<sub>1</sub><sup>(2)</sup>, ..., x<sub>1</sub><sup>(T)</sup> are i.i.d. from L(X<sub>1</sub>).</p>
                    </div>
                    
                    <p class="mb-4">For the posterior predictive distribution, we can set:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>L(X<sub>1</sub>) = L(Y<sub>n+1</sub>|Y<sub>1</sub>,...,Y<sub>n</sub>) (posterior predictive)</li>
                        <li>L(X<sub>1</sub>|X<sub>2</sub>) = L(Y<sub>n+1</sub>|θ) (sampling model)</li>
                        <li>L(X<sub>2</sub>) = L(θ|Y<sub>1</sub>,...,Y<sub>n</sub>) (posterior)</li>
                    </ul>
                    
                    <p class="mb-4">The algorithm becomes:</p>
                    <ol class="list-decimal ml-6 mb-4">
                        <li>Draw samples θ<sup>(1)</sup>, θ<sup>(2)</sup>, ..., θ<sup>(T)</sup> from the posterior π(θ|y<sub>1</sub>,...,y<sub>n</sub>)</li>
                        <li>For each θ<sup>(i)</sup>, draw a future observation y<sub>n+1</sub><sup>(i)</sup> from f(·|θ<sup>(i)</sup>)</li>
                        <li>The set {y<sub>n+1</sub><sup>(1)</sup>, y<sub>n+1</sub><sup>(2)</sup>, ..., y<sub>n+1</sub><sup>(T)</sup>} provides a sample from the posterior predictive distribution</li>
                    </ol>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">2.2 Rejection Sampling</h3>
                    
                    <p class="mb-4">Rejection Sampling is a method to draw samples from a target density π(θ) that we cannot directly sample from, by using another density that is easier to sample from.</p>
                    
                    <div class="formula">
                        <p>Let π(θ) be the target density and assume:</p>
                        <ul class="list-disc ml-6 mb-2">
                            <li>We can evaluate π(θ) for any θ ∈ Θ</li>
                            <li>There exist c > 0 and a density m(θ) such that:</li>
                            <li>We can sample from m(θ)</li>
                            <li>π(θ) ≤ c·m(θ) holds for all θ</li>
                        </ul>
                        <p>Note: If the inequality holds, c must be larger than 1</p>
                    </div>
                    
                    <div class="bg-gray-50 p-4 rounded-lg border border-gray-200 mb-6">
                        <h4 class="font-bold mb-2">Rejection Sampling Algorithm</h4>
                        <ol class="list-decimal ml-6">
                            <li>Draw a sample z ~ m(·) and compute r(z) = π(z)/(c·m(z)) ∈ (0,1)</li>
                            <li>Generate u ~ Uniform(0,1)</li>
                            <li>If u ≤ r(z), accept θ = z; otherwise, reject it and go back to step 1</li>
                        </ol>
                    </div>
                    
                    <div class="note-box">
                        <h4 class="font-bold">Proof of Correctness</h4>
                        <p>For the accepted sample θ and any t:</p>
                        <div class="formula">
                            P(θ ≤ t) = P(Z ≤ t | U ≤ π(Z)/(c·m(Z))) = ∫<sub>-∞</sub><sup>t</sup> π(z)dz / ∫<sub>ℝ</sub> π(z)dz
                        </div>
                        <p>This confirms that the accepted samples follow the target distribution π.</p>
                    </div>

                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">Key Points</h4>
                            <ul class="list-disc ml-6">
                                <li>The efficiency depends on choosing a good proposal m(θ) that closely approximates π(θ)</li>
                                <li>Smaller values of c result in higher acceptance rates</li>
                                <li>The acceptance rate is approximately 1/c</li>
                                <li>The proposal density should have heavier tails than the target</li>
                            </ul>
                        </div>
                        <div>
                            <div class="concept-diagram">
                                <svg width="300" height="200" viewBox="0 0 300 200" xmlns="http://www.w3.org/2000/svg">
                                    <!-- Target Distribution -->
                                    <path d="M50,180 C70,100 120,60 150,60 C180,60 230,100 250,180" stroke="#f97316" fill="none" stroke-width="3"/>
                                    <text x="150" y="100" text-anchor="middle" font-size="14">Target π(θ)</text>
                                    
                                    <!-- Proposal Distribution -->
                                    <path d="M20,180 C70,40 230,40 280,180" stroke="#3b82f6" fill="none" stroke-width="2" stroke-dasharray="5,5"/>
                                    <text x="150" y="30" text-anchor="middle" font-size="14">Proposal M·q(θ)</text>
                                    
                                    <!-- Accepted Points -->
                                    <circle cx="80" cy="120" r="4" fill="#10b981"/>
                                    <circle cx="130" cy="70" r="4" fill="#10b981"/>
                                    <circle cx="190" cy="90" r="4" fill="#10b981"/>
                                    
                                    <!-- Rejected Points -->
                                    <circle cx="100" cy="50" r="4" fill="#ef4444"/>
                                    <circle cx="220" cy="60" r="4" fill="#ef4444"/>
                                    
                                    <!-- Legend -->
                                    <circle cx="250" cy="170" r="4" fill="#10b981"/>
                                    <text x="260" y="173" font-size="10">Accept</text>
                                    <circle cx="250" cy="155" r="4" fill="#ef4444"/>
                                    <text x="260" y="158" font-size="10">Reject</text>
                                </svg>
                            </div>
                        </div>
                    </div>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Sampling from a Truncated Distribution</h4>
                        <p>Suppose we want to sample from a normal distribution N(0,1) but restricted to the interval [1,3].</p>
                        <ol class="list-decimal ml-6 mt-2">
                            <li>Use the unrestricted N(0,1) as the proposal q(θ)</li>
                            <li>Define p(θ|x) as the N(0,1) density for θ in [1,3] and 0 elsewhere</li>
                            <li>When a sample falls in [1,3], accept it; otherwise reject it</li>
                        </ol>
                        <p class="mt-2">This is a special case of rejection sampling where the acceptance probability is either 0 or 1.</p>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">2.3 Markov Chain Monte Carlo Methods</h3>
                    
                    <p>The goal of MCMC methods is to compute draws that are approximately from π(θ|y), the posterior distribution (density) of θ in our Bayesian model. Only for very simple models can we sample i.i.d. from the posterior directly.</p>
                    
                    <h4 class="font-bold mt-6">2.3.1 General State Space Markov Chains</h4>
                    
                    <p>A Markov chain is a sequence of random variables where the future state depends only on the current state, not on the sequence of states that preceded it. For Bayesian MCMC, we need to consider general state space Markov chains where the state space E is typically a subset of ℝ<sup>d</sup>.</p>
                    
                    <div class="formula">
                        <p>P(X<sub>n+1</sub> ∈ A | X<sub>0</sub> = x<sub>0</sub>, ..., X<sub>n</sub> = x<sub>n</sub>) = P(X<sub>n+1</sub> ∈ A | X<sub>n</sub> = x<sub>n</sub>) =: P(x<sub>n</sub>, A)</p>
                    </div>
                    
                    <p>The function P(x, A) is called the <strong>transition probability kernel</strong>. The goal of MCMC is to construct a Markov chain with a stationary distribution equal to our target posterior π(θ|y).</p>
                    
                    <div class="note-box">
                        <h4 class="font-bold">Key Properties of Markov Chains for MCMC</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>Irreducibility:</strong> The chain can reach any set with positive posterior probability from any starting state</li>
                            <li><strong>Aperiodicity:</strong> The chain doesn't cycle in a deterministic pattern</li>
                            <li><strong>Positive recurrence:</strong> The chain admits an invariant distribution (our posterior)</li>
                        </ul>
                        <p class="mt-2">A Markov chain with these properties is <strong>ergodic</strong> and converges to its unique stationary distribution, regardless of the starting state.</p>
                    </div>
                    
                    <div class="note-box">
                        <h4 class="font-bold">Ergodic Theorem</h4>
                        <p>For an ergodic Markov chain {X<sub>n</sub>} with stationary distribution π and function h where E<sub>π</sub>[|h|] < ∞:</p>
                        <div class="formula">
                            <p>(1/n)∑<sub>i=1</sub><sup>n</sup>h(X<sub>i</sub>) → ∫h(θ)π(dθ) almost surely as n → ∞</p>
                        </div>
                        <p>This means we can approximate expectations with respect to π by averaging over the chain states after an initial burn-in period.</p>
                    </div>
                    
                    <div class="tip-box">
                        <h4 class="font-bold">Statistical Inference with MCMC</h4>
                        <p>To compute posterior expectations E<sub>π</sub>[h(θ)]:</p>
                        <ol class="list-decimal ml-6 mt-2">
                            <li>Construct a Markov chain with stationary distribution π(θ|y)</li>
                            <li>Simulate T steps of the chain: θ<sup>(1)</sup>, θ<sup>(2)</sup>, ..., θ<sup>(T)</sup></li>
                            <li>Discard the first B iterations (burn-in period)</li>
                            <li>Approximate E<sub>π</sub>[h(θ)] ≈ (1/(T-B))∑<sub>t=B+1</sub><sup>T</sup>h(θ<sup>(t)</sup>)</li>
                        </ol>
                    </div>
                    
                    <p class="mb-4">In the following sections, we'll explore two popular MCMC algorithms: the Metropolis-Hastings algorithm and the Gibbs sampler.</p>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">2.4 The Metropolis-Hastings Algorithm</h3>
                    
                    <p class="mb-4">The Metropolis-Hastings algorithm is a general MCMC method for obtaining a sequence of random samples from a probability distribution that is difficult to sample from directly. It constructs a Markov chain that has the target posterior distribution as its stationary distribution.</p>
                    
                    <div class="note-box">
                        <h4 class="font-bold">Setup for the Metropolis-Hastings Algorithm</h4>
                        <p>Assume that the target distribution π has a density with respect to some measure µ (e.g., the Lebesgue measure) and denote this density by π(θ). Let E<sup>+</sup> = {θ ∈ E : π(θ) > 0} and consider a transition probability kernel Q(θ, dy) = q(θ, y) µ(dy).</p>
                    </div>
                    
                    <div class="bg-gray-50 p-4 rounded-lg border border-gray-200 mb-6">
                        <h4 class="font-bold mb-2">Algorithm Steps</h4>
                        <ol class="list-decimal ml-6">
                            <li>At iteration n, with the chain in state θ<sup>(n)</sup> = θ, generate a candidate y from the proposal distribution Q(θ, ·).</li>
                            <li>Calculate the acceptance probability:
                                <div class="formula">
                                    α(θ, y) = min\left\{\frac{π(y)q(y,θ)}{π(θ)q(θ,y)}, 1\right\} if π(θ)q(θ,y) > 0
                                </div>
                                <div class="formula">
                                    α(θ, y) = 1 if π(θ)q(θ,y) = 0
                                </div>
                            </li>
                            <li>Generate u ~ Uniform(0,1)</li>
                            <li>If u ≤ α(θ, y), set θ<sup>(n+1)</sup> = y; otherwise, set θ<sup>(n+1)</sup> = θ</li>
                            <li>Advance to the next iteration</li>
                        </ol>
                    </div>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Estimating Parameters in a Mixture Model</h4>
                        <p>Consider a dataset that might come from a mixture of two normal distributions. The posterior for the mixture parameters is complicated. Using Metropolis-Hastings:</p>
                        <ol class="list-decimal ml-6 mt-2">
                            <li>Start with initial guesses for the means, variances, and mixing proportion</li>
                            <li>Propose new values using normal distributions centered at the current values</li>
                            <li>Accept or reject based on how well they explain the data and the prior beliefs</li>
                        </ol>
                        <p class="mt-2">After convergence, the samples provide estimates of the posterior distribution for all parameters.</p>
                    </div>
                    
                    <div class="note-box">
                        <h4 class="font-bold">Proof of Stationarity</h4>
                        <p>To verify that π is a stationary distribution for the chain, we need to show that the chain is reversible with respect to π, meaning:</p>
                        <div class="formula">
                            <p>π(x) p(x,y) = π(y) p(y,x) for all x,y ∈ E</p>
                        </div>
                        <p>where p(x,y) is the transition density of the Markov chain. This can be verified by direct calculation using the definition of the acceptance probability α(x,y).</p>
                    </div>
                    
                    <div class="tip-box">
                        <h4 class="font-bold">Common Proposal Distributions</h4>
                        <ol class="list-decimal ml-6 mt-2">
                            <li><strong>Random Walk Metropolis-Hastings:</strong> q(x,y) = f(y-x) where f is a density
                                <ul class="list-disc ml-6">
                                    <li>This is equivalent to setting Y = x + Z with Z ~ f (typically Z ~ N(0,σ) or Z ~ t-distribution)</li>
                                    <li>If f(x) > 0 for all x ∈ E, then the chain is (π-)irreducible, recurrent, and aperiodic</li>
                                    <li>For a symmetric proposal (f(z) = f(-z)), the acceptance probability simplifies to α(x,y) = min(1, π(y)/π(x))</li>
                                </ul>
                            </li>
                            <li><strong>Independence Metropolis-Hastings:</strong> q(x,y) = f(y) where f is a density that doesn't depend on x
                                <ul class="list-disc ml-6">
                                    <li>This is equivalent to setting Y ~ f independent of the current state</li>
                                    <li>If f(x) > 0 almost everywhere on E<sup>+</sup>, then the chain is (π-)irreducible, recurrent, and aperiodic</li>
                                </ul>
                            </li>
                        </ol>
                    </div>
                    
                    <div class="tip-box">
                        <h4 class="font-bold">Practical Tips for Metropolis-Hastings</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li>In practice, the acceptance rate should typically be between 0.2 and 0.5</li>
                            <li>Too low an acceptance rate means the chain gets "stuck" at the same values</li>
                            <li>Too high an acceptance rate is associated with high correlation between successive draws</li>
                            <li>The acceptance rate can be controlled by tuning parameters in the proposal distribution (e.g., the variance in the random walk)</li>
                        </ul>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">2.5 Gibbs Sampler</h3>
                    
                    <p class="mb-4">The Gibbs sampler is a special case of the Metropolis-Hastings algorithm where the proposal distributions are the full conditional distributions, resulting in acceptance probabilities of 1.</p>
                    
                    <div class="bg-gray-50 p-4 rounded-lg border border-gray-200 mb-6">
                        <h4 class="font-bold mb-2">Algorithm Steps</h4>
                        <p>For a parameter vector θ = (θ₁, θ₂, ..., θₚ):</p>
                        <ol class="list-decimal ml-6">
                            <li>Start with initial values θ⁰ = (θ₁⁰, θ₂⁰, ..., θₚ⁰)</li>
                            <li>For t = 1 to T:
                                <ol class="list-decimal ml-6">
                                    <li>Sample θ₁ᵗ from p(θ₁|θ₂ᵗ⁻¹, θ₃ᵗ⁻¹, ..., θₚᵗ⁻¹, x)</li>
                                    <li>Sample θ₂ᵗ from p(θ₂|θ₁ᵗ, θ₃ᵗ⁻¹, ..., θₚᵗ⁻¹, x)</li>
                                    <li>...</li>
                                    <li>Sample θₚᵗ from p(θₚ|θ₁ᵗ, θ₂ᵗ, ..., θₚ₋₁ᵗ, x)</li>
                                </ol>
                            </li>
                        </ol>
                    </div>
                    
                    <div class="concept-diagram text-center">
                        <svg width="400" height="250" viewBox="0 0 400 250" xmlns="http://www.w3.org/2000/svg">
                            <!-- Axes -->
                            <line x1="50" y1="150" x2="350" y2="150" stroke="black" stroke-width="1"/>
                            <line x1="50" y1="50" x2="50" y2="150" stroke="black" stroke-width="1"/>
                            <text x="200" y="180" text-anchor="middle" font-size="14">θ₁</text>
                            <text x="30" y="100" text-anchor="middle" font-size="14" transform="rotate(-90, 30, 100)">θ₂</text>
                            
                            <!-- Contours of joint distribution -->
                            <ellipse cx="200" cy="100" rx="120" ry="70" fill="none" stroke="#3b82f6" stroke-width="1"/>
                            <ellipse cx="200" cy="100" rx="80" ry="45" fill="none" stroke="#3b82f6" stroke-width="1"/>
                            <ellipse cx="200" cy="100" rx="40" ry="25" fill="none" stroke="#3b82f6" stroke-width="1"/>
                            
                            <!-- Gibbs sampling path -->
                            <polyline points="150,150 220,150 220,80 270,80 270,110 200,110 200,90 240,90 240,125 180,125 180,95" 
                                    fill="none" stroke="#f97316" stroke-width="2"/>
                            
                            <!-- Starting point -->
                            <circle cx="150" cy="150" r="4" fill="#ef4444"/>
                            <text x="140" y="165" font-size="10">Start</text>
                            
                            <!-- Horizontal/Vertical Steps -->
                            <text x="350" y="175" font-size="10">Sample θ₁|θ₂</text>
                            <text x="20" y="40" font-size="10">Sample θ₂|θ₁</text>
                        </svg>
                    </div>
                    
                    <div class="example-box mt-6">
                        <h4 class="font-bold">Example: Wing Length of Midges</h4>
                        <p>We have collected n measurements of the wing length of midges (of the same species): y<sub>1</sub>, ..., y<sub>n</sub>. We assume:</p>
                        <div class="formula">
                            y = Xβ + Zu + ε
                        </div>
                        
                        <p class="mt-4">With a Bayesian approach:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>Prior:</strong> Normal priors for β coefficients based on previous studies</li>
                            <li><strong>Likelihood:</strong> Multivariate normal likelihood for wing length measurements</li>
                            <li><strong>Posterior inference:</strong> Gibbs sampler for posterior distribution</li>
                            <li><strong>Prediction:</strong> Posterior predictive probabilities with uncertainty quantification</li>
                        </ul>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">2.6 Convergence Diagnostics</h3>
                    
                    <p class="mb-4">Determining when a Markov chain has converged to its stationary distribution is crucial. Various diagnostic methods can help:</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">Visual Methods</h4>
                            <ul class="list-disc ml-6">
                                <li><strong>Trace plots:</strong> Plot parameter values against iteration number</li>
                                <li><strong>Autocorrelation plots:</strong> Show correlation between samples at different lags</li>
                                <li><strong>Running means:</strong> Plot cumulative means to check stability</li>
                            </ul>
                            <p class="mt-2">Look for stabilization and random fluctuation around a central value.</p>
                        </div>
                        <div>
                            <h4 class="font-bold">Numerical Diagnostics</h4>
                            <ul class="list-disc ml-6">
                                <li><strong>Gelman-Rubin statistic:</strong> Compare within-chain and between-chain variance</li>
                                <li><strong>Effective sample size:</strong> Estimate how many independent samples the correlated MCMC samples are equivalent to</li>
                                <li><strong>Geweke diagnostic:</strong> Compare means from different segments of the chain</li>
                            </ul>
                        </div>
                    </div>
                    
                    <div class="tip-box">
                        <h4 class="font-bold">Practical Tips for MCMC</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Run multiple chains with different starting points</li>
                            <li>Discard early samples as burn-in</li>
                            <li>Thin chains by keeping every kth sample if autocorrelation is high</li>
                            <li>Adjust proposal distributions to achieve acceptance rates around 20-40%</li>
                            <li>Check convergence with both visual and numerical diagnostics</li>
                        </ul>
                    </div>
                    
                    <div class="example-box mt-6">
                        <h4 class="font-bold">Real-World Example: Climate Model Parameter Estimation</h4>
                        <p>Climate scientists use MCMC methods to estimate parameters in complex climate models:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>Parameters:</strong> Climate sensitivity, aerosol effects, and ocean mixing rates</li>
                            <li><strong>Data:</strong> Historical temperature data</li>
                            <li><strong>Model:</strong> Complex climate model</li>
                            <li><strong>Posterior:</strong> Estimated using MCMC</li>
                            <li><strong>Uncertainty:</strong> Quantified through posterior distributions</li>
                        </ul>
                        <p class="mt-2">The resulting posterior distributions quantify uncertainty in future climate projections.</p>
                    </div>
                </section>

                <div class="section-divider"></div>

                <!-- Bayesian Linear Models -->
                <section id="linear-models" class="bg-white rounded-lg shadow-md p-8 mb-8">
                    <h2 class="text-2xl font-bold text-blue-700 mb-4">3. Bayesian Linear Models</h2>
                    
                    <p class="mb-4">Bayesian linear models extend traditional linear regression by incorporating prior information and quantifying uncertainty about all parameters.</p>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mb-3">3.1 The Likelihood in the Linear Regression Model</h3>
                    
                    <p class="mb-4">In a linear regression model, we have:</p>
                    
                    <div class="formula">
                        y = Xβ + ε, where ε ~ N(0, σ²I)
                    </div>
                    
                    <p>Where:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>y is an n×1 vector of responses</li>
                        <li>X is an n×p design matrix of predictors</li>
                        <li>β is a p×1 vector of regression coefficients</li>
                        <li>ε is an n×1 vector of independent normal errors with variance σ²</li>
                    </ul>
                    
                    <p>The likelihood function is:</p>
                    
                    <div class="formula">
                        p(y|X,β,σ²) = (2πσ²)^(-n/2) exp(-(y-Xβ)'(y-Xβ)/(2σ²))
                    </div>

                    <p>Alternatively, using the precision parameter τ = 1/σ², the likelihood can be written as:</p>
                    
                    <div class="formula">
                        p(y|X,β,τ) ∝ τ^(n/2) exp(-τ/2 · (y-Xβ)'(y-Xβ))
                    </div>
                    
                    <p>The maximum likelihood estimates (MLE) of these parameters are:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>β̂ = (X'X)⁻¹X'y with covariance cov(β̂|σ²) = σ²(X'X)⁻¹</li>
                        <li>σ̂² = s²/(n-p) where s² = (y-Xβ̂)'(y-Xβ̂) is the sum of squared residuals</li>
                    </ul>
                    
                    <p>The likelihood can be rewritten using these quantities:</p>
                    
                    <div class="formula">
                        (y-Xβ)'(y-Xβ) = s² + (β-β̂)'X'X(β-β̂)
                    </div>
                    
                    <div class="formula">
                        p(y|X,β,σ²) ∝ (σ²)^(-n/2) exp(-s²/(2σ²) - (β-β̂)'X'X(β-β̂)/(2σ²))
                    </div>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Housing Price Model</h4>
                        <p>A real estate analyst models house prices based on size, location, and age:</p>
                        <div class="formula">
                            Price = β₀ + β₁×Size + β₂×Location + β₃×Age + ε
                        </div>
                        <p class="mt-2">The likelihood tells us how probable the observed prices are, given specific values for the coefficients and error variance.</p>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">3.2 Priors and Posteriors</h3>
                    
                    <p class="mb-4">We'll discuss different types of prior specifications for (β,τ) or (β,σ²) in the linear regression model.</p>
                    
                    <h4 class="font-bold mt-6">Conjugate Prior for β when σ² is known</h4>
                    
                    <p class="mb-4">When σ² is known, the conjugate prior for β is:</p>
                    
                    <div class="formula">
                        β ~ N(b₀, B₀)
                    </div>
                    
                    <p>Where B₀ is an invertible p×p matrix. With this prior, the posterior distribution becomes:</p>
                    
                    <div class="formula">
                        β|y ~ N((τX'X + B₀⁻¹)⁻¹(τX'Xβ̂ + B₀⁻¹b₀), (τX'X + B₀⁻¹)⁻¹)
                    </div>
                    
                    <p class="mb-4">The posterior mean is a weighted average of the prior mean b₀ and the maximum likelihood estimate β̂. Note that X'X doesn't need to be invertible as long as B₀ is invertible.</p>
                    
                    <h4 class="font-bold mt-6">Conjugate Prior for β and σ²: Normal-invgamma Prior</h4>
                    
                    <p class="mb-4">The conjugate prior for both parameters is:</p>
                    
                    <div class="formula">
                        π(β,σ²) = π(β|σ²)π(σ²)
                    </div>
                    
                    <p>Where:</p>
                    
                    <div class="formula">
                        β|σ² ~ N(b₀, σ²B₀)
                    </div>
                    
                    <div class="formula">
                        σ² ~ inv-gamma(ν₀/2, ν₀σ₀²/2)
                    </div>
                    
                    <p>With B₀ invertible and ν₀, σ₀² > 0. The posterior distribution is:</p>
                    
                    <div class="formula">
                        π(β,σ²|y) = π(β|σ²,y)π(σ²|y)
                    </div>
                    
                    <p>Where:</p>
                    
                    <div class="formula">
                        β|σ²,y,X ~ N(bₙ, σ²Bₙ)
                    </div>
                    
                    <div class="formula">
                        σ²|y,X ~ inv-gamma(νₙ/2, νₙσₙ²/2)
                    </div>
                    
                    <p>With:</p>
                    
                    <ul class="list-disc ml-6 mb-4">
                        <li>Bₙ = (X'X + B₀⁻¹)⁻¹</li>
                        <li>bₙ = Bₙ(X'Xβ̂ + B₀⁻¹b₀)</li>
                        <li>νₙ = ν₀ + n</li>
                        <li>νₙσₙ² = ν₀σ₀² + s² + (b₀-β̂)'(B₀ + (X'X)⁻¹)⁻¹(b₀-β̂)</li>
                    </ul>
                    
                    <p class="mb-4">The marginal posterior for β is a multivariate t-distribution: β|y,X ~ t_νₙ(bₙ, σₙ²Bₙ).</p>
                    
                    <p>For new observations, the predictive distribution is:</p>
                    
                    <div class="formula">
                        Yₙₑw|y,X,Xₙₑw ~ t_νₙ(Xₙₑwbₙ, σₙ²(Iₘ + XₙₑwBₙXₙₑw'))
                    </div>
                    
                    <h4 class="font-bold mt-6">Zellner's g-Prior</h4>
                    
                    <p class="mb-4">A popular prior specification is the g-prior proposed by Zellner:</p>
                    
                    <div class="formula">
                        σ² ~ inv-gamma(0/2, 0×σ₀²/2) ∝ 1/σ² I(0,+∞)(σ²) [improper]
                    </div>
                    
                    <div class="formula">
                        β|σ² ~ N(b₀, σ²B₀), with B₀ = c(X'X)⁻¹
                    </div>
                    
                    <p>Where c is a positive constant. This prior requires X'X to be invertible. The posterior becomes:</p>
                    
                    <div class="formula">
                        β|σ²,y,X ~ N(bₙ, σ²Bₙ)
                    </div>
                    
                    <div class="formula">
                        σ²|y,X ~ inv-gamma(n/2, s²/2 + (β̂-b₀)'Bₙ⁻¹(β̂-b₀)/2)
                    </div>
                    
                    <p>Where:</p>
                    
                    <ul class="list-disc ml-6 mb-4">
                        <li>Bₙ = c/(c+1) · (X'X)⁻¹</li>
                        <li>bₙ = c/(c+1) · β̂ + 1/(c+1) · b₀</li>
                    </ul>
                    
                    <p class="mb-4">Note that the posterior mean is a weighted average of the prior mean b₀ and the MLE β̂. As c increases, more weight is given to the observed data. Often c = log(n) is used.</p>
                    
                    <h4 class="font-bold mt-6">Reference Prior</h4>
                    
                    <p class="mb-4">The flat (improper) prior for (β,σ²) is:</p>
                    
                    <div class="formula">
                        π(β,σ²) ∝ 1/σ² · I(0,+∞)(σ²) or π(β,τ) ∝ 1/τ · I(0,+∞)(τ)
                    </div>
                    
                    <p>With this prior, the posterior becomes:</p>
                    
                    <div class="formula">
                        π(β,τ|y) = π(β|τ,y)π(τ|y)
                    </div>
                    
                    <p>Where:</p>
                    
                    <div class="formula">
                        β|τ,y,X ~ N(β̂, (τX'X)⁻¹)
                    </div>
                    
                    <div class="formula">
                        τ|y,X ~ gamma((n-p)/2, s²/2)
                    </div>
                    
                    <p class="mb-4">This posterior is proper if n > p and X'X is invertible.</p>
                    
                    <p>For new observations, the predictive distribution is:</p>
                    
                    <div class="formula">
                        Yₙₑw|y,X,Xₙₑw ~ t_{n-p}(Xₙₑwβ̂, s²/(n-p) · (Iₘ + Xₙₑw(X'X)⁻¹Xₙₑw'))
                    </div>
                    
                    <h4 class="font-bold mt-6">Conditionally Conjugate Independence Prior</h4>
                    
                    <p class="mb-4">A common choice is to assume β and τ are a priori independent:</p>
                    
                    <div class="formula">
                        β ~ N₍ₚ₎(b₀, B₀) and τ ~ gamma(a, b)
                    </div>
                    
                    <p class="mb-4">This prior is semi-conjugate, meaning the full conditionals in a Gibbs sampler are in closed form. Specifically, the full conditional of β is a p-dimensional Gaussian, and the full conditional of τ is a gamma density.</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">3.2.1 Conjugate Prior for β when σ² is known</h4>
                            <p class="mb-2">When σ² is known, the conjugate prior for β is:</p>
                            <div class="formula">
                                β ~ N(b₀, B₀)
                            </div>
                            <p>Where B₀ is an invertible p×p matrix. With this prior, the posterior distribution becomes:</p>
                            <div class="formula">
                                β|y ~ N((τX'X + B₀⁻¹)⁻¹(τX'Xβ̂ + B₀⁻¹b₀), (τX'X + B₀⁻¹)⁻¹)
                            </div>
                            <p class="mb-4">The posterior mean is a weighted average of the prior mean b₀ and the maximum likelihood estimate β̂. Note that X'X doesn't need to be invertible as long as B₀ is invertible.</p>
                        </div>
                        <div>
                            <h4 class="font-bold">3.2.2 Conjugate Prior for β and σ²: Normal-invgamma Prior</h4>
                            <p class="mb-2">The conjugate prior for both parameters is:</p>
                            <div class="formula">
                                π(β,σ²) = π(β|σ²)π(σ²)
                            </div>
                            <p>Where:</p>
                            <div class="formula">
                                β|σ² ~ N(b₀, σ²B₀)
                            </div>
                            <div class="formula">
                                σ² ~ inv-gamma(ν₀/2, ν₀σ₀²/2)
                            </div>
                            <p>With B₀ invertible and ν₀, σ₀² > 0. The posterior distribution is:</p>
                            <div class="formula">
                                π(β,σ²|y) = π(β|σ²,y)π(σ²|y)
                            </div>
                            <p>Where:</p>
                            <div class="formula">
                                β|σ²,y,X ~ N(bₙ, σ²Bₙ)
                            </div>
                            <div class="formula">
                                σ²|y,X ~ inv-gamma(νₙ/2, νₙσₙ²/2)
                            </div>
                            <p>With:</p>
                            <ul class="list-disc ml-6 mb-4">
                                <li>Bₙ = (X'X + B₀⁻¹)⁻¹</li>
                                <li>bₙ = Bₙ(X'Xβ̂ + B₀⁻¹b₀)</li>
                                <li>νₙ = ν₀ + n</li>
                                <li>νₙσₙ² = ν₀σ₀² + s² + (b₀-β̂)'(B₀ + (X'X)⁻¹)⁻¹(b₀-β̂)</li>
                            </ul>
                            <p class="mb-4">The marginal posterior for β is a multivariate t-distribution: β|y,X ~ t_νₙ(bₙ, σₙ²Bₙ).</p>
                        </div>
                    </div>
                    
                    <div class="concept-diagram text-center mt-6">
                        <svg width="500" height="250" viewBox="0 0 500 250" xmlns="http://www.w3.org/2000/svg">
                            <!-- Data points -->
                            <circle cx="100" cy="80" r="3" fill="#3b82f6"/>
                            <circle cx="150" cy="100" r="3" fill="#3b82f6"/>
                            <circle cx="200" cy="130" r="3" fill="#3b82f6"/>
                            <circle cx="250" cy="135" r="3" fill="#3b82f6"/>
                            <circle cx="300" cy="160" r="3" fill="#3b82f6"/>
                            <circle cx="350" cy="170" r="3" fill="#3b82f6"/>
                            <circle cx="400" cy="200" r="3" fill="#3b82f6"/>
                            
                            <!-- Prior regression lines -->
                            <line x1="50" y1="50" x2="450" y2="150" stroke="#f97316" stroke-width="1" stroke-dasharray="5,5"/>
                            <line x1="50" y1="70" x2="450" y2="170" stroke="#f97316" stroke-width="1" stroke-dasharray="5,5"/>
                            <line x1="50" y1="90" x2="450" y2="190" stroke="#f97316" stroke-width="1" stroke-dasharray="5,5"/>
                            
                            <!-- Posterior regression lines -->
                            <line x1="50" y1="65" x2="450" y2="185" stroke="#10b981" stroke-width="2"/>
                            <line x1="50" y1="60" x2="450" y2="180" stroke="#10b981" stroke-width="1" stroke-opacity="0.5"/>
                            <line x1="50" y1="70" x2="450" y2="190" stroke="#10b981" stroke-width="1" stroke-opacity="0.5"/>
                            
                            <!-- Labels -->
                            <text x="75" y="30" font-size="12" fill="#f97316">Prior Lines</text>
                            <text x="75" y="45" font-size="12" fill="#10b981">Posterior Lines</text>
                            <text x="75" y="60" font-size="12" fill="#3b82f6">Data Points</text>
                        </svg>
                        <p class="text-center text-sm text-gray-500 mt-2">Visualization of how prior beliefs (orange dashed lines) are updated with observed data (blue points) to form posterior distributions (green lines).</p>
                    </div>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Stock Return Prediction</h4>
                        <p>A financial analyst develops a Bayesian linear model to predict stock returns based on economic indicators:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>Prior knowledge:</strong> Based on economic theory, the analyst believes that interest rates have a negative effect and economic growth has a positive effect on returns</li>
                            <li><strong>Data:</strong> Monthly returns and economic indicators for the past 10 years</li>
                            <li><strong>Posterior inference:</strong> The posterior distribution combines prior beliefs with the data, giving more precise estimates and credible intervals for the effects of each indicator</li>
                            <li><strong>Prediction:</strong> Posterior predictive probabilities with uncertainty quantification</li>
                        </ul>
                        <p class="mt-2">The analyst can use the posterior predictive distribution to forecast returns and quantify uncertainty.</p>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">3.3 Generalized Linear Models</h3>
                    
                    <p class="mb-4">Generalized Linear Models (GLMs) extend linear models to handle response variables with non-normal distributions.</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">Components of a GLM</h4>
                            <ul class="list-disc ml-6">
                                <li><strong>Random component:</strong> Response distribution from the exponential family</li>
                                <li><strong>Systematic component:</strong> Linear predictor η = Xβ</li>
                                <li><strong>Link function:</strong> g(μ) = η connects expected response to linear predictor</li>
                            </ul>
                        </div>
                        <div>
                            <h4 class="font-bold">Common GLM Types</h4>
                            <ul class="list-disc ml-6">
                                <li><strong>Logistic regression:</strong> For binary responses</li>
                                <li><strong>Poisson regression:</strong> For count data</li>
                                <li><strong>Gamma regression:</strong> For positive continuous data</li>
                                <li><strong>Negative binomial:</strong> For overdispersed counts</li>
                            </ul>
                        </div>
                    </div>
                    
                    <p class="mb-4">A GLM consists of three elements:</p>
                    
                    <ol class="list-decimal ml-6 mb-4">
                        <li><strong>Random component:</strong> The distribution of Y<sub>i</sub> given x<sub>i</sub> belongs to an exponential family:
                            <div class="formula">
                                f(y<sub>i</sub>|θ<sub>i</sub>,φ) = exp((y<sub>i</sub>θ<sub>i</sub>-b(θ<sub>i</sub>))/φ + c(y<sub>i</sub>,φ))
                            </div>
                            where θ<sub>i</sub> is the natural parameter and φ is the scale parameter. We denote by μ<sub>i</sub> the expectation E[Y<sub>i</sub>|x<sub>i</sub>].
                        </li>
                        <li><strong>Linear predictor:</strong> η<sub>i</sub> = x<sub>i</sub><sup>T</sup>β (we choose the covariates)</li>
                        <li><strong>Link function:</strong> g(μ<sub>i</sub>) = η<sub>i</sub>, or equivalently, μ<sub>i</sub> = h(η<sub>i</sub>) = h(x<sub>i</sub><sup>T</sup>β) where h is the inverse of g</li>
                    </ol>
                    
                    <p class="mb-4">The natural link function is obtained by equating the natural parameter to the linear predictor.</p>
                    
                    <p class="mb-4">The parameters of interest in GLMs are β and φ, with common prior specifications being:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>π(β,φ) = π(β|φ)π(φ)</li>
                        <li>π(β,φ) = π(β)π(φ)</li>
                    </ul>
                    
                    <p class="mb-4">The typical prior for β is N<sub>p</sub>(b<sub>0</sub>,B<sub>0</sub>), where the prior mean is often chosen as 0 if the response is centered, and B<sub>0</sub> is a covariance matrix given by a constant c (larger than 1, e.g., 10) times a frequentist estimate.</p>
                    
                    <h4 class="font-bold mt-6">3.3.1 Binary Response Regression</h4>
                    
                    <p class="mb-4">In binary response regression, Y ∈ {0,1} and the random component follows a Bernoulli distribution: Y<sub>i</sub>|x<sub>i</sub> ~ Be(π<sub>i</sub>).</p>
                    
                    <p class="mb-4">Since μ<sub>i</sub> = π<sub>i</sub> ∈ (0,1), a proper choice for the link function is π<sub>i</sub> = F(x<sub>i</sub><sup>T</sup>β) where F is a cumulative distribution function. Common choices include:</p>
                    
                    <ul class="list-disc ml-6 mb-4">
                        <li><strong>Probit model:</strong> F(x) = Φ(x) (standard normal CDF)</li>
                        <li><strong>Logit model:</strong> F(x) = e<sup>x</sup>/(1+e<sup>x</sup>) (logistic CDF)</li>
                        <li><strong>Complementary log-log model:</strong> F(x) = 1-e<sup>-e<sup>x</sup></sup></li>
                    </ul>
                    
                    <p class="mb-4">The Bernoulli distribution belongs to the exponential family with no scale parameter, as:</p>
                    
                    <div class="formula">
                        P(Y<sub>i</sub>=y|x<sub>i</sub>) = π<sub>i</sub><sup>y</sup>(1-π<sub>i</sub>)<sup>1-y</sup> = exp(y·log(π<sub>i</sub>/(1-π<sub>i</sub>))+log(1-π<sub>i</sub>))
                    </div>
                    
                    <h5 class="font-semibold mt-4 mb-2">Gibbs Sampler for the Probit Model</h5>
                    
                    <p class="mb-4">For the probit model, a useful computational approach introduces latent variables:</p>
                    
                    <div class="formula">
                        Z₁,...,Zₙ ~ N(xᵢ<sup>T</sup>β, 1) independently
                    </div>
                    
                    <div class="formula">
                        Yᵢ = { 1 if Zᵢ > 0
                              { 0 if Zᵢ ≤ 0
                    </div>
                    
                    <p class="mb-4">This is equivalent to Yᵢ being Bernoulli with parameter Φ(xᵢ<sup>T</sup>β). The full conditionals for Gibbs sampling are:</p>
                    
                    <ul class="list-disc ml-6 mb-4">
                        <li>Zᵢ|β,y ~ N(xᵢ<sup>T</sup>β,1) truncated to (0,∞) if yᵢ=1, or truncated to (-∞,0) if yᵢ=0</li>
                        <li>β|Z,y ~ N(b,B) where B = (X'X + B₀<sup>-1</sup>)<sup>-1</sup> and b = B(X'Xβ̂ + B₀<sup>-1</sup>b₀)</li>
                    </ul>
                    
                    <p class="mb-4">This approach transforms the probit regression into a conditionally normal model, making computation efficient.</p>
                    
                    <div class="formula">
                        p(y|X,β) = ∏<sub>i</sub> p<sub>i</sub><sup>y<sub>i</sub></sup>(1-p<sub>i</sub>)<sup>1-y<sub>i</sub></sup>
                    </div>
                    
                    <p>For the logit model, p<sub>i</sub> = 1/(1+exp(-x<sub>i</sub>'β))</p>
                    
                    <p class="mb-4">Unlike linear regression, there is no conjugate prior for β in logistic regression, so MCMC methods are typically used.</p>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Credit Default Prediction</h4>
                        <p>A bank wants to predict the probability of loan default based on customer characteristics:</p>
                        <div class="formula">
                            logit(P(Default)) = β₀ + β₁×Income + β₂×CreditScore + β₃×DebtRatio
                        </div>
                        
                        <p class="mt-4">With a Bayesian approach:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>Prior:</strong> Normal priors for β coefficients based on previous default studies</li>
                            <li><strong>Likelihood:</strong> Bernoulli likelihood for default/non-default outcomes</li>
                            <li><strong>Posterior:</strong> Obtained via MCMC sampling</li>
                            <li><strong>Prediction:</strong> Posterior predictive probabilities with uncertainty quantification</li>
                        </ul>
                    </div>
                    
                    <div class="note-box mt-6">
                        <h4 class="font-bold">Why Use Bayesian GLMs?</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Naturally incorporates uncertainty in all parameters</li>
                            <li>Handles separation problems (perfect prediction) better than maximum likelihood</li>
                            <li>Allows for inclusion of prior information about coefficients</li>
                            <li>Provides full posterior predictive distributions, not just point estimates</li>
                            <li>Can be extended to more complex models like multilevel/hierarchical structures</li>
                        </ul>
                    </div>
                </section>

                <div class="section-divider"></div>

                <!-- Hierarchical Models -->
                <section id="hierarchical" class="bg-white rounded-lg shadow-md p-8 mb-8">
                    <h2 class="text-2xl font-bold text-blue-700 mb-4">4. Hierarchical Models</h2>
                    
                    <p class="mb-4">Hierarchical models (also called multilevel models) are designed for data with a hierarchical or grouped structure. They allow for partial pooling of information across groups.</p>
                    
                    <p class="mb-4">These models address situations where important predictor variables may be:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>Not taken into account but should have been</li>
                        <li>Difficult to measure and consequently left out</li>
                        <li>Perhaps impossible to measure</li>
                    </ul>
                    
                    <p class="mb-4">Such variables are sometimes referred to as latent variables. In a generic two-level hierarchical model, we have:</p>
                    
                    <ul class="list-disc ml-6 mb-4">
                        <li>Data y</li>
                        <li>Latent variables γ</li>
                        <li>Parameters θ</li>
                    </ul>
                    
                    <p class="mb-4">The joint distribution is given by:</p>
                    
                    <div class="formula">
                        L(y,γ,θ) = p(y|θ,γ)p(γ|θ)π(θ)
                    </div>
                    
                    <p class="mb-4">From this we can obtain the posterior distribution:</p>
                    
                    <div class="formula">
                        p(θ,γ|y) ∝ L(y,γ,θ)
                    </div>
                    
                    <p class="mb-4">For prediction of future data z, the predictive density is:</p>
                    
                    <div class="formula">
                        p(z|y) = ∫∫ p(z|θ,γ,y)p(θ,γ|y) dθ dγ
                    </div>
                    
                    <div class="concept-diagram text-center mb-6">
                        <svg width="500" height="250" viewBox="0 0 500 250" xmlns="http://www.w3.org/2000/svg">
                            <!-- Levels -->
                            <rect x="50" y="30" width="400" height="50" fill="#e0e7ff" stroke="#3b82f6" stroke-width="2" rx="5"/>
                            <text x="250" y="60" text-anchor="middle" font-size="16">Hyperparameters (Population Level)</text>
                            
                            <rect x="50" y="110" width="400" height="50" fill="#ecfdf5" stroke="#10b981" stroke-width="2" rx="5"/>
                            <text x="250" y="140" text-anchor="middle" font-size="16">Group-Level Parameters</text>
                            
                            <rect x="50" y="190" width="400" height="50" fill="#fff7ed" stroke="#f97316" stroke-width="2" rx="5"/>
                            <text x="250" y="220" text-anchor="middle" font-size="16">Data (Individual Observations)</text>
                            
                            <!-- Arrows -->
                            <line x1="250" y1="80" x2="250" y2="110" stroke="black" stroke-width="2" marker-end="url(#arrow)"/>
                            <line x1="250" y1="160" x2="250" y2="190" stroke="black" stroke-width="2" marker-end="url(#arrow)"/>
                            
                            <!-- Arrow Marker -->
                            <defs>
                                <marker id="arrow" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                    <path d="M0,0 L0,6 L9,3 z" fill="black"/>
                                </marker>
                            </defs>
                        </svg>
                    </div>
                    
                    <div class="tip-box">
                        <h4 class="font-bold">Why Use Hierarchical Models?</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>Partial pooling:</strong> Balance between complete pooling (ignoring group differences) and no pooling (analyzing groups separately)</li>
                            <li><strong>Shrinkage:</strong> Group estimates are pulled toward the population mean, especially for groups with little data</li>
                            <li><strong>Information sharing:</strong> Groups can "borrow strength" from each other</li>
                            <li><strong>Handle unbalanced designs:</strong> Works well even when group sizes vary substantially</li>
                        </ul>
                    </div>
                    
                    <div class="example-box mt-6">
                        <h4 class="font-bold">Example: Educational Assessment</h4>
                        <p>Researchers analyze test scores of students nested within schools:</p>
                        
                        <div class="mt-4 mb-4">
                            <p class="font-semibold">Hierarchical Structure:</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li><strong>Level 1:</strong> Students (individual test scores)</li>
                                <li><strong>Level 2:</strong> Schools (school-specific effects)</li>
                                <li><strong>Level 3:</strong> Districts (district-level factors)</li>
                            </ul>
                        </div>
                        
                        <div class="formula">
                            Score_ijk = β₀ + α_j + γ_k + β₁×X_ijk + ε_ijk
                        </div>
                        <p class="mt-2">Where:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li>i = student, j = school, k = district</li>
                            <li>α_j ~ N(0, σ²_school) are school-specific random effects</li>
                            <li>γ_k ~ N(0, σ²_district) are district-specific random effects</li>
                        </ul>
                        
                        <p class="mt-4">This model allows researchers to:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Estimate school and district effects while accounting for student characteristics</li>
                            <li>Quantify variation at each level of the hierarchy</li>
                            <li>Make more accurate predictions for new schools with limited data</li>
                        </ul>
                    </div>
                    
                    <p class="mt-6 mb-4">A general hierarchical model can be written as:</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">Data Level</h4>
                            <div class="formula">
                                y_ij ~ p(y_ij|θ_j)
                            </div>
                            <p>Data within group j depends on group-specific parameters θ_j</p>
                        </div>
                        <div>
                            <h4 class="font-bold">Group Level</h4>
                            <div class="formula">
                                θ_j ~ p(θ_j|φ)
                            </div>
                            <p>Group parameters come from a population distribution with hyperparameters φ</p>
                        </div>
                    </div>
                    
                    <div>
                        <h4 class="font-bold">Population Level</h4>
                        <div class="formula">
                            φ ~ p(φ)
                        </div>
                        <p>Hyperparameters have their own prior distributions</p>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">4.1 Linear Mixed Effect Models</h3>
                    
                    <p class="mb-4">Linear mixed effects models include both fixed effects (population-level parameters) and random effects (group-level deviations).</p>
                    
                    <div class="formula">
                        y = Xβ + Zu + ε
                    </div>
                    
                    <p>Where:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>β are fixed effects (common across all groups)</li>
                        <li>u are random effects (specific to each group)</li>
                        <li>X and Z are design matrices for fixed and random effects</li>
                        <li>u ~ N(0, Σ) with covariance matrix Σ</li>
                        <li>ε ~ N(0, σ²I) are individual-level errors</li>
                    </ul>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Longitudinal Study of Patient Recovery</h4>
                        <p>A medical study tracks recovery times for patients after surgery, with measurements taken at several time points:</p>
                        
                        <div class="formula">
                            y_ij = β₀ + β₁×time_ij + u₀ᵢ + u₁ᵢ×time_ij + ε_ij
                        </div>
                        
                        <p class="mt-2">Where:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li>y_ij is the recovery measure for patient i at time j</li>
                            <li>β₀, β₁ are fixed effects (population average intercept and slope)</li>
                            <li>u₀ᵢ, u₁ᵢ are random effects (patient-specific deviations from the population intercept and slope)</li>
                            <li>(u₀ᵢ, u₁ᵢ) ~ N(0, Σ) with Σ allowing for correlation between random intercepts and slopes</li>
                        </ul>
                        
                        <div class="concept-diagram text-center mt-4">
                            <svg width="500" height="200" viewBox="0 0 500 200" xmlns="http://www.w3.org/2000/svg">
                                <!-- Axes -->
                                <line x1="50" y1="150" x2="450" y2="150" stroke="black" stroke-width="1"/>
                                <line x1="50" y1="50" x2="50" y2="150" stroke="black" stroke-width="1"/>
                                <text x="250" y="180" text-anchor="middle" font-size="14">Time</text>
                                <text x="25" y="100" text-anchor="middle" font-size="14" transform="rotate(-90, 25, 100)">Recovery</text>
                                
                                <!-- Population (fixed effects) line -->
                                <line x1="50" y1="120" x2="450" y2="60" stroke="#3b82f6" stroke-width="3"/>
                                <text x="460" y="60" font-size="12">Population</text>
                                
                                <!-- Patient-specific (random effects) lines -->
                                <line x1="50" y1="140" x2="450" y2="100" stroke="#f97316" stroke-width="1.5"/>
                                <line x1="50" y1="130" x2="450" y2="40" stroke="#f97316" stroke-width="1.5"/>
                                <line x1="50" y1="100" x2="450" y2="50" stroke="#f97316" stroke-width="1.5"/>
                                <line x1="50" y1="110" x2="450" y2="80" stroke="#f97316" stroke-width="1.5"/>
                                
                                <text x="460" y="90" font-size="12">Patients</text>
                            </svg>
                        </div>
                        
                        <p class="mt-4">This model captures both the average recovery trajectory and individual differences in recovery patterns.</p>
                    </div>
                    
                    <div class="comparison-table mt-6">
                        <h4 class="font-bold mb-2">Frequentist vs. Bayesian Mixed Effects Models</h4>
                        <table class="w-full border-collapse">
                            <thead>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Frequentist Approach</th>
                                    <th>Bayesian Approach</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Variance components</td>
                                    <td>Point estimates via REML or ML</td>
                                    <td>Full posterior distributions with uncertainty</td>
                                </tr>
                                <tr>
                                    <td>Random effects</td>
                                    <td>Estimated as BLUPs</td>
                                    <td>Full posterior distributions</td>
                                </tr>
                                <tr>
                                    <td>Inference</td>
                                    <td>Approximate confidence intervals</td>
                                    <td>Exact credible intervals</td>
                                </tr>
                                <tr>
                                    <td>Small samples</td>
                                    <td>Can be biased</td>
                                    <td>Can incorporate prior information</td>
                                </tr>
                                <tr>
                                    <td>Complex covariance</td>
                                    <td>Computational challenges</td>
                                    <td>Handled naturally with MCMC</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    
                    <div class="note-box mt-6">
                        <h4 class="font-bold">Implementation in Practice</h4>
                        <p>Bayesian hierarchical models are typically implemented using:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>MCMC sampling:</strong> Using software like JAGS, Stan, or PyMC</li>
                            <li><strong>Gibbs sampling:</strong> Particularly efficient for certain hierarchical structures</li>
                            <li><strong>Hamiltonian Monte Carlo:</strong> More efficient for complex hierarchical models with many parameters</li>
                        </ul>
                        <p class="mt-2">The posterior samples provide full information about all parameters at all levels of the hierarchy.</p>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">4.2 Linear Mixed Effect Models</h3>
                    
                    <p class="mb-4">Linear mixed effects models include both fixed effects (population-level parameters) and random effects (group-level deviations).</p>
                    
                    <p class="mb-4">In LMMs, we have:</p>
                    
                    <ul class="list-disc ml-6 mb-4">
                        <li><strong>Within-group model:</strong>
                            <div class="formula">
                                Yᵢⱼ = xᵢⱼᵀβⱼ + εᵢⱼ, εᵢⱼ ~ N(0,σ²) i.i.d.
                            </div>
                            Or in matrix form:
                            <div class="formula">
                                Yⱼ|βⱼ,σ² ~ N(Xⱼβⱼ, σ²Iₙⱼ)
                            </div>
                        </li>
                        <li><strong>Between-group model:</strong>
                            <div class="formula">
                                βⱼ|θ,Σ ~ N(θ,Σ) i.i.d.
                            </div>
                        </li>
                        <li><strong>Prior:</strong>
                            <div class="formula">
                                θ,Σ ~ π(θ)π(Σ)
                            </div>
                        </li>
                    </ul>
                    
                    <p class="mb-4">This can be reparameterized as:</p>
                    
                    <div class="formula">
                        βⱼ = θ + γⱼ, with γⱼ ~ N(0,Σ) i.i.d.
                    </div>
                    
                    <p class="mb-4">Substituting into the within-group model:</p>
                    
                    <div class="formula">
                        Yᵢⱼ = xᵢⱼᵀθ + xᵢⱼᵀγⱼ + εᵢⱼ
                    </div>
                    
                    <p>Here, θ is called a <strong>fixed effect</strong> (constant across groups), and γ₁,...,γⱼ are <strong>random effects</strong> (group-specific). This explains the name "mixed effects" model.</p>
                    
                    <p class="mb-4">A more general form allows different covariates for fixed and random effects:</p>
                    
                    <div class="formula">
                        Yᵢⱼ = xᵢⱼᵀθ + zᵢⱼᵀγⱼ + εᵢⱼ
                    </div>
                    
                    <p>Typical priors for the parameters are:</p>
                    
                    <ul class="list-disc ml-6 mb-4">
                        <li>θ ~ N(μ₀,L₀)</li>
                        <li>Σ ~ InvWishart(S₀⁻¹,η₀)</li>
                        <li>σ² ~ InvGamma(ν₀/2, ν₀σ₀²/2)</li>
                    </ul>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Math Scores and Socioeconomic Status</h4>
                        <p>Extending our math score example to include socioeconomic status (SES):</p>
                        <div class="formula">
                            Yᵢⱼ = β₁ⱼ + β₂ⱼSESᵢⱼ + εᵢⱼ
                        </div>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Each school has its own intercept (β₁ⱼ) and slope (β₂ⱼ)</li>
                            <li>These coefficients follow a bivariate normal distribution across schools</li>
                            <li>Schools with few students have regression lines shrunk toward the overall average</li>
                            <li>This balances individual school patterns with patterns across all schools</li>
                        </ul>
                        <p class="mt-2">The hierarchical approach provides more stable estimates, especially for schools with limited data.</p>
                    </div>
                    
                    <div class="note-box mt-6">
                        <h4 class="font-bold">Key Benefits of Hierarchical Models</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Efficiently share information across related groups</li>
                            <li>Provide more stable estimates for groups with limited data</li>
                            <li>Model variance and correlation at multiple levels</li>
                            <li>Balance between complete pooling (ignoring group differences) and no pooling (analyzing groups separately)</li>
                            <li>Naturally handle unbalanced designs (different sample sizes per group)</li>
                        </ul>
                    </div>
                </section>

                <div class="section-divider"></div>

                <!-- Model Assessment -->
                <section id="model-assessment" class="bg-white rounded-lg shadow-md p-8 mb-8">
                    <h2 class="text-2xl font-bold text-blue-700 mb-4">5. Model Assessment</h2>
                    
                    <p class="mb-4">Model assessment in Bayesian statistics involves evaluating how well models fit the data, comparing different models, and selecting variables for inclusion.</p>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mb-3">5.1 Model Selection</h3>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">5.1.1 Model Selection Based on Posterior Probabilities</h4>
                            <p class="mb-2">When comparing a set of models M₁, M₂, ..., Mₖ, we can compute the posterior probability of each model:</p>
                            <div class="formula">
                                P(Mᵢ|y) ∝ P(y|Mᵢ) × P(Mᵢ)
                            </div>
                            <p>Where P(y|Mᵢ) is the marginal likelihood or evidence for model i, and P(Mᵢ) is the prior probability of the model.</p>
                            
                            <div class="mt-4">
                                <p class="font-semibold">Bayes Factors</p>
                                <p>The Bayes factor for comparing models i and j is:</p>
                                <div class="formula">
                                    BF_ij = P(y|Mᵢ) / P(y|Mⱼ) = m_i(y) / m_j(y)
                                </div>
                                <p>Where m_i(y) = ∫ f_i(y|θ_i)g_i(θ_i)dθ_i is the marginal likelihood of model i.</p>
                                <p>For models with common parameter space Θ:</p>
                                <div class="formula">
                                    BF₀₁ = ∫ r(θ)g₁(θ|y)dθ
                                </div>
                                <p>Where r(θ) = f₀(y|θ)g₀(θ)/f₁(y|θ)g₁(θ)</p>
                                <p>Interpretations of Bayes factors:</p>
                                <ul class="list-disc ml-6 mt-2">
                                    <li>1 &lt; BF₀₁ &lt; 3: Anecdotal evidence for H₀</li>
                                    <li>3 &lt; BF₀₁ &lt; 10: Moderate evidence for H₀</li>
                                    <li>10 &lt; BF₀₁ &lt; 30: Strong evidence for H₀</li>
                                    <li>30 &lt; BF₀₁ &lt; 100: Very Strong evidence for H₀</li>
                                    <li>BF₀₁ &gt; 100: Decisive evidence for H₀</li>
                                </ul>
                            </div>
                        </div>
                        <div>
                            <h4 class="font-bold">5.1.2 Model Selection Based on Predictive Information Criteria</h4>
                            <p class="mb-2">Information criteria balance model fit and complexity:</p>
                            
                            <div class="mt-2">
                                <p class="font-semibold">Deviance Information Criterion (DIC)</p>
                                <div class="formula">
                                    DIC = D(θ̄) + 2p_D
                                </div>
                                <p>Where D(θ̄) is the deviance at the posterior mean, and p_D is the effective number of parameters.</p>
                            </div>
                            
                            <div class="mt-4">
                                <p class="font-semibold">Widely Applicable Information Criterion (WAIC)</p>
                                <div class="formula">
                                    WAIC = -2(lppd - p_WAIC)
                                </div>
                                <p>Where lppd is the log pointwise predictive density, and p_WAIC measures model complexity.</p>
                                <p class="mt-2">WAIC is an improvement over DIC that better handles hierarchical models. For large samples, WAIC approximates leave-one-out cross-validation.</p>
                            </div>
                            
                            <div class="mt-4">
                                <p class="font-semibold">Log Pseudo-Marginal Likelihood (LPML)</p>
                                <div class="formula">
                                    LPML_j = ∑ log(CPO_i)
                                </div>
                                <p>Where CPO_i = p(y_i|y_{-i}) is the conditional predictive ordinate for observation i, using leave-one-out cross-validation.</p>
                                <p>Higher LPML values indicate better predictive performance.</p>
                            </div>
                            
                            <div class="mt-4">
                                <p class="font-semibold">Leave-One-Out Cross-Validation (LOO)</p>
                                <p>LOO estimates out-of-sample predictive accuracy by approximating leave-one-out cross-validation.</p>
                            </div>
                        </div>
                    </div>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Comparing Economic Growth Models</h4>
                        <p>Economists compare three models for predicting GDP growth:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>M₁:</strong> Based on domestic indicators only</li>
                            <li><strong>M₂:</strong> Includes international trade factors</li>
                            <li><strong>M₃:</strong> Adds monetary policy variables</li>
                        </ul>
                        
                        <div class="mt-4">
                            <p class="font-semibold">Results:</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li>WAIC: M₁ = 450, M₂ = 425, M₃ = 428</li>
                                <li>LOO: M₁ = 455, M₂ = 430, M₃ = 435</li>
                                <li>Bayes Factor BF₂₁ = 15 (positive evidence for M₂ over M₁)</li>
                                <li>Bayes Factor BF₂₃ = 2.5 (weak evidence for M₂ over M₃)</li>
                            </ul>
                            <p class="mt-2">Conclusion: Model M₂ with international trade factors has the best predictive performance according to all criteria, though the advantage over M₃ is modest.</p>
                        </div>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">5.2 Model Checking</h3>
                    
                    <p class="mb-4">Model checking evaluates how well a model fits the observed data and identifies potential problems. Even the "best" model according to selection criteria may still provide a poor fit.</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">Posterior Predictive Checks</h4>
                            <p>Compare observed data to data simulated from the posterior predictive distribution:</p>
                            <ol class="list-decimal ml-6 mt-2">
                                <li>Draw parameters θ from the posterior distribution</li>
                                <li>Generate replicated data y_rep from the model using θ</li>
                                <li>Compare features of y_rep to the observed data y</li>
                            </ol>
                            <p class="mt-2">Discrepancies indicate potential model inadequacies.</p>
                        </div>
                        <div>
                            <h4 class="font-bold">Bayesian p-values</h4>
                            <p>Calculate the probability that a test statistic T for replicated data exceeds the observed statistic:</p>
                            <div class="formula">
                                p_B = P(T(y_rep) ≥ T(y) | y)
                            </div>
                            <p>Values close to 0 or 1 indicate poor fit.</p>
                            
                            <div class="concept-diagram text-center mt-4">
                                <svg width="300" height="150" viewBox="0 0 300 150" xmlns="http://www.w3.org/2000/svg">
                                    <!-- Distribution of T(y_rep) -->
                                    <path d="M20,120 C40,30 120,10 150,10 C180,10 260,30 280,120" stroke="#3b82f6" fill="#e0e7ff" fill-opacity="0.5" stroke-width="2"/>
                                    
                                    <!-- Observed T(y) -->
                                    <line x1="90" y1="10" x2="90" y2="120" stroke="#f97316" stroke-width="2" stroke-dasharray="5,3"/>
                                    <text x="90" y="135" text-anchor="middle" font-size="12">T(y)</text>
                                    
                                    <!-- Labels -->
                                    <text x="150" y="75" text-anchor="middle" font-size="12">Distribution of T(y_rep)</text>
                                    <text x="240" y="100" text-anchor="middle" font-size="12" fill="#f97316">p_B = 0.8</text>
                                </svg>
                            </div>
                        </div>
                    </div>

                    <p class="mt-2">For outlier detection, we can use predictive tail probabilities:</p>
                    <div class="formula">
                        p_i = min{P(y > y_i|y), P(y < y_i|y)}
                    </div>
                    <p>Values of p_i below 0.05 suggest the observation might be an outlier.</p>
                    
                    <div class="tip-box">
                        <h4 class="font-bold">Diagnostic Plots for Model Checking</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>Residual plots:</strong> Check for patterns in residuals</li>
                            <li><strong>QQ plots:</strong> Compare quantiles of residuals to theoretical distributions</li>
                            <li><strong>Predictive intervals:</strong> Check if observed values fall within predicted ranges</li>
                            <li><strong>Density overlays:</strong> Compare densities of observed and replicated data</li>
                        </ul>
</div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">5.3 Covariate Selection</h3>
                    
                    <p class="mb-4">Selecting which covariates to include in a model is a critical step in model building.</p>
                    
                    <h4 class="font-bold mt-6">5.3.1 A Hierarchical Mixture Model for Variable Selection</h4>
                    
                    <p class="mb-4">Bayesian variable selection often uses indicator variables to determine whether predictors should be included:</p>
                    
                    <div class="formula">
                        β_j = γ_j × α_j
                    </div>
                    
                    <p>Where:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>γ_j is an indicator (0 or 1) for inclusion of predictor j</li>
                        <li>α_j is the coefficient value if the predictor is included</li>
                        <li>P(γ_j = 1) = π_j is the prior probability of including predictor j</li>
                    </ul>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">Spike and Slab Priors</h4>
                            <p>A common approach uses a mixture of two distributions:</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li><strong>Spike:</strong> Concentrated mass at or near zero</li>
                                <li><strong>Slab:</strong> Diffuse distribution for non-zero coefficients</li>
                            </ul>
                            <div class="formula">
                                p(β_j) = π_j × N(0, τ²) + (1-π_j) × δ₀
                            </div>
                            <p>Where δ₀ is a point mass at zero or a very narrow distribution.</p>
                        </div>
                        <div>
                            <div class="concept-diagram">
                                <svg width="300" height="200" viewBox="0 0 300 200" xmlns="http://www.w3.org/2000/svg">
                                    <!-- Axes -->
                                    <line x1="50" y1="150" x2="250" y2="150" stroke="black" stroke-width="1"/>
                                    <line x1="150" y1="50" x2="150" y2="150" stroke="black" stroke-width="1"/>
                                    <text x="150" y="170" text-anchor="middle" font-size="12">0</text>
                                    <text x="250" y="170" text-anchor="middle" font-size="12">β</text>
                                    
                                    <!-- Spike -->
                                    <path d="M145,150 L145,60 L155,60 L155,150 Z" fill="#f97316" fill-opacity="0.7"/>
                                    <text x="150" y="45" text-anchor="middle" font-size="12">Spike</text>
                                    
                                    <!-- Slab -->
                                    <path d="M50,150 C80,140 120,80 150,70 C180,80 220,140 250,150" stroke="#3b82f6" fill="#e0e7ff" fill-opacity="0.5" stroke-width="2"/>
                                    <text x="200" y="100" text-anchor="middle" font-size="12">Slab</text>
                                </svg>
                            </div>
                        </div>
                    </div>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Genomic Prediction</h4>
                        <p>In a genomic study, researchers want to identify which genes influence a disease from thousands of potential predictors:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>Prior:</strong> Each gene has a small prior probability of being relevant (π_j = 0.01)</li>
                            <li><strong>Data:</strong> Genomic data for thousands of genes</li>
                            <li><strong>Posterior inference:</strong> The posterior distribution combines prior beliefs with the data, giving posterior inclusion probabilities for each gene</li>
                            <li><strong>Selection:</strong> Genes with high posterior probabilities (e.g., > 0.5) are selected for further investigation</li>
                        </ul>
                        
                        <div class="mt-4">
                            <p class="font-semibold">Results for Key Genes:</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li>Gene A: Posterior inclusion probability = 0.95</li>
                                <li>Gene B: Posterior inclusion probability = 0.82</li>
                                <li>Gene C: Posterior inclusion probability = 0.12</li>
                                <li>Gene D: Posterior inclusion probability = 0.03</li>
                            </ul>
                            <p class="mt-2">Genes A and B are strong candidates for association with the disease.</p>
                        </div>
                    </div>
                    
                    <div class="note-box mt-6">
                        <h4 class="font-bold">Alternative Approaches to Variable Selection</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>Shrinkage priors:</strong> Use priors that shrink small effects toward zero (Laplace, horseshoe)</li>
                            <li><strong>Bayes factors:</strong> Compare models with and without specific predictors</li>
                            <li><strong>Model averaging:</strong> Average predictions across models with different variable combinations</li>
                            <li><strong>Projection predictive selection:</strong> Project complex models onto simpler submodels</li>
                        </ul>
                        <p class="mt-2">These approaches avoid the "all or nothing" nature of classical variable selection methods.</p>
                    </div>
                </section>

                <div class="section-divider"></div>

                <!-- Survival Analysis -->
                <section id="survival" class="bg-white rounded-lg shadow-md p-8 mb-8">
                    <h2 class="text-2xl font-bold text-blue-700 mb-4">6. Survival Analysis</h2>
                    
                    <p class="mb-4">Survival analysis deals with time-to-event data, such as the time until death, failure, or any defined event occurs. Bayesian approaches provide a natural framework for handling the unique challenges of survival data.</p>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mb-3">6.1 Models for Exchangeable Observations</h3>
                    
                    <h4 class="font-bold mt-6">6.1.1 Survival and Hazard Functions</h4>
                    
                    <p class="mb-4">Key functions in survival analysis include:</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <p class="font-semibold">Survival Function S(t)</p>
                            <div class="formula">
                                S(t) = P(T > t)
                            </div>
                            <p>Probability of surviving beyond time t</p>
                            
                            <div class="concept-diagram mt-4">
                                <svg width="250" height="150" viewBox="0 0 250 150" xmlns="http://www.w3.org/2000/svg">
                                    <!-- Axes -->
                                    <line x1="30" y1="120" x2="220" y2="120" stroke="black" stroke-width="1"/>
                                    <line x1="30" y1="30" x2="30" y2="120" stroke="black" stroke-width="1"/>
                                    <text x="125" y="140" text-anchor="middle" font-size="14">Time</text>
                                    <text x="15" y="75" text-anchor="middle" font-size="14" transform="rotate(-90, 15, 75)">S(t)</text>
                                    
                                    <!-- Survival curve -->
                                    <path d="M30,30 L80,30 L80,50 L130,50 L130,70 L160,70 L160,90 L190,90 L190,120 L220,120" stroke="#3b82f6" fill="none" stroke-width="2"/>
                                </svg>
                            </div>
                        </div>
                        <div>
                            <p class="font-semibold">Hazard Function h(t)</p>
                            <div class="formula">
                                h(t) = lim<sub>Δt→0</sub> P(t ≤ T < t+Δt | T ≥ t) / Δt
                            </div>
                            <p>Instantaneous rate of event occurrence</p>
                            
                            <div class="concept-diagram mt-4">
                                <svg width="250" height="150" viewBox="0 0 250 150" xmlns="http://www.w3.org/2000/svg">
                                    <!-- Axes -->
                                    <line x1="30" y1="120" x2="220" y2="120" stroke="black" stroke-width="1"/>
                                    <line x1="30" y1="30" x2="30" y2="120" stroke="black" stroke-width="1"/>
                                    <text x="125" y="140" text-anchor="middle" font-size="14">Time</text>
                                    <text x="15" y="75" text-anchor="middle" font-size="14" transform="rotate(-90, 15, 75)">h(t)</text>
                                    
                                    <!-- Hazard curve (bathtub shape) -->
                                    <path d="M30,90 C60,50 100,100 150,100 C180,100 200,50 220,30" stroke="#f97316" fill="none" stroke-width="2"/>
                                </svg>
                            </div>
                        </div>
                    </div>
                    
                    <p class="mb-4">The relationship between survival and hazard functions:</p>
                    
                    <div class="formula">
                        S(t) = exp(-∫<sub>0</sub><sup>t</sup> h(u) du)
                    </div>
                    
                    <p>The cumulative hazard function is H(t) = -log(S(t)).</p>
                    
                    <div class="note-box">
                        <h4 class="font-bold">Advantages of Bayesian Survival Analysis</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Naturally incorporates prior information about survival patterns</li>
                            <li>Handles different types of censoring in a unified framework</li>
                            <li>Provides full posterior distributions for all parameters</li>
                            <li>Easily extends to complex hierarchical models</li>
                            <li>Generates probabilistic predictions with uncertainty quantification</li>
                        </ul>
                    </div>
                    
                    <h4 class="font-bold mt-6">6.1.2 Censoring</h4>
                    
                    <p class="mb-4">Censoring occurs when the exact event time is not observed:</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-6">
                        <div class="card bg-blue-50 p-4">
                            <h5 class="font-bold text-blue-700">Right Censoring</h5>
                            <p>Event occurs after the observation period ends</p>
                            <div class="concept-diagram mt-2">
                                <svg width="150" height="70" viewBox="0 0 150 70" xmlns="http://www.w3.org/2000/svg">
                                    <line x1="10" y1="35" x2="130" y2="35" stroke="black" stroke-width="1"/>
                                    <circle cx="10" cy="35" r="3" fill="#3b82f6"/>
                                    <line x1="100" y1="25" x2="100" y2="45" stroke="#f97316" stroke-width="2"/>
                                    <text x="105" y="20" font-size="12">Censored</text>
                                    <text x="140" y="35" font-size="12">→</text>
                                </svg>
                            </div>
                        </div>
                        <div class="card bg-blue-50 p-4">
                            <h5 class="font-bold text-blue-700">Left Censoring</h5>
                            <p>Event occurs before observation begins</p>
                            <div class="concept-diagram mt-2">
                                <svg width="150" height="70" viewBox="0 0 150 70" xmlns="http://www.w3.org/2000/svg">
                                    <line x1="20" y1="35" x2="140" y2="35" stroke="black" stroke-width="1"/>
                                    <circle cx="140" cy="35" r="3" fill="#3b82f6"/>
                                    <line x1="50" y1="25" x2="50" y2="45" stroke="#f97316" stroke-width="2"/>
                                    <text x="45" y="20" font-size="12">Censored</text>
                                    <text x="10" y="35" font-size="12">←</text>
                                </svg>
                            </div>
                        </div>
                        <div class="card bg-blue-50 p-4">
                            <h5 class="font-bold text-blue-700">Interval Censoring</h5>
                            <p>Event occurs within a known interval</p>
                            <div class="concept-diagram mt-2">
                                <svg width="150" height="70" viewBox="0 0 150 70" xmlns="http://www.w3.org/2000/svg">
                                    <line x1="10" y1="35" x2="140" y2="35" stroke="black" stroke-width="1"/>
                                    <circle cx="10" cy="35" r="3" fill="#3b82f6"/>
                                    <circle cx="140" cy="35" r="3" fill="#3b82f6"/>
                                    <line x1="50" y1="25" x2="50" y2="45" stroke="#f97316" stroke-width="2"/>
                                    <line x1="100" y1="25" x2="100" y2="45" stroke="#f97316" stroke-width="2"/>
                                    <text x="70" y="20" font-size="12">Censored</text>
                                </svg>
                            </div>
                        </div>
                    </div>
                    
                    <div class="note-box">
                        <h4 class="font-bold">Increasing and Decreasing Failure Rate</h4>
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                            <div>
                                <p class="font-semibold">IFR (Increasing Failure Rate)</p>
                                <ul class="list-disc ml-6">
                                    <li>Hazard h(t) increases over time</li>
                                    <li>Represents aging/deterioration</li>
                                    <li>P(T > t+s | T > t) < P(T > s)</li>
                                </ul>
                            </div>
                            <div>
                                <p class="font-semibold">DFR (Decreasing Failure Rate)</p>
                                <ul class="list-disc ml-6">
                                    <li>Hazard h(t) decreases over time</li>
                                    <li>Represents improvement/burn-in</li>
                                    <li>P(T > t+s | T > t) > P(T > s)</li>
                                </ul>
                            </div>
                        </div>
                        <p class="mt-2">The exponential distribution is the only continuous distribution with constant hazard (both IFR and DFR).</p>
                    </div>
                    
                    <h4 class="font-bold mt-6">6.1.3 Likelihood for Right Censored Data</h4>
                    
                    <p class="mb-4">For right-censored data, we observe pairs (Yi, δi) where:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>Yi = min(Ti, Ci) is the observed time (either event or censoring)</li>
                        <li>δi = 1 if the event is observed (Ti ≤ Ci), or 0 if censored (Ti > Ci)</li>
                    </ul>
                    
                    <p>The likelihood contribution is:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>For exact failure times (δi = 1): f(ti) = h(ti) × S(ti)</li>
                        <li>For right-censored observations (δi = 0): S(ti)</li>
                    </ul>
                    
                    <div class="formula">
                        L(θ) = ∏<sub>i∈D</sub> h(t_i|θ) × ∏<sub>i∈C</sub> S(t_i|θ)
                    </div>
                    
                    <p>Where D is the set of individuals with observed events, and C is the set of censored individuals.</p>
                    
                    <p class="mt-2">For valid inference, we assume:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li><strong>Independent censoring:</strong> T is independent of C</li>
                        <li><strong>Non-informative censoring:</strong> The censoring distribution doesn't depend on the same parameters as S(t)</li>
                    </ul>
                    
                    <p class="mb-4">For parametric models, we can use:</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h5 class="font-bold">Exponential Distribution</h5>
                            <ul class="list-disc ml-6">
                                <li>Constant hazard: h(t) = λ</li>
                                <li>Survival function: S(t) = exp(-λt)</li>
                                <li>Common prior: λ ~ Gamma(α, β)</li>
                                <li>Posterior: λ|data ~ Gamma(α + d, β + Σt_i)</li>
                            </ul>
                            <p class="mt-2">Where d is the number of events, and Σt_i is the sum of all observation times.</p>
                        </div>
                        <div>
                            <h5 class="font-bold">Weibull Distribution</h5>
                            <ul class="list-disc ml-6">
                                <li>Hazard: h(t) = λαt^(α-1)</li>
                                <li>Survival: S(t) = exp(-λt^α)</li>
                                <li>Flexible shape: increasing (α > 1) or decreasing (α < 1) hazard</li>
                                <li>No conjugate prior: MCMC used for posterior sampling</li>
                            </ul>
                        </div>
                    </div>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Medical Device Reliability</h4>
                        <p>Engineers analyze the time to failure for medical implants:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li>100 devices were monitored for up to 5 years</li>
                            <li>15 failed during the study period</li>
                            <li>85 were still functioning at the end of the study (right-censored)</li>
                        </ul>
                        
                        <div class="mt-4">
                            <p class="font-semibold">Bayesian Analysis with Weibull Model:</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li>Prior belief: α ~ Gamma(2, 1) and λ ~ Gamma(1, 1)</li>
                                <li>Posterior estimation via MCMC</li>
                                <li>Results: α = 1.8 (95% CI: 1.2-2.5), indicating increasing hazard with time</li>
                                <li>Predicted 10-year reliability: 0.72 (95% CI: 0.65-0.79)</li>
                            </ul>
                        </div>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">6.2 Time-to-Event Regression Models</h3>
                    
                    <p class="mb-4">Regression models incorporate covariates to explain variation in survival times.</p>
                    
                    <h4 class="font-bold mt-6">6.2.1 Accelerated Failure-Time Regression Models</h4>
                    
                    <p class="mb-4">In Accelerated Failure-Time (AFT) models, covariates act multiplicatively on time:</p>
                    
                    <div class="formula">
                        log(T) = Xβ + σε
                    </div>
                    
                    <p>Where ε follows a specified distribution (e.g., standard normal for log-normal AFT).</p>
                    
                    <div class="concept-diagram text-center mt-6">
                        <svg width="400" height="200" viewBox="0 0 400 200" xmlns="http://www.w3.org/2000/svg">
                            <!-- Axes -->
                            <line x1="50" y1="150" x2="350" y2="150" stroke="black" stroke-width="1"/>
                            <line x1="50" y1="50" x2="50" y2="150" stroke="black" stroke-width="1"/>
                            <text x="200" y="180" text-anchor="middle" font-size="14">Time</text>
                            <text x="25" y="100" text-anchor="middle" font-size="14" transform="rotate(-90, 25, 100)">S(t)</text>
                            
                            <!-- Survival curves for different covariate values -->
                            <path d="M50,50 L100,50 L100,70 L150,70 L150,90 L200,90 L200,110 L250,110 L250,130 L300,130 L300,150 L350,150" stroke="#3b82f6" fill="none" stroke-width="2"/>
                            <path d="M50,50 L150,50 L150,70 L200,70 L200,90 L250,90 L250,110 L300,110 L300,130 L350,130" stroke="#f97316" fill="none" stroke-width="2" stroke-dasharray="5,3"/>
                            
                            <!-- Labels -->
                            <text x="360" y="150" font-size="12" fill="#3b82f6">X = 0</text>
                            <text x="360" y="130" font-size="12" fill="#f97316">X = 1</text>
                        </svg>
                    </div>
                    
                    <div class="example-box mt-6">
                        <h4 class="font-bold">Example: Cancer Survival Analysis</h4>
                        <p>Oncologists analyze survival times for cancer patients under different treatments:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Covariates: treatment type (A or B), age, tumor size, and biomarker level</li>
                            <li>Weibull AFT model with log(T) = β₀ + β₁×Treatment + β₂×Age + β₃×TumorSize + β₄×Biomarker + σε</li>
                        </ul>
                        
                        <div class="mt-4">
                            <p class="font-semibold">Bayesian Analysis Results:</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li>Treatment effect (β₁): 0.75 (95% CI: 0.45-1.05), indicating Treatment B extends survival time by factor of exp(0.75) = 2.1</li>
                                <li>Age effect (β₂): -0.03 (95% CI: -0.04 to -0.02), showing decreased survival with increasing age</li>
                                <li>Biomarker effect (β₄): 0.25 (95% CI: 0.10-0.40), suggesting positive association with survival</li>
                            </ul>
                            <p class="mt-2">The model enables personalized survival predictions based on individual patient characteristics.</p>
                        </div>
                    </div>
                    
                    <div class="note-box mt-6">
                        <h4 class="font-bold">Extensions to Basic Survival Models</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>Proportional hazards models:</strong> Cox regression with Bayesian estimation</li>
                            <li><strong>Time-varying coefficients:</strong> Allowing effects to change over time</li>
                            <li><strong>Joint models:</strong> Connecting survival with longitudinal measurements</li>
                            <li><strong>Cure models:</strong> Accounting for a proportion of subjects who will never experience the event</li>
                            <li><strong>Competing risks:</strong> Handling multiple possible event types</li>
                        </ul>
                    </div>
                </section>

                <div class="section-divider"></div>

                <!-- Spatial Models -->
                <section id="spatial" class="bg-white rounded-lg shadow-md p-8 mb-8">
                    <h2 class="text-2xl font-bold text-blue-700 mb-4">7. Spatial Models</h2>
                    
                    <p class="mb-4">Spatial models account for dependencies between observations based on their locations in space. Bayesian methods are particularly well-suited for spatial modeling due to their ability to incorporate complex dependency structures.</p>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mb-3">7.1 References and Software</h3>
                    
                    <div class="tip-box">
                        <h4 class="font-bold">Common Tools for Bayesian Spatial Analysis</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li><strong>Stan:</strong> Flexible platform for Bayesian modeling with efficient HMC sampling</li>
                            <li><strong>INLA:</strong> Integrated Nested Laplace Approximation for fast spatial inference</li>
                            <li><strong>R packages:</strong> spBayes, CARBayes, geoR, spTDyn</li>
                            <li><strong>Python libraries:</strong> PyMC, GeoPandas with Bambi</li>
                        </ul>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">7.2 Point-referenced Data</h3>
                    
                    <p class="mb-4">Point-referenced data (also called geostatistical data) consist of measurements at specific locations, typically represented as coordinates.</p>
                    
                    <div class="concept-diagram text-center mb-6">
                        <svg width="400" height="250" viewBox="0 0 400 250" xmlns="http://www.w3.org/2000/svg">
                            <!-- Region -->
                            <rect x="50" y="50" width="300" height="150" fill="#f1f5f9" stroke="#64748b" stroke-width="1"/>
                            
                            <!-- Data points -->
                            <circle cx="100" cy="80" r="5" fill="#3b82f6"/>
                            <circle cx="150" cy="120" r="5" fill="#f97316"/>
                            <circle cx="200" cy="70" r="5" fill="#10b981"/>
                            <circle cx="250" cy="150" r="5" fill="#f97316"/>
                            <circle cx="300" cy="100" r="5" fill="#3b82f6"/>
                            <circle cx="120" cy="160" r="5" fill="#10b981"/>
                            <circle cx="270" cy="90" r="5" fill="#f97316"/>
                            
                            <!-- Prediction point -->
                            <circle cx="180" cy="120" r="5" fill="none" stroke="#ef4444" stroke-width="2" stroke-dasharray="2,2"/>
                            <text x="190" y="115" font-size="12" fill="#ef4444">Prediction location</text>
                            
                            <!-- Labels -->
                            <text x="200" y="230" text-anchor="middle" font-size="14">Point-referenced data</text>
                        </svg>
                    </div>
                    
                    <h4 class="font-bold mt-6">7.2.1 A Gaussian Spatial Regression Model</h4>
                    
                    <p class="mb-4">A basic spatial regression model with Gaussian process is:</p>
                    
                    <div class="formula">
                        Y(s) = Xβ + w(s) + ε(s)
                    </div>
                    
                    <p>Where:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>Y(s) is the response at location s</li>
                        <li>Xβ represents the fixed effects (trend)</li>
                        <li>w(s) is a spatial random effect (Gaussian process)</li>
                        <li>ε(s) is measurement error (white noise)</li>
                    </ul>
                    
                    <p class="mb-4">The spatial random effect w(s) follows a multivariate normal distribution with covariance function C(s_i, s_j) that depends on the distance between locations.</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">Common Covariance Functions</h4>
                            <ul class="list-disc ml-6">
                                <li><strong>Exponential:</strong> σ² exp(-d/φ)</li>
                                <li><strong>Matérn:</strong> Flexible family with smoothness parameter</li>
                                <li><strong>Gaussian:</strong> σ² exp(-d²/φ²)</li>
                                <li><strong>Spherical:</strong> Has a finite range</li>
                            </ul>
                            <p class="mt-2">Where d is the distance between locations, σ² is the variance, and φ is the range parameter.</p>
                        </div>
                        <div>
                            <div class="concept-diagram">
                                <svg width="300" height="200" viewBox="0 0 300 200" xmlns="http://www.w3.org/2000/svg">
                                    <!-- Axes -->
                                    <line x1="50" y1="150" x2="250" y2="150" stroke="black" stroke-width="1"/>
                                    <line x1="50" y1="50" x2="50" y2="150" stroke="black" stroke-width="1"/>
                                    <text x="150" y="170" text-anchor="middle" font-size="12">Distance (d)</text>
                                    <text x="25" y="100" text-anchor="middle" font-size="12" transform="rotate(-90, 25, 100)">Correlation</text>
                                    
                                    <!-- Correlation functions -->
                                    <path d="M50,50 C100,90 170,140 250,150" stroke="#3b82f6" fill="none" stroke-width="2"/>
                                    <path d="M50,50 C70,70 100,120 250,150" stroke="#f97316" fill="none" stroke-width="2" stroke-dasharray="5,3"/>
                                    <path d="M50,50 C100,50 120,100 150,150 L250,150" stroke="#10b981" fill="none" stroke-width="2" stroke-dasharray="2,2"/>
                                    
                                    <!-- Labels -->
                                    <text x="270" y="80" font-size="10" fill="#3b82f6">Gaussian</text>
                                    <text x="270" y="100" font-size="10" fill="#f97316">Exponential</text>
                                    <text x="270" y="120" font-size="10" fill="#10b981">Spherical</text>
                                </svg>
                            </div>
                        </div>
                    </div>
                    
                    <h4 class="font-bold mt-6">7.2.2 Bayesian Kriging</h4>
                    
                    <p class="mb-4">Kriging is a method for spatial interpolation that provides the best linear unbiased predictor. Bayesian kriging extends this by accounting for uncertainty in all parameters.</p>
                    
                    <div class="formula">
                        p(Y(s₀)|Y) ∝ ∫ p(Y(s₀)|Y, θ) p(θ|Y) dθ
                    </div>
                    
                    <p>Where Y(s₀) is the value at a new location s₀, Y is the observed data, and θ represents all model parameters.</p>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Environmental Monitoring</h4>
                        <p>Environmental scientists monitor air pollution levels across a city:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Data: PM2.5 measurements at 30 monitoring stations</li>
                            <li>Covariates: Elevation, distance to major roads, population density</li>
                            <li>Spatial model with Matérn covariance function</li>
                        </ul>
                        
                        <div class="mt-4">
                            <p class="font-semibold">Bayesian Analysis Results:</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li>Fixed effects: Significant positive association with road proximity and population density</li>
                                <li>Spatial range (φ): 3.2 km (95% CI: 2.1-4.5 km)</li>
                                <li>Spatial variance (σ²): 5.4 (95% CI: 3.2-8.1)</li>
                            </ul>
                            <p class="mt-2">The model produces a complete pollution surface across the city with uncertainty quantification, helping identify high-risk areas.</p>
                        </div>
                    </div>
                    
                    <h4 class="font-bold mt-6">7.2.3 More Insight on Bayesian Models for Geo-referenced Spatio-temporal Data</h4>
                    
                    <p class="mb-4">Spatio-temporal models extend spatial models by incorporating time dependence:</p>
                    
                    <div class="formula">
                        Y(s,t) = Xβ + w(s,t) + ε(s,t)
                    </div>
                    
                    <p>Common structures for the spatio-temporal process w(s,t) include:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li><strong>Separable models:</strong> Covariance factorizes into spatial and temporal components</li>
                        <li><strong>Nonseparable models:</strong> More flexible but computationally demanding</li>
                        <li><strong>Dynamic models:</strong> Temporal evolution of spatial fields</li>
                    </ul>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">7.3 Areal Data</h3>
                    
                    <p class="mb-4">Areal data are aggregated over regions such as counties, census tracts, or postal codes.</p>
                    
                    <div class="concept-diagram text-center mb-6">
                        <svg width="400" height="250" viewBox="0 0 400 250" xmlns="http://www.w3.org/2000/svg">
                            <!-- Regions -->
                            <path d="M50,50 L150,70 L180,150 L100,180 L50,120 Z" fill="#e0e7ff" stroke="#3b82f6" stroke-width="2"/>
                            <path d="M150,70 L250,50 L300,130 L180,150 Z" fill="#ecfdf5" stroke="#10b981" stroke-width="2"/>
                            <path d="M180,150 L300,130 L320,200 L220,220 L100,180 Z" fill="#fff7ed" stroke="#f97316" stroke-width="2"/>
                            
                            <!-- Region labels -->
                            <text x="100" y="120" text-anchor="middle" font-size="14">A</text>
                            <text x="220" y="100" text-anchor="middle" font-size="14">B</text>
                            <text x="200" y="180" text-anchor="middle" font-size="14">C</text>
                            
                            <!-- Title -->
                            <text x="200" y="230" text-anchor="middle" font-size="14">Areal data with neighboring regions</text>
                        </svg>
                    </div>
                    
                    <h4 class="font-bold mt-6">7.3.1 Conditionally Autoregressive (CAR) Model</h4>
                    
                    <p class="mb-4">CAR models account for spatial dependence between adjacent regions:</p>
                    
                    <div class="formula">
                        φᵢ|φ₋ᵢ ~ N(ρ ∑ⱼ wᵢⱼφⱼ/wᵢ₊, τ²/wᵢ₊)
                    </div>
                    
                    <p>Where:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>φᵢ is the spatial effect for region i</li>
                        <li>wᵢⱼ = 1 if regions i and j are adjacent, 0 otherwise</li>
                        <li>wᵢ₊ = ∑ⱼ wᵢⱼ is the number of neighbors for region i</li>
                        <li>ρ controls the strength of spatial dependence</li>
                        <li>τ² is the variance parameter</li>
                    </ul>
                    
                    <h4 class="font-bold mt-6">7.3.2 Modification of the Intrinsic CAR Model</h4>
                    
                    <p class="mb-4">The intrinsic CAR (ICAR) model is a special case where ρ = 1:</p>
                    
                    <div class="formula">
                        φᵢ|φ₋ᵢ ~ N(∑ⱼ wᵢⱼφⱼ/wᵢ₊, τ²/wᵢ₊)
                    </div>
                    
                    <p>The ICAR model induces strong spatial smoothing but has an improper joint distribution. In practice, it's often combined with an unstructured random effect.</p>
                    
                    <h4 class="font-bold mt-6">7.3.3 GLMM + CAR Prior on the Spatial Random Effects</h4>
                    
                    <p class="mb-4">Generalized Linear Mixed Models (GLMMs) allow incorporating spatial dependence in non-Gaussian data (e.g., counts, proportions) using CAR priors on random effects:</p>
                    
                    <div class="formula">
                        g(E[Yᵢ]) = Xᵢβ + φᵢ + εᵢ
                    </div>
                    
                    <p>Where:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>g(.) is the link function (e.g., log for Poisson)</li>
                        <li>E[Yᵢ] is the expected value of the response in region i</li>
                        <li>Xᵢβ represents fixed effects</li>
                        <li>φᵢ follows a CAR prior (e.g., ICAR, BYM)</li>
                        <li>εᵢ is unstructured error (optional)</li>
                    </ul>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Disease Mapping</h4>
                        <p>Epidemiologists analyze disease incidence rates across regions:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Data: Disease counts and population sizes for each region</li>
                            <li>Covariates: Environmental factors, socioeconomic indicators</li>
                            <li>Poisson GLMM with ICAR prior on spatial random effects</li>
                        </ul>
                        
                        <div class="mt-4">
                            <p class="font-semibold">Bayesian Analysis Results:</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li>Fixed effects: Significant positive association with environmental factors</li>
                                <li>Spatial variance (τ²): 0.5 (95% CI: 0.2-1.1)</li>
                                <li>Relative risk map: Highlights high-risk regions</li>
                            </ul>
                            <p class="mt-2">The model provides a spatially smoothed disease map, helping identify areas for targeted interventions.</p>
                        </div>
                    </div>
                    
                    <p class="mt-6 mb-4">A general hierarchical model can be written as:</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                        <div>
                            <h4 class="font-bold">Data Level</h4>
                            <div class="formula">
                                y_ij ~ p(y_ij|θ_j)
                            </div>
                            <p>Data within group j depends on group-specific parameters θ_j</p>
                        </div>
                        <div>
                            <h4 class="font-bold">Group Level</h4>
                            <div class="formula">
                                θ_j ~ p(θ_j|φ)
                            </div>
                            <p>Group parameters come from a population distribution with hyperparameters φ</p>
                        </div>
                    </div>
                    
                    <div>
                        <h4 class="font-bold">Population Level</h4>
                        <div class="formula">
                            φ ~ p(φ)
                        </div>
                        <p>Hyperparameters have their own prior distributions</p>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">7.4 Spatial Regression Models</h3>
                    
                    <p class="mb-4">Spatial regression models incorporate spatial dependence in the response variable:</p>
                    
                    <div class="formula">
                        y = Xβ + w + ε
                    </div>
                    
                    <p>Where:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>y is the response vector</li>
                        <li>Xβ represents fixed effects</li>
                        <li>w is a spatial random effect</li>
                        <li>ε is measurement error</li>
                    </ul>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Housing Prices</h4>
                        <p>A real estate analyst models housing prices based on location and features:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Data: Housing prices and features (size, age, location)</li>
                            <li>Covariates: Location coordinates, size, age</li>
                            <li>Spatial regression model with ICAR prior on spatial random effects</li>
                        </ul>
                        
                        <div class="mt-4">
                            <p class="font-semibold">Bayesian Analysis Results:</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li>Fixed effects: Significant positive association with size and location</li>
                                <li>Spatial variance (τ²): 0.2 (95% CI: 0.1-0.4)</li>
                                <li>Predicted housing prices: Spatially smoothed surface</li>
                            </ul>
                            <p class="mt-2">The model provides a spatially informed housing price map, helping identify high-value areas.</p>
                        </div>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">7.5 Spatial Autoregressive (SAR) Models</h3>
                    
                    <p class="mb-4">SAR models incorporate spatial dependence in the response variable using a spatial autoregressive term:</p>
                    
                    <div class="formula">
                        y = ρWy + Xβ + ε
                    </div>
                    
                    <p>Where:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>y is the response vector</li>
                        <li>W is a spatial weights matrix</li>
                        <li>ρ is the spatial autoregressive parameter</li>
                        <li>Xβ represents fixed effects</li>
                        <li>ε is measurement error</li>
                    </ul>
                    
                    <div class="example-box">
                        <h4 class="font-bold">Example: Crime Rates</h4>
                        <p>Criminologists analyze crime rates across neighborhoods:</p>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Data: Crime rates and neighborhood characteristics</li>
                            <li>Covariates: Neighborhood features (poverty rate, population density)</li>
                            <li>SAR model with spatial autoregressive term</li>
                        </ul>
                        
                        <div class="mt-4">
                            <p class="font-semibold">Bayesian Analysis Results:</p>
                            <ul class="list-disc ml-6 mt-2">
                                <li>Fixed effects: Significant positive association with poverty rate</li>
                                <li>Spatial autoregressive parameter (ρ): 0.5 (95% CI: 0.2-0.8)</li>
                                <li>Predicted crime rates: Spatially smoothed surface</li>
                            </ul>
                            <p class="mt-2">The model provides a spatially informed crime rate map, helping identify high-crime areas.</p>
                        </div>
                    </div>
                </section>

                <div class="section-divider"></div>

                <!-- Bayesian Nonparametrics -->
                <section id="nonparametrics" class="bg-white rounded-lg shadow-md p-8 mb-8">
                    <h2 class="text-2xl font-bold text-blue-700 mb-4">8. Bayesian Nonparametrics</h2>
                    
                    <p class="mb-4">Bayesian Nonparametrics (BNP) offers flexible modeling by placing priors on infinite-dimensional spaces (e.g., spaces of functions or distributions), allowing model complexity to adapt to the data.</p>
                    
                    <h3 class="text-xl font-semibold text-blue-600 mb-3">8.1 The Dirichlet Process</h3>
                    
                    <p class="mb-4">The Dirichlet Process (DP) is a fundamental BNP prior. It's a distribution over probability distributions.</p>
                    <p>Key properties:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li><strong>Parameters:</strong> Base measure G₀ (prior expectation) and concentration parameter α (controls discreteness).</li>
                        <li><strong>Samples:</strong> Samples G ~ DP(α, G₀) are discrete probability measures almost surely.</li>
                    </ul>
                    
                    <div class="formula">
                        G ~ DP(α, G₀)
                    </div>

                    <h4 class="font-bold mt-6">8.1.1 Stick Breaking Construction</h4>
                    <p class="mb-4">Provides a constructive way to sample from a DP:</p>
                    <div class="formula">
                        G = ∑_{k=1}^∞ wₖ δ_{θ*ₖ}
                    </div>
                    <p>Where:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>θ*ₖ ~ G₀ (Atom locations)</li>
                        <li>wₖ are weights derived from Beta(1, α) variables (Stick-breaking weights).</li>
                    </ul>
                    <div class="stick-breaking-viz concept-diagram">
                        <div class="stick">
                            <div class="stick-segment w1">w₁</div>
                            <div class="stick-segment w2">w₂</div>
                            <div class="stick-segment w3">w₃</div>
                            <div class="stick-segment w4">w₄</div>
                            <div class="stick-segment w-rem">...</div>
                        </div>
                        <p class="text-center text-sm text-gray-500">Visualization of the stick-breaking process generating weights.</p>
                    </div>

                    <h4 class="font-bold mt-6">8.1.2 Weak Convergence of sequences of Dirichlet Processes</h4>
                    <p class="mb-4">Relates to the asymptotic consistency of BNP posteriors. Under suitable conditions, the posterior distribution concentrates around the true data-generating distribution as sample size increases.</p>

                    <h4 class="font-bold mt-6">8.1.3 Marginal Distribution of a Sample from a Dirichlet Process</h4>
                    <p class="mb-4">Samples θ₁, θ₂, ... drawn hierarchically (θᵢ ~ G, G ~ DP) exhibit a clustering property described by the Polya Urn Scheme (or Blackwell-MacQueen Urn Scheme).</p>
                    <p>The predictive distribution for θ_{n+1} given θ₁,...,θₙ involves choosing:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>A new value from G₀ with probability proportional to α.</li>
                        <li>An existing distinct value θ*ₖ with probability proportional to its current count nₖ.</li>
                    </ul>
                    <div class="note-box">
                        <p>This "rich get richer" property naturally induces clustering.</p>
                    </div>

                    <h3 class="text-xl font-semibold text-blue-600 mt-8 mb-3">8.2 Dirichlet Process Mixture</h3>
                    
                    <p class="mb-4">DPMs use the DP as a prior for the mixing distribution in a mixture model, enabling flexible density estimation and clustering without fixing the number of components.</p>
                    
                    <h4 class="font-bold mt-6">8.2.1 The Dirichlet Process Mixture Model</h4>
                    <p class="mb-4">Hierarchical structure:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li>yᵢ | θᵢ ~ F(θᵢ) (Likelihood)</li>
                        <li>θᵢ | G ~ G (Parameters from random measure G)</li>
                        <li>G | α, G₀ ~ DP(α, G₀) (DP prior on G)</li>
                    </ul>
                    <p>Since G is discrete, multiple θᵢ will share the same value, grouping observations (yᵢ) into clusters.</p>
                    
                    <div class="dpm-viz concept-diagram">
                        <div class="dpm-level">
                            <h5>DP Prior: G ~ DP(α, G₀)</h5>
                            <p class="text-xs text-gray-500">Defines a distribution over distributions</p>
                        </div>
                        <div class="text-center my-2">↓ Generates ↓</div>
                        <div class="dpm-level">
                            <h5>Random Measure G = ∑ wₖ δ<sub>θ*ₖ</sub></h5>
                            <p class="text-xs text-gray-500">(Discrete distribution with random atoms and weights)</p>
                            <div class="dpm-atoms">
                                <span class="dpm-atom">θ*₁ (w₁)</span>
                                <span class="dpm-atom">θ*₂ (w₂)</span>
                                <span class="dpm-atom">θ*₃ (w₃)</span>
                                <span>...</span>
                            </div>
                        </div>
                         <div class="text-center my-2">↓ Samples Drawn ↓</div>
                        <div class="dpm-level">
                            <h5>Parameters: θᵢ ~ G</h5>
                             <p class="text-xs text-gray-500">(Multiple θᵢ share the same θ*ₖ value → Clustering)</p>
                             <div class="dpm-params">
                                <span class="dpm-param">θ₁=θ*₂</span> <span class="dpm-arrow">→</span>
                                <span class="dpm-param">θ₂=θ*₁</span> <span class="dpm-arrow">→</span>
                                <span class="dpm-param">θ₃=θ*₂</span> <span class="dpm-arrow">→</span>
                                <span class="dpm-param">θ₄=θ*₁</span> <span class="dpm-arrow">→</span>
                                <span class="dpm-param">θ₅=θ*₃</span> <span class="dpm-arrow">→</span>
                                <span>...</span>
                            </div>
                        </div>
                         <div class="text-center my-2">↓ Generate Data ↓</div>
                         <div class="dpm-level">
                            <h5>Data: yᵢ ~ F( . | θᵢ)</h5>
                             <p class="text-xs text-gray-500">(Observations within a cluster share the same parameter)</p>
                            <div class="dpm-data">
                                <span class="dpm-point">y₁</span>
                                <span class="dpm-point">y₂</span>
                                <span class="dpm-point">y₃</span>
                                <span class="dpm-point">y₄</span>
                                <span class="dpm-point">y₅</span>
                                <span>...</span>
                            </div>
                        </div>
                    </div>
                       <p class="text-center text-sm text-gray-500">Conceptual diagram of a Dirichlet Process Mixture model.</p>

                    <h4 class="font-bold mt-6">8.2.2 Clustering under the Dirichlet Process Mixture</h4>
                    <p class="mb-4">The DPM automatically infers the number of clusters from the data. Posterior inference typically involves MCMC methods:</p>
                    <ul class="list-disc ml-6 mb-4">
                        <li><strong>Collapsed Gibbs sampling:</strong> Integrates out G, samples cluster assignments.</li>
                        <li><strong>Blocked Gibbs sampling:</strong> Uses truncated stick-breaking representation.</li>
                        <li><strong>Slice sampling:</strong> Avoids truncation using auxiliary variables.</li>
                    </ul>
                    <p>The posterior provides insights into the number of clusters, cluster parameters, and cluster assignments.</p>
                    
                    <div class="tip-box">
                        <h4 class="font-bold">Advantages of DPMs</h4>
                        <ul class="list-disc ml-6 mt-2">
                            <li>Infers number of clusters automatically.</li>
                            <li>Provides uncertainty quantification for clusters and parameters.</li>
                            <li>Flexible density estimation.</li>
                        </ul>
                    </div>
                </section>
                
            </div> <!-- End Main Content Column -->
        </div> <!-- End Grid -->

        <footer class="text-center mt-12 pt-4 border-t border-gray-300 text-gray-500 text-sm">
            Bayesian Statistics Guide - &copy; 2025
        </footer>
    </div> <!-- End Container -->

    <script>
        // Optional: Add JS for smooth scrolling or dynamic TOC highlighting
        document.querySelectorAll('.toc a').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>
